Date,Repository-Link,Github-Link,Summary,Blog-Title,Blog-Post,Meta-Description,Classification,Star-Count-Delta,Image-Links,Video-Links,Stars,Repository-Creation-Date
2024-02-28,https://github.com/charlax/professional-programming,https://raw.githubusercontent.com/charlax/professional-programming/master/README.md,"The text you provided is a comprehensive guide covering various aspects of professional programming. It includes principles, must-read books, articles, general material, resources, and topics related to software development. The guide aims to help developers become more proficient by sharing inspiring resources and timeless classics. It covers a wide array of topics like algorithm and data structures, API design & development, authentication/authorization, automation, biases, career growth, coding & code quality, communication, compilers, configuration, continuous integration, databases, and more. It also includes valuable insights on career advice, code reviews, mindset, productivity, and personal growth in the software engineering field.","Professional Programming Resources: Must-read Books, Articles, and More","Discover a curated collection of full-stack resources tailored to make you a proficient developer. From must-read books like 'The Pragmatic Programmer' to insightful articles sharing hard-earned lessons in software development, this blogpost offers valuable guidance and resources carefully selected to enhance your programming journey. Whether you are seeking tips on career growth, best practices for code reviews, or recommendations for continuous learning, this comprehensive list covers a wide array of topics essential for every professional programmer.","Explore a comprehensive list of resources for professional programmers, including must-read books, insightful articles, and essential guidelines for career growth and code reviews. Enhance your programming journey with carefully curated advice and valuable resources to become a more proficient developer.",Software Development,"9,321 stars this week",https://raw.githubusercontent.com/charlax/professional-programming/master/./images/amazon_writing_rules.jpeg,https://www.youtube.com/watch?v=kPRA0W1kECg; https://www.youtube.com/watch?v=zkTf0LmDqKI; https://www.youtube.com/watch?v=mVVNJKv9esE; https://www.youtube.com/watch?v=LnX3B9oaKzw; https://www.youtube.com/watch?v=FKTxC9pl-WM; https://www.youtube.com/watch?v=f84n5oFoZBc; https://www.youtube.com/watch?v=2V1FtfBDsLU; https://www.youtube.com/watch?v=E7Fbf7R3x6I; https://www.youtube.com/watch?v=y8OnoxKotPQ; https://www.youtube.com/watch?v=Oj8bfBlwHAg,43213,2015-11-07T05:07:52Z
2024-02-28,https://github.com/sherlock-project/sherlock,https://raw.githubusercontent.com/sherlock-project/sherlock/master/README.md,"The text provides information about a tool called Sherlock, which allows users to search for social media accounts by username across different platforms. The document includes details on installation steps, how to use the tool for single or multiple usernames, and additional notes for Windows users and Docker installation. It also mentions contribution guidelines and testing information for developers, as well as provides links to the project's repository, wiki, and license. The tool is open-source under the MIT license and was created by Siddharth Dushantha. There is also a visualization of the project's stargazers over time.","Hunt Down Social Media Accounts with Sherlock: Installation, Usage, and Docker Notes","Learn how to use Sherlock to hunt down social media accounts by username. This blog post covers the installation process, how to use Sherlock for searching user accounts, and tips on running Sherlock in a Docker container. Discover how to search for single or multiple users, handle Anaconda notes in Windows, and contribute to Sherlock's development. Make the most of Sherlock by understanding its features and running tests to ensure smooth functionality.","Discover how to effectively hunt down social media accounts with Sherlock. This blog post covers the installation process, usage guide, and Docker notes for using Sherlock. Learn how to search for user accounts, handle Anaconda notes, contribute to Sherlock's development, and run tests to ensure reliable results.",Open Source Tool,"1,709 stars this week",,,49598,2018-12-24T14:30:48Z
2024-02-28,https://github.com/karpathy/minbpe,https://raw.githubusercontent.com/karpathy/minbpe/master/README.md,"The text describes a Python repository, ""minbpe,"" that implements the Byte Pair Encoding (BPE) algorithm used for Language Model tokenization. It provides Tokenizer classes for training vocabulary, encoding text to tokens, and decoding tokens to text. The repository includes classes like BasicTokenizer, RegexTokenizer, and GPT4Tokenizer, with different functionalities. The code also allows training custom tokenizers, handling special tokens, and ensuring feature parity with the GPT-4 tokenizer from tiktoken. The text suggests paths for training tokenizers and includes examples, tests, an exercise for studying BPE, and mentions future improvements like optimized Python versions. The code is open source under the MIT license.",Byte Pair Encoding (BPE) Algorithm: A Comprehensive Guide for Tokenization,"Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is 'byte-level' because it runs on UTF-8 encoded strings. This algorithm was popularized for LLMs by the GPT-2 paper and the associated GPT-2 code release from OpenAI. Sennrich et al. 2015 is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs use this algorithm to train their tokenizers.","Learn about the Byte Pair Encoding (BPE) algorithm, its significance in tokenization for language models, and its role in popular LLMs like GPT series. Understand the implementation through examples and comparisons with GPT-4. Find out how to train your own tokenizer and explore the potential paths for development. Discover special token handling and ways to optimize the BPE algorithm. Dive into tests, exercises, and lectures on BPE. MIT License.",Language Models,"2,238 stars this week",,https://www.youtube.com/watch?v=zduSFxRajkE,7308,2024-02-16T16:18:15Z
2024-02-28,https://github.com/chatchat-space/Langchain-Chatchat,https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md,"LangChain-Chatchat is an open-source project that implements a retriever-augmented generator (RAG) large model knowledge base using ChatGLM and Langchain frameworks. Version `0.2.10` marks the end of the `0.2.x` series with no more updates, focusing on developing a more practical `Langchain-Chatchat 0.3.x`. The project aims to provide a user-friendly, offline-capable knowledge base question-answering solution tailored for the Chinese scene using open-source models. By leveraging FastChat, LangChain-Chatchat integrates various models like Vicuna, Alpaca, LLaMA, Koala, RWKV via FastAPI API or Streamlit WebUI. It supports private deployment using open LLM and Embedding models, and plans to expand integration with different models and APIs.",,,,Language Models,"1,590 stars this week",https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/logo-long-chatchat-trans-v2.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/langchain+chatglm.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/langchain+chatglm2.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/fastapi_docs_026.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/LLM_success.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/init_knowledge_base.jpg,,23991,2023-03-31T12:12:45Z
2024-02-28,https://github.com/facebookresearch/DiT,https://raw.githubusercontent.com/facebookresearch/DiT/main/README.md,"The text introduces ""Scalable Diffusion Models with Transformers (DiT)"" implemented in PyTorch. The authors analyze the scalability and performance of DiTs compared to U-Net backbones using transformer models. They achieved state-of-the-art results on ImageNet benchmarks and provide pre-trained models for sampling. The repository includes PyTorch implementations, pre-trained models, training scripts, as well as sampling utilities. The impact of training on PyTorch versus JAX is discussed, showing similar performances. Additionally, tips for speeding up training and features to add are suggested. The text includes links to the paper, project page, implementation, and sampling resources and presents BibTeX for citation.",Scalable Diffusion Models with Transformers (DiT) - Official PyTorch Implementation,"This blog post presents the official PyTorch implementation of Scalable Diffusion Models with Transformers (DiT). The training process, model definitions, pre-trained weights, and sampling code are provided for DiTs. The post highlights the scalability of DiTs, their performance on ImageNet benchmarks, and the comparison with prior diffusion models. It also covers the setup, sampling, training, evaluation, and differences from JAX of the models.","Explore the official PyTorch implementation of Scalable Diffusion Models with Transformers (DiT). Learn about the scalability and performance of DiTs on ImageNet benchmarks. Discover the setup, sampling, training, evaluation, and differences from JAX of these models.",Language Models,"1,025 stars this week",https://raw.githubusercontent.com/facebookresearch/DiT/main/visuals/sample_grid_0.png; https://raw.githubusercontent.com/facebookresearch/DiT/main/visuals/sample_grid_1.png,,4046,2022-12-16T01:00:34Z
2024-02-28,https://github.com/jackfrued/Python-100-Days,https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/README.md,"The text discusses a 100-day journey to go from a beginner to an expert in Python. It includes content revision and video tutorials for beginners to grasp foundational concepts easily. The author also emphasizes the application areas of Python and career development opportunities in fields like backend development, DevOps, data science, machine learning, and more. The text covers various topics such as basic Python language elements, branching and looping structures, functions, object-oriented programming, GUI and game development, file handling, string manipulation, regular expressions, network programming, image and document processing, data analysis, machine learning, and team project development. During the journey, various tools and concepts like Agile development, Docker containers, MySQL performance optimization, REST API design, Django development, software testing, deployment procedures, e-commerce website essentials, performance tuning, and interview preparation are discussed. The 100-day journey culminates with a detailed Python interview question compilation.",Python - 100å¤©ä»Žæ–°æ‰‹åˆ°å¤§å¸ˆ,Python - 100å¤©ä»Žæ–°æ‰‹åˆ°å¤§å¸ˆ blogpost text not longer than 5 sentences...,"Python - 100å¤©ä»Žæ–°æ‰‹åˆ°å¤§å¸ˆ blogpost with tips, projects, and resources. Learn Python and advance from novice to expert in 100 days with this comprehensive guide.",Python Learning Journey,"1,042 stars this week",https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/agile-scrum-sprint-cycle.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/company_architecture.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/pylint.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/requirements_by_xmind.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/uml-class-diagram.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/power-designer-pdm.png,,147646,2018-03-01T16:05:52Z
2024-02-28,https://github.com/pydantic/FastUI,https://raw.githubusercontent.com/pydantic/FastUI/main/README.md,"FastUI is a new approach to creating web application user interfaces using declarative Python code, aiming to simplify frontend development. It allows Python developers to build responsive web apps with React without writing JavaScript, and enables frontend developers to focus on building reusable components. FastUI consists of Pydantic models and TypeScript interfaces to define the user interface and ensures validation at build time by TypeScript and Pyright/Mypy, and at runtime by Pydantic. It offers a PyPI package for UI components, a React TypeScript package, a Bootstrap implementation, and a pre-built React app. The RESTful principle behind FastUI enables a decoupled frontend and backend, promoting code reusability and development efficiency.",FastUI: Building Web Applications with Declarative Python Code,"FastUI is a new way to build web application interfaces using declarative Python code. It allows Python developers to create responsive web applications with React without writing any JavaScript. For frontend developers, this means concentrating on building reusable components. FastUI offers a true separation of concerns, enabling the backend to define the entire application while the frontend focuses solely on user interface implementation. This blog post explores the principles behind FastUI, its components, and how it simplifies web application development.","Discover FastUI, a new approach to building web app interfaces with declarative Python code. Learn how FastUI allows Python developers to create responsive web apps with React, no JavaScript required. Find out how FastUI promotes code reusability and separates frontend from backend concerns. Dive into the RESTful principle applied in FastUI and its advantages for web development.",Python Web Development,431 stars this week,https://raw.githubusercontent.com/pydantic/FastUI/main/screenshot.png,,4182,2023-09-18T08:12:00Z
2024-02-28,https://github.com/facebookresearch/jepa,https://raw.githubusercontent.com/facebookresearch/jepa/main/README.md,"The text provides information about the **V-JEPA (Video Joint Embedding Predictive Architecture)**, an architecture for self-supervised learning of visual representations from video developed by Meta AI Research at Facebook (FAIR). V-JEPA models are trained on the VideoMix2M dataset without requiring adaptation of model parameters. The models produce versatile visual representations that excel in downstream tasks. The method uses unsupervised feature prediction and does not rely on pretrained image encoders, text, negative examples, human annotations, or pixel-level reconstruction. The text also includes details about the architecture, visualizations, model zoo with pretrained models, code structure, data preparation, launching V-JEPA pretraining, and license information.",V-JEPA: Video Joint Embedding Predictive Architecture - Unsupervised Visual Representation Learning from Video,"Official PyTorch codebase for the video joint-embedding predictive architecture, V-JEPA, a method for self-supervised learning of visual representations from video. V-JEPA models produce versatile visual representations that perform well on downstream tasks using unsupervised feature prediction. The blog also discusses the method, visualizations, model zoo, code structure, data preparation, launching V-JEPA pretraining, and evaluating the models.","Explore V-JEPA, an architecture for self-supervised learning of visual representations from videos. Discover how V-JEPA models create versatile visual representations and perform well on various downstream video and image tasks. Learn about V-JEPA's unsupervised feature prediction approach and how it achieves spatio-temporal consistency with video regions.",Self-Supervised Learning Architecture.,473 stars this week,,https://www.youtube.com/watch?v=7UkJPwz_N_0,1614,2024-02-12T15:34:31Z
2024-02-28,https://github.com/danswer-ai/danswer,https://raw.githubusercontent.com/danswer-ai/danswer/main/README.md,"Danswer is an open-source tool that allows users to ask questions in natural language and receive answers based on specific team documents. It connects to common workplace tools such as Slack, Google Drive, and Confluence. Teams have used Danswer to improve customer support, engineering efficiency, sales preparation, customer request tracking, and more. The tool offers document search, AI answers, connectors to various tools, chat support, and the ability to create custom AI assistants. Danswer also provides features like hybrid search, user authentication, admin dashboard, custom deep learning models, and flexible deployment options. The roadmap includes features like organizational understanding, code search, and more. If you're interested in contributing, check out the Contribution Guide.",Enhance Team Efficiency with Danswer: AI Document Search and Gen-AI Chat,"Danswer is an Open Source Unified Search and Gen-AI Chat tool that empowers teams to improve customer support, engineering efficiency, sales preparation, and more. It connects to various workplace tools such as Slack, Google Drive, Confluence, and others, enabling users to ask questions in natural language and receive answers from team-specific documents. With Danswer, teams can streamline processes, track customer requests, and self-serve in various domains including IT, onboarding, and HR. Explore Danswer's features, deployment options, connectors, and roadmap to maximize your team's productivity.","Discover how Danswer, an AI-powered document search and chat tool, enhances team efficiency by improving customer support, engineering processes, sales preparation, and more. Connect with various workplace tools and streamline workflows effectively. Learn about Danswer's features, deployment options, and roadmap for continuous improvement.",Natural Language Processing,848 stars this week,,https://www.youtube.com/watch?v=geNzY1nbCnU,8229,2023-04-27T06:04:01Z
2024-02-28,https://github.com/public-apis/public-apis,https://raw.githubusercontent.com/public-apis/public-apis/master/README.md,"The text provides a comprehensive list of free Public APIs for software and web development. It includes APIs from various categories like Animals, Anime, Anti-Malware, Art & Design, Authentication & Authorization, Blockchain, Books, Business, Calendar, Cloud Storage & File Sharing, Continuous Integration, Cryptocurrency, and Currency Exchange. Each category contains multiple APIs with descriptions, authentication requirements, HTTPS support, and CORS availability. Developers can access a wide range of data and functionalities for their projects, such as animal pictures, anime quotes, holiday data, file sharing, market data for cryptocurrencies, exchange rates, and much more.",Discover Public APIs for Software and Web Development,"Explore a collective list of free APIs for software and web development purposes. Find APIs related to various categories such as Animals, Anime, Anti-Malware, Art & Design, Authentication & Authorization, Blockchain, Books, Business, Calendar, Cloud Storage & File Sharing, Continuous Integration, Cryptocurrency, Currency Exchange, and more. Each API comes with a description, authentication requirements, whether it supports HTTPS, and its CORS policy.","Discover a curated list of free APIs for software and web development purposes. Explore various categories such as Animals, Anime, Anti-Malware, Art & Design, Blockchain, Books, Business, Calendar, Cryptocurrency, and more. Each API includes a description, authentication details, HTTPS support, and CORS policy.",Public APIs Collection,"1,567 stars this week",,,283081,2016-03-20T23:49:42Z
2024-02-28,https://github.com/cubiq/ComfyUI_InstantID,https://raw.githubusercontent.com/cubiq/ComfyUI_InstantID/main/README.md,"The text discusses the ComfyUI InstantID extension, providing native support for InstantID with important updates like noise injection and bug fixes. Users can find basic workflows and a video tutorial for installation assistance. To install, users must upgrade ComfyUI, download required libraries/models, and address watermarks. Lowering CFG and using noise injection can enhance results. The extension supports face keypoints, additional controlnets, styling with IPAdapter, and multi-ID. An advanced node offers more control over InstantID modeling and noise. Overall, the extension works best with SDXL Turbo/Lighting and community's checkpoints for optimal results.",ComfyUI InstantID (Native Support) - Important Updates and Installation Guide,"Native InstantID support for ComfyUI. This extension integrates InstantID natively with ComfyUI, eliminating the need for diffusers. Updates include noise injection in negative embeds, bug fixes, and improved node usability. Learn how to install InstantID with InsightFace model, controlnet, and main model. Discover tips for avoiding watermarks, adjusting CFG, utilizing face keypoints, noise injection, and additional controlnets. Explore styling options with IPAdapter and consider Multi-ID support. An advanced InstantID node is available for fine-tuning compositions.","Discover the latest updates and installation guide for ComfyUI's Native InstantID support. Learn about noise injection, bug fixes, model installations, watermark avoidance, CFG adjustments, face keypoints, and styling options. Explore Multi-ID support and the advanced InstantID node for efficient composition tweaking.",Software Development,102 stars this week,https://raw.githubusercontent.com/cubiq/ComfyUI_InstantID/main/examples/instantid_basic_workflow.jpg,https://www.youtube.com/watch?v=wMLiGhogOPE; https://www.youtube.com/watch?v=wMLiGhogOPE,367,2024-01-27T17:07:29Z
2024-02-28,https://github.com/lllyasviel/stable-diffusion-webui-forge,https://raw.githubusercontent.com/lllyasviel/stable-diffusion-webui-forge/main/README.md,"The Stable Diffusion WebUI Forge is a platform built on top of the Stable Diffusion WebUI to enhance development, resource management, and speed up inference. The project draws inspiration from ""Minecraft Forge."" It offers significant speed-ups in inference based on different GPU configurations. Forge introduces the Unet Patcher feature, making it easier to implement methods like Self-Attention Guidance, Kohya High Res Fix, FreeU, etc., in just about 100 lines of code. Additionally, Forge includes new samplers like DDPM, DDPM Karras, DPM++ 2M Turbo, among others. Extensions like Masked Ip-Adapter, Masked ControlNet, PhotoMaker, as well as various preprocessor and control enhancements, have been made possible with Forge. Contributing to Forge is done through a bot that merges commits from the original repository automatically.",Stable Diffusion WebUI Forge Features and Installation Guide,"Stable Diffusion WebUI Forge is a powerful platform built on top of Stable Diffusion WebUI with the goal of enhancing development, optimizing resource management, and boosting inference speed. It introduces significant improvements, like the 'Unet Patcher' which simplifies the implementation of advanced methods like Self-Attention Guidance and Kohya High Res Fix. Additionally, Forge offers new capabilities such as ControlNets, samplers like DDPM and LCM Karras, and a seamless installation process for users proficient in Git or using the one-click installation package.","Discover the new features and enhancements brought by Stable Diffusion WebUI Forge, a powerful platform offering improved resource management, faster inference speeds, and simplified extension development. Learn how to install Forge using Git or a convenient one-click installation package.",Deep Learning Platform,540 stars this week,,,2508,2024-01-14T11:39:30Z
2024-02-28,https://github.com/guoyww/AnimateDiff,https://raw.githubusercontent.com/guoyww/AnimateDiff/main/README.md,"The text discusses the AnimateDiff project, an implementation to animate personalized text-to-image diffusion models without specific tuning. It consists of various versions, such as v1, v2, and v3, as well as modules like MotionLoRA and SparseCtrl. The project enables animation generation from community models like RealisticVision and ToonYou. The user interface was developed by the community, and a Gradio demo is available. The text provides technical details, model zoo links, setup instructions, demo examples, common issues, and contact information. The project is for academic use, and citations are provided. For more details, you can refer to the original text.",AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning,"This repository is the official implementation of AnimateDiff, a plug-and-play module turning most community models into animation generators, without the need of additional training. We developed four versions of AnimateDiff, each offering unique features and improvements over the previous versions. The latest version, AnimateDiff v3, introduces features like **Domain Adapter LoRA**, as well as two (RGB image/scribble) SparseCtrl Encoders for enhanced control over the generation process. AnimateDiff makes it easy to create animations from text-to-image models, providing various options for users to customize their animations.","Learn about AnimateDiff, a powerful module that allows users to animate text-to-image diffusion models without specific tuning. Discover the latest version, AnimateDiff v3, with features like Domain Adapter LoRA and SparseCtrl Encoders for enhanced control. Read about the various community models and contributions, and explore the possibilities of creating personalized animations using AnimateDiff.",Computer Vision,181 stars this week,,https://www.youtube.com/watch?v=mfaqqL5yOO4; https://www.youtube.com/watch?v=N1tXVR9lplM; https://www.youtube.com/watch?v=zss3xbtvOWw,7878,2023-06-17T11:14:28Z
2024-02-28,https://github.com/vvbbnn00/WARP-Clash-API,https://raw.githubusercontent.com/vvbbnn00/WARP-Clash-API/master/README.md,"The text is a guide for using the WARP Clash API, a tool that enables subscription-based usage of WARP+ for various clients like Clash and Shadowrocket. It includes features like unlimited WARP+ traffic, IP optimization, automated traffic scraping, and manual IP optimization. The setup involves installing Docker and Docker Compose, cloning the project, configuring environment variables, compiling and running with Docker Compose, and obtaining the subscription link. The text also covers manual IP optimization steps and a list of available environment variables for customization. Advanced operations such as resetting PublicKey/PrivateKey and setting LicenseKey are explained as well. The text acknowledges and references open-source projects the WARP Clash API is built upon. There's also information about a community-deployed instance of the tool.",WARP Clash API: Enjoy Fast Private Nodes with Docker Compose Deployment,"The WARP Clash API project allows you to use 'WARP+' by subscription, supporting clients like Clash and Shadowrocket. It features unlimited WARP+ traffic access with IP optimization and Docker compose one-click deployment. Enjoy automatic traffic renewal and random node updates for a unique experience. Follow the easy steps to set up and run the project, including configuring your own 'LicenseKey' and optional settings like 'SECRET_KEY'. Take advantage of manual IP optimization if needed.","Discover how to set up and deploy WARP Clash API for fast, private nodes using Docker compose. Enjoy unique features like IP optimization and automatic traffic renewal. Learn about configuring settings such as 'SECRET_KEY' and 'LicenseKey'. Read on for a step-by-step guide and manual IP optimization options.",Public APIs Collection,"2,873 stars this week",,,4033,2023-08-23T19:19:40Z
2024-02-28,https://github.com/vinta/awesome-python,https://raw.githubusercontent.com/vinta/awesome-python/master/README.md,"The text is a list of various Python libraries and resources categorized into different sections like Admin Panels, Algorithms and Design Patterns, ASGI Servers, Asynchronous Programming, Audio, Authentication, and many more. Each section contains libraries and frameworks related to that specific topic. It covers a wide range of areas from web development, machine learning, networking, to game development. Some notable libraries mentioned include Django, SQLAlchemy, Scikit-learn, Flask, TensorFlow, and many more. Overall, the text serves as a comprehensive guide to the diverse ecosystem of Python libraries and resources available for developers.",A Comprehensive Guide to Python Frameworks and Libraries,"Discover a wide range of Python frameworks, libraries, and tools in this detailed blog post. From web development frameworks like Django and Flask to machine learning frameworks like scikit-learn and TensorFlow, explore the diverse ecosystem of Python resources available for developers. Dive into categories such as Admin Panels, Algorithms and Design Patterns, ASGI Servers, Asynchronous Programming, Audio processing, Authentication, and much more. Delve into essential tools for web crawling, GUI development, data visualization, and distributed computing. Whether you're a beginner or an experienced developer, this blog post will help you navigate the vast landscape of Python libraries and choose the right tools for your projects.","Explore a comprehensive guide to Python frameworks, libraries, and tools covering web development, machine learning, data analysis, and more. Learn about popular categories such as Admin Panels, Algorithms, ASGI Servers, Asynchronous Programming, Audio processing, Authentication, and more.",Python Libraries Collection,925 stars this week,,,199360,2014-06-27T21:00:06Z
2024-02-28,https://github.com/zhayujie/chatgpt-on-wechat,https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/README.md,"This project is an intelligent chatbot based on large models, supporting integration with WeChat, Enterprise WeChat, Public Accounts, Feishu, and DingTalk. Users can choose from various models like GPT3.5, GPT4.0, Claude, Wenxin One Word, Xunfei Starfire, Tongyi Qianwen, Gemini, LinkAI, ZhipuAI, capable of processing text, voice, and images. It can access external resources such as operating systems and the internet through plugins, supporting customization for enterprise AI applications based on proprietary knowledge bases. The latest version includes features like multi-platform deployment, basic conversation capabilities, speech recognition, image processing, rich plugins, and knowledge base customization. 

For commercial support and business consultancy, contact the product consultant provided in the text. The project also offers support for enterprise-level AI application platforms with features like knowledge bases, agent plugins, application management, SaaS services, private deployment, and stable hosting access. It has accumulated various AI solutions in scenes such as private domain operations, intelligent customer service, and enterprise efficiency assistants, aiming to create a one-stop platform for small and medium-sized enterprises embracing AI technology. 

For more detailed information, updates, and interactions with the open-source community, refer to the links and resources provided in the text.","Intelligent Chatbot Blog: Features, Deployment Options, and More","This project is an intelligent chatbot based on large models, supporting integration with WeChat, enterprise WeChat, public accounts, Feishu, and DingTalk. It offers a choice of GPT3.5/GPT4.0/Claude/Wenxin Yiyuan/Xunfei Xinghuo/Tongyi Qianwen/Gemini/LinkAI/ZhipuAI models, with capabilities to handle text, voice, and images. The latest version includes features such as multi-end deployment, basic conversation capabilities, voice recognition, image capabilities, rich plugins, and knowledge base customization.","Explore the latest features, deployment options, and capabilities of an intelligent chatbot based on large models. Learn about multi-end deployment, basic conversation features, voice and image capabilities, rich plugins, and knowledge base customization. Discover how to integrate GPT3.5/GPT4.0/Claude/Wenxin Yiyuan/Xunfei Xinghuo/Tongyi Qianwen/Gemini/LinkAI/ZhipuAI models for enhanced AI applications.",Artificial Intelligence,710 stars this week,,,22441,2022-08-07T08:33:41Z
2024-02-28,https://github.com/FujiwaraChoki/MoneyPrinterV2,https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinterV2/main/README.md,"MoneyPrinter V2 is an application designed to automate the process of earning money online. It is the second version of the MoneyPrinter project, featuring a comprehensive rewrite for improved functionality and a modular structure. The application includes features like a Twitter Bot, YouTube Shorts Automater, affiliate marketing options, and tools for finding local businesses and conducting outreach. To use MPV2 effectively, Python 3.9 is required. Installation involves setting up Microsoft Visual C++ build tools and potentially the Go Programming Language. The project is licensed under the Affero General Public License v3.0 and is strictly for educational purposes. Detailed documentation and contribution guidelines are provided in the project repository.",MoneyPrinter V2 - Automate Your Online Earnings Effortlessly,"An Application that automates the process of making money online. MPV2 (MoneyPrinter Version 2) is, as the name suggests, the second version of the MoneyPrinter project. It is a complete rewrite of the original project, with a focus on a wider range of features and a more modular architecture. This blogpost introduces the features, installation steps, usage instructions, documentation, contribution guidelines, license information, acknowledgments, and a disclaimer for the MoneyPrinterV2 project.","Discover how MoneyPrinter V2 can streamline your online income generation with its advanced features and modular architecture. Learn how to install and use the application effectively. Find out about the documentation, contribution guidelines, license details, acknowledgments, and disclaimer associated with MoneyPrinter V2.",Money Making Automation,951 stars this week,,https://www.youtube.com/watch?v=wAZ_ZSuIqfk,1369,2024-02-12T11:20:42Z
2024-02-28,https://github.com/microsoft/UFO,https://raw.githubusercontent.com/microsoft/UFO/main/README.md,"The text introduces a novel framework called **UFO** (UI-Focused Agent for Windows OS Interaction), which consists of two agents, AppAgent and ActAgent, along with Control Interaction. UFO leverages GPT-Vision to understand and fulfill user requests across multiple applications on Windows OS. It facilitates natural language translation into actionable operations, offers interactive mode handling multiple sub-requests, includes safeguards for sensitive actions, and allows for easy extensibility. The provided steps guide users to install and configure UFO, start the process, and review execution logs. The text also mentions the availability of a technical report, news updates, highlights, citations, and related projects.",UFO: A UI-Focused Agent for Windows OS Interaction,"UFO is a UI-Focused dual-agent framework for fulfilling user requests on Windows OS. It comprises AppAgent for selecting applications and ActAgent for executing actions, with Control Interaction translating actions into UI interactions. Using GPT-Vision, UFO understands app UIs to fulfill requests. Features include being the first Windows agent, interactive mode, action safeguards, and extensibility.","Learn about UFO, a pioneering UI-Focused agent framework for Windows OS interaction. Discover its capabilities like app selection, action execution, and UI interaction translation using GPT-Vision. Find out about its features such as being the first Windows agent, interactive mode, and extensibility for tackling diverse tasks.",Natural Language Processing.,764 stars this week,,,2273,2024-01-08T05:07:52Z
2024-02-28,https://github.com/mouredev/Hello-Python,https://raw.githubusercontent.com/mouredev/Hello-Python/main/README.md,"The text provides information about a Python programming course that covers various topics such as fundamentals, intermediate concepts, backend development, and integrating ChatGPT into projects. It includes video classes on different aspects of Python, backend, frontend development, and a session on integrating ChatGPT. The course addresses common FAQs and provides links to helpful resources, official Python documentation, and tools like FastAPI, MongoDB, and Deta for backend development. The course's creator encourages support through GitHub stars. Additionally, there's an invitation to join the developer community on platforms like Twitch, Discord, and links to the creatorâ€™s social media channels.",Learn Python Programming from Scratch for Beginners,"Curso para aprender el lenguaje de programaciÃ³n Python desde cero y para principiantes. Proyecto realizado durante emisiones en directo desde Twitch. Â¡NUEVO! Curso de Python para web. Clases en vÃ­deo que cubren desde fundamentos hasta backend. TambiÃ©n incluye cursos de frontend y cÃ³mo integrar ChatGPT en tu proyecto. AdemÃ¡s, un taller de introducciÃ³n al testing con Python y curiosidades sobre Python.","Join our course to learn Python programming from scratch designed for beginners. This course includes live-streamed projects, a new Python web course, video classes covering fundamentals to backend, frontend projects, integrating ChatGPT, introduction to testing, and fun Python facts.",Python Learning Journey,605 stars this week,https://raw.githubusercontent.com/mouredev/Hello-Python/main/./Images/header.jpg; https://raw.githubusercontent.com/mouredev/mouredev/master/mouredev_emote.png,https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=Kp4Mvapo5kc; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=TbcEqkabAWU; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=_y9qQZXE24A; https://www.youtube.com/watch?v=b8COygWdvmw; https://www.youtube.com/watch?v=344uwF1z2Gg; https://www.youtube.com/watch?v=q2lCm2KAz3w,20463,2022-08-03T17:14:53Z
2024-02-28,https://github.com/xtekky/gpt4free,https://raw.githubusercontent.com/xtekky/gpt4free/main/README.md,"The text provides information about a project called ""gpt4free"" which is a proof of concept demonstrating the development of an API package with features like timeouts, load balance, and flow control. It includes details on the latest version, stats, how to get started using Docker or Python, and the usage of various models like GPT-4 and GPT-3.5. It also mentions the availability of a web UI, interference API, and configuration options. The text also covers a table of contents, providers and models list, related projects, how to contribute, list of contributors, copyright information, star history, and the project's license which is GNU GPL v3.","Latest Developments in AI API Package: New Features, Guides, and Providers","Discover the latest innovations in the AI API package world! From new guides on using smartphones to run the package to exploring how AI can assist with code writing, there's something for everyone. Join our active community on Telegram and Discord for updates and discussions. Have a site on the repository and need it taken down? Simply email takedown@g4f.ai! We're focused on improving documentation, provider status updates, and enhancing compatibility and error handling. Stay tuned for upcoming improvements and tutorials!","Stay updated with the latest developments in the AI API package world, featuring new guides, providers, and features. Join our Telegram and Discord community, learn how to use smartphones, and discover AI's potential in code writing. Need a site taken down from the repository? Contact us at takedown@g4f.ai. We're working on enhancing documentation, provider status, and more - all aimed at improving user experience and functionality.",Open Source Tool,935 stars this week,https://raw.githubusercontent.com/xtekky/gpt4free/main//docs/cat.jpeg,,53958,2023-03-29T17:00:43Z
2024-02-28,https://github.com/reflex-dev/reflex,https://raw.githubusercontent.com/reflex-dev/reflex/main/README.md,"The text informs readers that Pynecone has been renamed to Reflex and is the right repo to search for. Reflex is a tool for creating performant, customizable web apps in pure Python. The installation process involves running `pip install reflex` in the terminal. Users can create their first app by utilizing the `reflex` command line tool. An example app is provided for creating an image generation UI around DALLÂ·E using the OpenAI API. The text also covers the Reflex UI, state management, event handlers, routing, status updates, contributing guidelines, and acknowledges contributors. Reflex is in the Public Beta stage as of July 2023.",Reflex: Performant Python Web Apps - Quick Installation & Building Examples,"Learn how to quickly create performant and customizable web apps in pure Python using Reflex. Check out the easy installation steps and dive into building examples like creating an image generation UI around OpenAI's DALLÂ·E. With Reflex, you can easily deploy and host your apps with fast refreshes for instant changes.",Discover how to build performant and customizable web apps in Python with Reflex. Follow the simple installation steps and explore building examples like creating an image generation UI using OpenAI's DALLÂ·E. Deploy your apps quickly and enjoy fast refreshes for instant updates.,Python Web Development,146 stars this week,,,15099,2022-10-25T03:08:48Z
2024-02-28,https://github.com/ndleah/python-mini-project,https://raw.githubusercontent.com/ndleah/python-mini-project/main/README.md,"The provided text is about a Python Mini Projects repository where beginners and experts can learn and share their knowledge. It includes a collection of easy Python projects like dice rolling, dictionary, hangman game, tic-tac-toe, plotter, and more. The text also explains how to contribute to the project, including starring the repo, forking, cloning, creating feature branches, making pull requests, and updating the local repository. Additionally, it provides a table of contents detailing the aim of the project, contributing guidelines, README template, list of projects, and feedback section. You can find more details and guidelines on the project's GitHub repository page.",Python Mini Projects - Easy Python Small Projects to Improve Programming Skills,A collection of easy Python small projects to help you improve your programming skills. This project is designed for folks who are just getting started with Python principles and exploring GitHub as 'contributors.' Let's 'folk-ing' create amazing things together! Follow the steps to contribute and explore various mini projects from dice rolling to game creation.,"Explore a collection of easy Python small projects designed to help you improve your programming skills. Contribute, learn, and share knowledge from dice rolling stimulator to game creation. Let's 'folk-ing' create amazing things with these fun Python projects!",Python Learning Journey,320 stars this week,https://docs.github.com/assets/images/help/stars/starring-a-repository.png; https://upload.wikimedia.org/wikipedia/commons/3/38/GitHub_Fork_Button.png; https://i.ytimg.com/vi/rgbCcBNZcdQ/maxresdefault.jpg,,2140,2021-07-16T09:05:09Z
2024-02-28,https://github.com/Pythagora-io/gpt-pilot,https://raw.githubusercontent.com/Pythagora-io/gpt-pilot/main/README.md,"GPT Pilot is an AI developer that assists in coding, debugging, and creating apps by coding step by step, similar to real-life processes, while the developer oversees. It interacts with you, asks questions, sets up requirements, and helps with coding tasks. The VS Code extension offers a real AI developer companion. The aim is for AI to write most of the code for an app, requiring human intervention for only 5% of tasks until full AGI is achieved. GPT Pilot works collaboratively with developers, focusing on developing production-ready apps, with examples like a chat app and a markdown editor. Join their Discord for updates and contributions.",GPT Pilot: AI Developer Companion for Code Generation,"GPT Pilot is a cutting-edge AI developer that assists in writing code, debugging, and more. This innovative tool engages users in specifying the type of app they want to create, asking clarifying questions, creating technical requirements, setting up the environment, and coding the app step by step while allowing developers to review and intervene when necessary. GPT Pilot aims to explore the potential of leveraging AI, particularly GPT-4, to generate fully functional apps, emphasizing the need for developer oversight in the final stages for optimal results.","Discover how GPT Pilot, a true AI developer companion, streamlines the app development process by handling code generation and step-by-step coding under developer supervision. Explore the capabilities and workings of GPT Pilot, a tool designed to assist in writing production-ready apps while highlighting the essential role of developers in ensuring code quality and functionality.",AI Coding Assistant,470 stars this week,,https://www.youtube.com/watch?v=-OB6BJKADEo; https://www.youtube.com/watch?v=7t-Q2e7QsbE; https://www.youtube.com/watch?v=bUj9DbMRYhA; https://www.youtube.com/watch?v=uZeA1iX9dgg; https://www.youtube.com/watch?v=CMN3W18zfiE,21946,2023-08-16T11:56:07Z
2024-02-28,https://github.com/huggingface/transformers,https://raw.githubusercontent.com/huggingface/transformers/main/README.md,"The text discusses the Hugging Face Transformers Library, a state-of-the-art machine learning library for JAX, PyTorch, and TensorFlow. The library provides pre-trained models for various tasks such as text, vision, and audio processing. It offers APIs for downloading and using pretrained models, fine-tuning them, and sharing them with the community. The library is compatible with the three popular deep learning libraries (JAX, PyTorch, TensorFlow) and allows seamless integration between them. It also provides online demos and showcases multiple examples of using pretrained models for tasks like language processing, image analysis, and more. The text also covers the installation process for the library and introduces various model architectures available within the library.","Transformers: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow","ðŸ¤— Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. These models can be applied on various tasks like text classification, image recognition, and speech processing, supporting over 100 languages. The library offers APIs for easy model downloads, fine-tuning on custom datasets, and model sharing through the model hub. Built on JAX, PyTorch, and TensorFlow, ðŸ¤— Transformers enables seamless integration between deep learning frameworks, making it easy to train and deploy models.","Discover how ðŸ¤— Transformers library provides state-of-the-art pretrained models for text, vision, and audio tasks in multiple languages. Learn how to download, fine-tune, and share models using APIs for JAX, PyTorch, and TensorFlow.",Natural Language Processing.,572 stars this week,,,121022,2018-10-29T13:56:00Z
2024-02-28,https://github.com/s0md3v/roop,https://raw.githubusercontent.com/s0md3v/roop/main/README.md,"The text describes a project called Roop, which has been discontinued with no future updates. However, the software can still be used for face replacement in videos. The installation process requires technical skills and is not recommended for beginners. The software is designed to assist in tasks like character animation, but users are advised to use it responsibly and abide by local laws. The developers have taken measures to prevent inappropriate content use. The software uses third-party libraries and pre-trained models with their own licenses. Credits are given to deepinsight for their insightface project and to other developers whose libraries were used. You can refer to the provided documentation for more details.",Roop - Video Face Swapper and Enhancer Software with Ethical Guidelines,"This project Roop, a video face swapper tool, has been discontinued but still operational. The software won't receive updates but can replace faces in videos. Users are guided on installation through a helpful Discord community. Roop aims to positively impact the AI-generated media industry while enforcing ethical usage guidelines.",Roop is a powerful video face swapper and enhancer software that continues to work despite being discontinued. Learn how to install and use the tool through a helpful Discord community. Follow ethical guidelines to ensure responsible usage of the software for positive contributions to the AI-generated media industry.,Computer Vision,222 stars this week,,,24075,2023-05-28T14:37:54Z
2024-02-29,https://github.com/OpenCodeInterpreter/OpenCodeInterpreter,https://raw.githubusercontent.com/OpenCodeInterpreter/OpenCodeInterpreter/main/README.md,"OpenCodeInterpreter integrates code generation with execution and refinement, enhancing capabilities by combining large language models with sophisticated systems like GPT-4 Code Interpreter. The suite includes various models that have been open-sourced on Hugging Face. Data collection involves interactions and feedback from the Code-Feedback dataset to refine code dynamically. Evaluation employs frameworks like HumanEval and MBPP, with extended versions for comprehensive assessment. An open-source demo allows users to generate and execute code with a locally trained language model, providing automated feedback and adjusting code based on interactions. Detailed instructions are available to explore the demo and engage in chat-based interactions with the model. For inquiries, contact via email.",OpenCodeInterpreter: Enhancing Code Generation with Execution and Refinement,"OpenCodeInterpreter is a suite of open-source code generation systems that integrate execution and iterative refinement functionalities to enhance code generation capabilities, bridging the gap between large language models and proprietary systems like the GPT-4 Code Interpreter. The models within the OpenCodeInterpreter series have been open-sourced on Hugging Face, offering access to a range of models for code generation. Data Collection for OpenCodeInterpreter is supported by the Code-Feedback dataset, which features multi-turn interactions for dynamic code refinement. The evaluation framework of OpenCodeInterpreter utilizes HumanEval and MBPP methods, along with their extended versions, for a more comprehensive assessment. Additionally, an open-source demo is available for users to generate and execute code with the LLM, providing automated execution feedback and chat-based interactions with the model.","Explore how OpenCodeInterpreter enhances code generation by integrating execution and iterative refinement functionalities. Learn about the open-source models on Hugging Face, data collection with the Code-Feedback dataset, evaluation methods using HumanEval and MBPP, and experience the capabilities of the demo for generating and executing code. Reach out to the team for inquiries and get involved in enhancing code generation processes!",Language Models.,"Python





        935





        134


        Built by

          









        62 stars today",,,935,2024-02-19T14:43:38Z
2024-02-29,https://github.com/joaomdmoura/crewAI,https://raw.githubusercontent.com/joaomdmoura/crewAI/main/README.md,"The text is about **crewAI**, an advanced framework for managing autonomous AI agents collaboratively. It enables agents to work together on complex tasks by assuming roles, sharing goals, and operating seamlessly. The framework allows for installation and setup steps, creating agents with roles and goals, defining tasks, managing processes, and connecting agents to models like OpenAI or local models through tools. Key features include role-based agent design, autonomous task delegation, task management, saving output, parsing output, and compatibility with open-source models. CrewAI is compared to Autogen and ChatDev, highlighting its flexibility and adaptability. The text also covers contribution guidelines, hiring options, telemetry usage, and licensing.",Unlocking AI Collaboration with crewAI: A Cutting-Edge Framework for Agents,"crewAI is a cutting-edge framework designed to facilitate seamless collaboration among AI agents, enabling them to work together in a cohesive unit. Whether you're creating a smart assistant platform or a multi-agent research team, crewAI provides the foundation for sophisticated multi-agent interactions. With role-based agent design, autonomous delegation capabilities, and flexible task management, crewAI empowers agents to tackle complex tasks effectively. Explore how crewAI compares to other AI frameworks and learn how to connect your crew to different Language Model Models (LLMs). Join us in harnessing the power of collaborative intelligence with crewAI!","Discover crewAI, a state-of-the-art framework that enables AI agents to collaborate effectively. Learn about role-based agent design, autonomous delegation, and task management features. Find out how crewAI stands out among other AI frameworks and how to connect your crew to LLMs. Unleash the potential of collaborative intelligence with crewAI!",Collaborative AI Framework,"Python





        8,540





        953


        Built by

          









        86 stars today",https://raw.githubusercontent.com/joaomdmoura/crewAI/main/./docs/crewai_logo.png; https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg; https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg; https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg; https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg,https://www.youtube.com/watch?v=tnejrr-0a94; https://www.youtube.com/watch?v=u98wEMz-9to; https://www.youtube.com/watch?v=xis7rWp-hjs; https://www.youtube.com/watch?v=e0Uj4yWdaAg,8540,2023-10-27T03:26:59Z
2024-02-29,https://github.com/SciPhi-AI/R2R,https://raw.githubusercontent.com/SciPhi-AI/R2R/main/README.md,"The text introduces the R2R framework, designed for deploying robust RAG systems. R2R offers a semi-opinionated approach to simplify deployment, adaptation, and maintenance of RAG pipelines for production. It aims to enhance ease of use and effectiveness in the industry. The framework provides a quick install guide using pip and offers basic examples for application deployment and interaction. Further, it includes a demo for visual intelligence and provides detailed steps for a full install using Poetry. Key features include rapid deployment, flexible standardization, easy modification, versioning, extensibility, OSS community support, and deployment assistance. Core abstractions focus on Ingestion, Embedding, RAG, and Eval Pipelines, each supported by a logging database for observability.",R2R: Production-ready RAG Systems - Simplifying Deployment and Maintenance,"R2R is a semi-opinionated framework designed to bridge the gap between experimental RAG models and robust, production-ready systems. Offering a straightforward path to deploy, adapt, and maintain RAG pipelines in production, R2R prioritizes simplicity and practicality to set a new industry benchmark. With core abstractions focused on Ingestion, Embedding, RAG, and Evaluation Pipelines, it ensures rapid deployment, flexible standardization, and easy modification while supporting extensibility and versioning for reproducibility and traceability. Built for the OSS community, R2R facilitates quick integration with various VectorDBs, LLMs, and Embeddings Models, making it suitable for startups and enterprises seeking to build and deploy RAG systems end-to-end.","Explore R2R, a semi-opinionated RAG framework that simplifies the deployment and maintenance of production-ready systems. With core abstractions centered around Ingestion, Embedding, RAG, and Evaluation Pipelines, R2R offers rapid deployment, flexible standardization, and easy modification. Built for the OSS community, it supports extensibility and versioning, making it ideal for startups and enterprises looking to build and deploy RAG systems with ease.",Collaborative AI Framework.,"Python





        737





        50


        Built by

          









        158 stars today",https://raw.githubusercontent.com/SciPhi-AI/R2R/main/./docs/pages/getting-started/demo_screenshot.png,,738,2024-02-12T03:24:27Z
2024-02-29,https://github.com/myshell-ai/MeloTTS,https://raw.githubusercontent.com/myshell-ai/MeloTTS/main/README.md,"The text provides information on MeloTTS, a high-quality multi-lingual text-to-speech library by MyShell.ai. It supports various languages with examples provided for each language. The library includes features like support for mixed Chinese and English, as well as fast CPU real-time inference. Usage instructions are provided both for quick use without installation and for local installation. The text also mentions opportunities to join the community through open-source AI grants and contributing to the repository. The library is licensed under the MIT License, allowing both commercial and non-commercial use. Acknowledgements are given to the sources on which the implementation is based.",Enhance Text-to-Speech with MeloTTS Library by MyShell.ai,"MeloTTS is a high-quality multi-lingual text-to-speech library developed by MyShell.ai. It supports various languages such as American English, British English, Indian English, Australian English, Spanish, French, Chinese, Japanese, and Korean. The library also offers features like mixed Chinese and English support and quick CPU real-time inference. Join the community to contribute to open-source AI projects and explore the usage options provided in the documentation.","Discover how MeloTTS, a powerful text-to-speech library by MyShell.ai, supports multiple languages and advanced features like mixed Chinese and English support. Join the community to contribute to open-source AI projects and explore various usage options available.",Language Models,"Python





        633





        67


        Built by

          







        102 stars today",,,633,2024-02-19T16:49:14Z
2024-02-29,https://github.com/WongKinYiu/yolov9,https://raw.githubusercontent.com/WongKinYiu/yolov9/main/README.md,"The text discusses the implementation of YOLOv9, based on the paper ""YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information."" It provides performance metrics for different YOLOv9 models - YOLOv9-S, YOLOv9-M, YOLOv9-C, and YOLOv9-E on the MS COCO dataset. The models are evaluated in terms of Average Precision (AP) and Average Recall (AR) at various Intersection over Union (IoU) thresholds. It includes details on useful links, installation using Docker, evaluation with Python scripts, data preparation for training, single and multiple GPU training procedures, re-parameterization, citations, acknowledgments, and a teaser for YOLOR-Based Multi-Task Learning. The text also contains links to related repositories and code bases.",YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information,"Implementation of paper - YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information. This blogpost provides details on different YOLOv9 models (YOLOv9-S, YOLOv9-M, YOLOv9-C, YOLOv9-E) and their performance metrics based on MS COCO dataset. It includes useful links for custom training, ONNX export, TensorRT inference, C# inference, Hugging Face demo, CoLab demo, ONNXSlim export, YOLOv9 ByteTrack, YOLOv9 DeepSORT, YOLOv9 counting, and AnyLabeling tool. The blog also covers installation instructions, evaluation results, training processes, re-parameterization details, citation, teaser, and acknowledgements.","Learn about the YOLOv9 model, its variants, performance on MS COCO dataset, training processes, evaluation results, and useful links for custom training, export, inference, and deployment. Dive into re-parameterization, citation, acknowledgements, and get insights into YOLOR-Based Multi-Task Learning. Explore this comprehensive guide on implementing YOLOv9 for object detection tasks.",Computer Vision,"Python





        5,642





        678


        Built by

          





        959 stars today",,,5642,2024-02-18T10:09:29Z
2024-02-29,https://github.com/donnemartin/system-design-primer,https://raw.githubusercontent.com/donnemartin/system-design-primer/master/README.md,"The text is about the System Design Primer, a guide to help engineers learn how to design large-scale systems and prepare for system design interviews. It covers various topics such as scalability, load balancing, database management, and more. The guide emphasizes the importance of understanding system design principles and provides resources like Anki flashcards for retention. It also discusses concepts like CAP theorem, consistency patterns, and availability vs consistency trade-offs. The use of CDNs, load balancers, reverse proxies, and microservices are highlighted for scaling and improving system performance. Overall, it aims to help engineers build systems that can handle large loads efficiently.",Designing Large-Scale Systems: Scalability Principles and Patterns,"Learning how to design scalable systems will help you become a better engineer. System design is a broad topic with a vast amount of resources scattered throughout the web on system design principles. This organized collection of resources will help you learn how to build systems at scale. Whether you are preparing for a system design interview or looking to understand the complexities of large-scale systems, this blog post provides insights into scalability, availability, load balancing, and database management. Explore the trade-offs between performance vs scalability, consistency vs availability, and learn about key design patterns like master-slave replication, sharding, and more.","Learn how to design scalable systems with this comprehensive blog post covering key principles and patterns in system design. Explore trade-offs between performance and scalability, consistency and availability, and dive into important concepts like master-slave replication and sharding. Whether you are preparing for a system design interview or seeking to enhance your engineering skills, this resource provides valuable insights into building systems at scale.",System Design Education,"Python





        247,760





        42,526


        Built by

          









        125 stars today",https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/OfVllex.png; https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/4edXG0T.png; https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/jrUBAF7.png; https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/bWxPtQA.png; https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/V5q57vU.png; https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/cdCv5g7.png; https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/4j99mhe.png; https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/MzExP06.png; https://raw.githubusercontent.com/donnemartin/system-design-primer/master/images/jj3A5N8.png,https://www.youtube.com/watch?v=ZgdS0EUmn70; https://www.youtube.com/watch?v=-W9F__D3oY4; https://www.youtube.com/watch?v=k-Yaq8AHlFA; https://www.youtube.com/watch?v=kKjm4ehYiMs; https://www.youtube.com/watch?v=kKjm4ehYiMs; https://www.youtube.com/watch?v=kKjm4ehYiMs; https://www.youtube.com/watch?v=qI_g07C_Q5I; https://www.youtube.com/watch?v=kKjm4ehYiMs; https://www.youtube.com/watch?v=1KRYH75wgy4; https://www.youtube.com/watch?v=PE4gwstWhmc; https://www.youtube.com/watch?v=b1e4t2k2KJY; https://www.youtube.com/watch?v=PE4gwstWhmc; https://www.youtube.com/watch?v=5cKTP36HVgI; https://www.youtube.com/watch?v=z8LU0Cj6BOU; https://www.youtube.com/watch?v=w5WVu624fY8,247760,2017-02-26T16:15:28Z
2024-02-29,https://github.com/KillianLucas/open-interpreter,https://raw.githubusercontent.com/KillianLucas/open-interpreter/main/README.md,"The text describes the features of the Open Interpreter tool, which allows language models to run code locally. It highlights the ability to interact with the computer through natural language commands, such as creating and editing various files, controlling a browser, and analyzing datasets. The tool also provides a comparison to OpenAI's Code Interpreter and offers various commands and capabilities, including setting up local and online modes, customizing settings, starting a chat, and running a FastAPI server. The text emphasizes caution when running code and provides guidance on safety practices. Additionally, it encourages contributions from the community and outlines the project's roadmap.",Open Interpreter: Run Code with Language Models Locally,"Open Interpreter is a tool that lets language models run code locally, enabling users to interact with their computer's capabilities in a natural-language manner. With features like the Computer API and the `--os` flag, Open Interpreter introduces a new way to work with various tasks including creating and editing media, browsing, analyzing data, and more. Unlike hosted solutions, Open Interpreter runs in your local environment, providing full internet access and flexibility to use any package or library without limitations. The blogpost explains how Open Interpreter works, its interactive features, comparison to other tools, setup guides, safety measures, and the option to control it via HTTP REST endpoints.","Discover Open Interpreter, a local tool that enables language models to run code on your computer, providing full internet access and flexibility to execute tasks effortlessly. Learn about its features like the Computer API and interactive chat, compare it to other tools like Code Interpreter, understand its setup for different environments, and explore safety precautions to maintain control over code execution. See how Open Interpreter can enhance your workflow by harnessing the power of language models in a local development environment.",AI Coding Assistant,"Python





        41,213





        3,594


        Built by

          









        164 stars today",,,41213,2023-07-14T07:10:44Z
2024-02-29,https://github.com/binary-husky/gpt_academic,https://raw.githubusercontent.com/binary-husky/gpt_academic/master/README.md,"The text provides information about the latest version updates of the GPT Academic project. It introduces new features such as Mermaid for drawing diagrams, real-time voice input, support for various language models like ChatGLM and MOSS, and the ability to translate PDF and Arxiv papers. The project also includes a Void Terminal for executing functions through natural language input and supports custom shortcuts and function plugins. The development history and version updates are outlined, and users are encouraged to join the developer community for further learning and support.","GPT Academic Updates: Mermaid Charts, ChatGLM3 Support, and More","GPT Academic introduces new features in version 3.70, including support for Mermaid charts for brain mapping, ChatGLM3, and other Chinese models. The latest update also enhances AutoGen plugin and introduces a Void Terminal feature where you can interact with the system in natural language. Learn how to customize new convenient buttons with academic shortcuts and develop your own function plugins easily. The development history of GPT Academic showcases continuous improvements in UI design, integration of powerful functions, and support for various AI models.","Discover the latest GPT Academic updates with Mermaid charts, ChatGLM3 support, and a Void Terminal feature. Learn how to customize academic shortcuts and develop function plugins easily. Explore the evolution of GPT Academic with continuous enhancements and support for various AI models.",Language Models,"Python





        52,200





        6,622


        Built by

          









        66 stars today",,,52201,2023-03-20T09:05:13Z
2024-02-29,https://github.com/ronibandini/reggaetonBeGone,https://raw.githubusercontent.com/ronibandini/reggaetonBeGone/main/README.md,"The text describes a project called ""Reggaeton Be Gone"" that uses Machine Learning to detect reggaeton music and disable Bluetooth speakers. The project involves Raspberry Pi 3, DFRobot Oled 128x32 screen, push button, BT Audio Receiver 5.0, and jumper cables. The Machine Learning model is trained using the Edge Impulse platform. The full instructions for the project are available on Hackster.io. The connections for the components are detailed, and the project uses a specific Oled screen font. The creator of the project is Roni Bandini, who can be contacted on Twitter. Full details can be found at the provided links.",How to Detect Reggaeton with Machine Learning and Disable Bluetooth Speakers Tutorial,"Learn how to detect reggaeton music using Machine Learning and disable Bluetooth speakers with a Raspberry Pi 3 and DFRobot components. The model is trained using the Edge Impulse platform, and complete instructions can be found on Hackster.io. Connect the DFRobot OLED screen and push button to the Raspberry Pi GPIO pins for this fun project.",Discover how to identify reggaeton music genre using Machine Learning and block Bluetooth speakers using a Raspberry Pi and DFRobot components. Get step-by-step instructions on setting up the project. Dive into the world of edge computing and music detection in this exciting tutorial.,Machine Learning Music_detection,"Python





        331





        34


        Built by

          





        64 stars today",,,331,2024-02-20T21:10:05Z
2024-02-29,https://github.com/joaomdmoura/crewAI-examples,https://raw.githubusercontent.com/joaomdmoura/crewAI-examples/main/README.md,"The text provides examples for utilizing crewAI to enhance the collaboration of role-playing AI agents. It showcases different applications of the crewAI framework for automating various processes. The examples are categorized into Basic and Advanced Examples, including tasks such as creating job postings, trip planning, Instagram post creation, markdown validation, game generation, and utilizing Azure OpenAI API. There is also mention of starting your own example using the provided starter template. Advanced examples cover areas like stock analysis, landing page generation, and integrating crewAI with LangGraph. The text serves as a resource for users interested in leveraging crewAI for AI agent collaboration.",Examples of Using crewAI Framework for AI Automation | joaomdmoura,"crewAI is a tool created to enhance the cooperation among AI role-playing agents, providing a framework to automate various processes. The blog by [@joaomdmoura](https://x.com/joaomdmoura) showcases a range of examples demonstrating the versatility of crewAI. From fundamental tasks like job posting and trip planning to advanced projects such as stock analysis and landing page generation, the blog illustrates diverse applications of crewAI in AI automation.",Explore a collection of examples showcasing the application of crewAI framework for AI automation. Learn how to automate tasks with crewAI - from basic ones like job posting and trip planning to advanced projects including stock analysis and landing page generation. Read more at the blog by [@joaomdmoura](https://x.com/joaomdmoura).,Collaborative AI Framework.,"Python





        857





        236


        Built by

          









        19 stars today",,,857,2023-12-19T11:46:48Z
2024-02-29,https://github.com/MrMimic/data-scientist-roadmap,https://raw.githubusercontent.com/MrMimic/data-scientist-roadmap/master/README.md,"The text discusses a data science skills roadmap created by Swami Chandrasekaran, shared on his blog. It highlights the increasing popularity of data science jobs and the availability of tutorials to guide individuals interested in learning about this field. The roadmap emphasizes the use of Wikipedia and LLMs for resources and encourages collaboration through forking the repository and making pull requests. The guidelines include commenting on code, maintaining a specific file structure, and sharing helpful links in README files. Overall, the text presents a structured approach for beginners to start their journey in data science.",Ultimate Data Scientist Roadmap for Aspiring Data Science Professionals,"I just found this data science skills roadmap, drawn by Swami Chandrasekaran on his cool blog. Jobs linked to data science are becoming more and more popular. A bunch of tutorials could easily complete this roadmap, helping whoever wants to start learning stuff about data science. For the moment, a lot is got on Wikipedia or generated by LLMs (except for codes, always handmade). Any help's thus welcome!","Discover the ultimate data scientist roadmap created by Swami Chandrasekaran to guide aspiring data science professionals. Explore the growing popularity of data science jobs and the resources available to start learning about data science. Join the community, contribute, and enhance your skills in this exciting field.",Data Science Learning,"Python





        6,704





        1,859


        Built by

          









        29 stars today",http://nirvacana.com/thoughts/wp-content/uploads/2013/07/RoadToDataScientist1.png,,6704,2017-06-05T06:30:08Z
2024-02-29,https://github.com/state-spaces/mamba,https://raw.githubusercontent.com/state-spaces/mamba/main/README.md,"The text discusses Mamba, a linear-time sequence modeling architecture based on selective state spaces. This model is designed for dense data like language modeling and is more efficient than previous subquadratic models. It utilizes a structured state space model approach and has efficient hardware-aware design similar to FlashAttention. Installation requirements include specific PyTorch versions, CUDA, and NVIDIA GPU. The interface exposes features like Selective SSM layer and Mamba Block. Pretrained models are available, trained on the Pile dataset. Evaluations and benchmarks are provided to analyze model performance. Troubleshooting tips and a citation are also included.",Mamba: Linear-Time Sequence Modeling with Selective State Spaces - Overview and Installation Guide,"Mamba is a novel state space model architecture designed for information-dense data like language modeling. It offers efficient hardware-aware design, with performance surpassing traditional subquadratic models. The blog post covers the model's structure, installation process, usage examples, and pre-trained models available. It also provides guidance on evaluations, troubleshooting tips, and a citation reference.","Learn about Mamba, a state space model architecture focused on language modeling efficiency. This blog post includes installation instructions, usage examples, pre-trained model details, evaluations, troubleshooting tips, and a citation guide for referencing the work.",Language Models,"Python





        7,037





        553


        Built by

          









        72 stars today",,,7038,2023-12-01T01:17:39Z
2024-02-29,https://github.com/Azure/PyRIT,https://raw.githubusercontent.com/Azure/PyRIT/main/README.md,"The Python Risk Identification Tool (PyRIT) is an automation framework created to aid security professionals and ML engineers in assessing the robustness of their AI models against various types of harmful content such as bias, harassment, and fabrication. PyRIT automates tasks related to AI red teaming, enabling operators to focus on complex tasks while identifying security and privacy risks like malware generation and identity theft. Researchers can use PyRIT to establish a performance baseline for their models, track improvements, and enhance mitigations against different harms. Microsoft is utilizing PyRIT for product iterations and protection against prompt injection attacks. More information on PyRIT can be found on Microsoft Learn and in the project's documentation.",Python Risk Identification Tool for generative AI (PyRIT) - Empowering Security Professionals and ML Engineers,"The Python Risk Identification Tool for generative AI (PyRIT) is a powerful automation framework developed to enhance the assessment of model robustness. PyRIT assists in identifying and combating various security harms such as fabrication, misuse, and prohibited content. By automating AI Red Teaming tasks, PyRIT enables researchers to focus on complex assignments and detect security and privacy vulnerabilities. Researchers can utilize PyRIT to establish a performance baseline and enhance mitigation strategies for different harm categories.","Learn how PyRIT, a Python Risk Identification Tool for generative AI, empowers security professionals and ML engineers to assess model robustness. Automate AI Red Teaming tasks, focus on complex assignments, and detect security and privacy vulnerabilities effectively with PyRIT.",AI Red Teaming,"Python





        945





        180


        Built by

          









        189 stars today",https://github.com/Azure/PyRIT/blob/main/assets/pyrit_architecture.png,,945,2023-12-12T15:46:28Z
2024-02-29,https://github.com/prowler-cloud/prowler,https://raw.githubusercontent.com/prowler-cloud/prowler/master/README.md,"The text provides information about Prowler, an Open Source security tool for assessing and monitoring security practices on AWS, GCP, Azure, and Kubernetes. It covers multiple compliance frameworks and categories, offering detailed checks for each provider. The tool can be installed via Pip package, containers, or Github. It requires proper authentication and permissions for AWS, Azure, and Google Cloud Platform. Prowler generates reports in various formats and allows users to customize checks configurations. Detailed usage instructions for executing specific checks/services and modifying configurations are provided. Prowler is licensed under Apache License 2.0.",Enhance Cloud Security with Prowler's Dynamic Assessments,"""Prowler SaaS and Prowler Open Source are as dynamic and adaptable as the environment theyâ€™re meant to protect. Trusted by the leaders in security. Learn more at prowler.com. Prowler is an Open Source security tool to perform AWS, GCP and Azure security best practices assessments, audits, incident response, continuous monitoring, hardening and forensics readiness. It contains hundreds of controls covering various compliance frameworks and categories from AWS, GCP, Azure, and Kubernetes. The full documentation can now be found at https://docs.prowler.com/projects/prowler-open-source/en/latest/.""","Learn how Prowler, an Open Source security tool, can enhance the security of your cloud infrastructure with dynamic assessments, best practices audits, incident response, and more. Discover hundreds of controls covering various compliance frameworks and categories. Find out more at prowler.com.",Cybersecurity Tool,"Python





        9,219





        1,340


        Built by

          









        15 stars today",,,9219,2016-08-24T15:12:24Z
2024-02-29,https://github.com/mistralai/client-python,https://raw.githubusercontent.com/mistralai/client-python/main/README.md,"The Mistral Python Client is developed based on the cohere-python project. You can easily interact with the Mistral AI API by installing the client using pip. The client relies on `poetry` for managing dependencies and setting up a virtual environment. To run the examples provided in the `examples/` directory, you can use `poetry run` or enter the virtual environment with `poetry shell`. To use the client, you need to obtain a Mistral API key, set it as an environment variable, and then you can run the examples like `python chat_no_streaming.py`. The integration provides an easy way to leverage the Mistral AI capabilities through Python scripting.",How to Use the Mistral Python Client for Mistral AI API,"Learn how to interact with Mistral AI API using the Mistral Python client, inspired by cohere-python. Install the client through pip or from source using poetry. Set up your API key and run examples from the provided directory using poetry run or poetry shell.","Discover how to leverage the Mistral Python client to access Mistral AI API with ease. Follow step-by-step instructions to install the client, set up your API key, and run examples seamlessly. Enhance your AI projects today!",AI Python Client,"Python





        322





        45


        Built by

          









        14 stars today",,,322,2023-12-07T10:09:51Z
2024-02-29,https://github.com/wagtail/wagtail,https://raw.githubusercontent.com/wagtail/wagtail/main/README.md,"Wagtail is an open source content management system built on Django. It offers precise control for designers and developers, with features like a fast interface, control over design, scalability, content API, powerful search, and multi-site readiness. It runs on Python 3 and supports multiple platforms. Organizations like NASA, Google, and more use Wagtail. The full documentation for Wagtail is available at docs.wagtail.org. There is active community support on Stack Overflow and Slack, and commercial support is provided by Torchbox. Security is taken seriously, and Wagtail follows a strict release schedule with regular updates. The project is licensed under BSD.",Introducing Wagtail: A Powerful open-source CMS built on Django,"Wagtail is an open source content management system built on Django, with a strong community and commercial support. It's focused on user experience, and offers precise control for designers and developers. Features include fast and attractive interface, complete control over front-end design, scalability, powerful search, and multi-site readiness. Wagtail is trusted by organizations like NASA, Google, and more. Explore more at wagtail.org.","Discover Wagtail, a user-friendly open-source CMS built on Django with features like fast interface, powerful search, and multi-site readiness. Trusted by organizations like NASA and Google. Learn more at wagtail.org.",Software Development,"Python





        16,849





        3,587


        Built by

          









        15 stars today",https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/wagtail-screenshot-with-browser.png; https://raw.githubusercontent.com/wagtail/wagtail/main/.github/install-animation.gif; https://raw.githubusercontent.com/wagtail/wagtail/main/.github/join-slack-community.png; https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/assistivlabs-logo.png,,16849,2014-02-03T12:41:59Z
2024-02-29,https://github.com/gpt-engineer-org/gpt-engineer,https://raw.githubusercontent.com/gpt-engineer-org/gpt-engineer/main/README.md,"GPT-Engineer is an AI-powered tool that allows users to specify software requirements in natural language and then generates and executes the corresponding code. Users can also ask the AI to make improvements to the code. The tool supports Python versions 3.10 to 3.12 and offers various installation options. It requires an API key for operation and provides different ways to run the tool, including using Docker. The project aims to maintain coding tools for building AI agents and encourages collaboration within the open-source community. Users interested in contributing can refer to the roadmap and join the Discord community for guidance on how to get involved.",Automating Software Development with GPT-Engineer: A Comprehensive Guide,"GPT-engineer is a powerful tool that allows you to specify software in natural language and witness an AI write and execute the code for you. With GPT-Engineer, you can easily ask the AI to implement improvements, making your development process more efficient and productive. Learn how to get started by installing GPT-engineer and setting up your API key. Explore various ways to run GPT-Engineer, create new code, and improve existing projects. Join the gpt-engineer community to contribute and be a part of the open-source mission.","Discover the power of GPT-Engineer in automating software development. Learn how to specify software in natural language, let AI write and execute code, and implement improvements effortlessly. Join the open-source community, get started with installation, and leverage the tool for enhanced coding experiences.",AI Coding Assistant,"Python





        49,317





        6,392


        Built by

          









        30 stars today",,,49317,2023-04-29T12:52:15Z
2024-02-29,https://github.com/Clouditera/SecGPT,https://raw.githubusercontent.com/Clouditera/SecGPT/main/README.md,"The text introduces SecGPT, a large model aimed at incorporating artificial intelligence technology into the field of cybersecurity to enhance network defense efficiency and effectiveness. SecGPT can be used for various cybersecurity tasks such as vulnerability analysis, trace analysis, traffic analysis, threat assessment, command interpretation, and cybersecurity knowledge Q&A. It features self-training code for memory savings, high-quality cybersecurity training sets, DPO reinforcement learning, and unrestricted GPT modeling for in-depth analysis. The model is open-source, providing training codes and datasets for users to train their own large-scale cybersecurity models. Users are advised to carefully evaluate and use the generated content when utilizing the model.",Exploring SecGPT: Advancing Network Security with Large Models,"SecGPT aims to bring AI technology into the field of network security to enhance defense efficiency and effectiveness. It serves as a foundational security model for various network security tasks, such as vulnerability analysis, forensics, traffic analysis, attack assessment, command interpretation, and cybersecurity knowledge Q&A. Unlike other open-source models, SecGPT offers unique features like self-written training code for memory savings, high-quality security training datasets, DPO reinforcement learning, and ethical unrestricted analysis capabilities.","Discover how SecGPT leverages AI technology in network security, contributing to better cybersecurity defenses. Learn about its applications in vulnerability analysis, forensics, traffic analysis, and more. Uncover the unique features of SecGPT like self-written training code, high-quality datasets, and DPO reinforcement learning.",Cybersecurity Tool,"Python





        807





        111


        Built by

          







        23 stars today",https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/640.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/641.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/6402.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/640%203.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/640%204.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/640%205.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/6406.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/640%207.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/640%208.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/640%209.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/61.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/62.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/63.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/64.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/image-2.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/image-3.png; https://raw.githubusercontent.com/Clouditera/SecGPT/main/images/image-4.png,,807,2023-11-20T05:41:24Z
2024-02-29,https://github.com/rany2/edge-tts,https://raw.githubusercontent.com/rany2/edge-tts/master/README.md,"`edge-tts` is a Python module that enables the use of Microsoft Edge's online text-to-speech service. It provides commands like `edge-tts` and `edge-playback` for Python developers to integrate this service into their code. The installation can be done via pip or pipx. Basic usage involves generating speech using specified text and output options. One can also change the voice, adjust speech rate, volume, and pitch. The `edge-playback` command facilitates immediate playback of generated speech. Custom SSML support has been discontinued. The module can be directly used in Python scripts for various applications as demonstrated in the provided examples.",Using Microsoft Edge Online Text-to-Speech with edge-tts Python Module,"`edge-tts` is a Python module that enables utilizing Microsoft Edge's online text-to-speech service directly in Python code or via the `edge-tts` and `edge-playback` commands. To install, use `pip` or `pipx` for command line usage. Explore various options like changing voices, adjusting rate/volume/pitch, and utilizing the Python module directly. Learn more about the commands, voice customization, and usage examples in the blogpost.","Learn how to integrate Microsoft Edge's online text-to-speech service into your Python projects with the `edge-tts` module. Install and use the commands, customize voices, adjust speech characteristics, and discover Python module usage examples in this comprehensive guide.",AI Python Client,"Python





        2,905





        313


        Built by

          









        20 stars today",,,2905,2021-05-10T18:55:14Z
2024-02-29,https://github.com/Fanghua-Yu/SUPIR,https://raw.githubusercontent.com/Fanghua-Yu/SUPIR/master/README.md,"The text discusses a project called SUPIR, focusing on model scaling for photo-realistic image restoration. The project involves a team from various institutions and labs, working on enhancing image quality in real-world settings. It emphasizes high RAM and VRAM costs, requiring an online demo. The process involves cloning the repository, installing dependencies, and downloading checkpoints. Different models are provided for training settings. The text explains usage instructions for SUPIR, including quick inference and Python script examples. It also mentions an upcoming online demo. Additionally, it provides contact information and a declaration for non-commercial use of the software.",Scaling Up to Excellence: Model Scaling for Photo-Realistic Image Restoration - CVPR2024,"The blog post discusses the practice of model scaling for photo-realistic image restoration in the wild, focusing on the SUPIR project. It covers the dependencies and installation steps, including cloning the repository, installing dependent packages, and downloading checkpoints. The post also provides information on dependent models, custom path editing for checkpoints, quick inference methods, usage of SUPIR, and Python scripts for different scenarios. Additionally, it mentions online demo availability and includes BibTeX citation for reference.","Explore the practice of model scaling for photo-realistic image restoration in the wild with the SUPIR project. Learn about installation steps, dependent models, quick inference methods, Python scripts, and more. Check out the BibTeX citation and ways to contact the developers for non-commercial use. Stay tuned for the online demo release!",Computer Vision,"Python





        1,830





        129


        Built by

          





        71 stars today",https://github.com/Fanghua-Yu/SUPIR/blob/master/assets/teaser.png,,1830,2023-12-21T11:23:35Z
2024-02-29,https://github.com/LiheYoung/Depth-Anything,https://raw.githubusercontent.com/LiheYoung/Depth-Anything/main/README.md,"The text provides an overview of ""Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data."" The project aims to enhance robust monocular depth estimation by training on a mix of 1.5M labeled images and over 62M unlabeled images. Noteworthy features include relative depth estimation, metric depth estimation with strong capabilities, a better depth-conditioned ControlNet, and downstream high-level scene understanding. The text showcases the project's performance compared to the earlier MiDaS model, highlighting key metrics. Pre-trained models are offered in different scales for relative depth estimation. The text also mentions details on installation, running the project, and resources for further community support. It concludes with acknowledgments and a citation request.",Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data,"This work presents Depth Anything, a highly practical solution for robust monocular depth estimation by training on a combination of 1.5M labeled images and 62M+ unlabeled images. It emphasizes on features like Relative depth estimation, Metric depth estimation, Better depth-conditioned ControlNet, and Downstream high-level scene understanding. The blogpost also includes a comparison of Depth Anything performance with the MiDaS model, information on pre-trained models, installation instructions, running details, and community support acknowledgements.","Discover Depth Anything, a powerful solution for monocular depth estimation using a mix of labeled and unlabeled data. Explore its features, performance comparisons, pre-trained models, installation instructions, and community support. Unleash the potential of large-scale unlabeled data with Depth Anything.",Self-Supervised Learning Architecture.,"Python





        4,807





        318


        Built by

          






        34 stars today",https://raw.githubusercontent.com/LiheYoung/Depth-Anything/main/assets/teaser.png,,4807,2024-01-22T01:09:25Z
2024-02-29,https://github.com/Eladlev/AutoPrompt,https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/README.md,"The text provides an overview of the AutoPrompt framework, a tool designed to enhance and refine prompts for real-world applications. AutoPrompt employs a calibration process to generate high-quality prompts tailored to user intentions, addressing issues such as prompt sensitivity and ambiguity. The system is applicable to tasks like moderation and content generation, with the ability to optimize prompts efficiently. The framework supports various open-source tools and LLM providers for flexible integration. Users can follow the setup instructions to configure their system, set budget limits, and run prompt optimization pipelines. The project is open for contributions and is licensed under Apache 2.0. For further details, the full text and references are provided in the document.",Enhance Prompt Engineering with AutoPrompt: A Framework for Optimizing Prompts,"Auto Prompt is a prompt optimization framework designed to enhance and perfect prompts for real-world use cases. The framework generates high-quality prompts tailored to user intentions through a sophisticated calibration process. Addressing prompt sensitivity and ambiguity issues, Auto Prompt empowers users to create robust prompts with minimal effort. The system implements an Intent-based Prompt Calibration method, refining prompts based on user-provided inputs and task descriptions. By combining synthetic data generation and prompt optimization, Auto Prompt outperforms traditional methods while ensuring efficient performance enhancements.","Learn how AutoPrompt, a prompt optimization framework, refines and perfects prompts for real-world scenarios. Empower users to create robust prompts with minimal effort and address prompt sensitivity and ambiguity. Discover how AutoPrompt's Intent-based Prompt Calibration method improves prompt performance and generates high-quality prompts tailored to user intentions.",Collaborative AI Framework.,"Python





        897





        62


        Built by

          









        176 stars today",https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/./docs/AutoPrompt_Diagram.png; https://raw.githubusercontent.com/Eladlev/AutoPrompt/main/./docs/autoprompt_recording.gif,,897,2023-12-02T18:45:14Z
2024-02-29,https://github.com/Avaiga/taipy,https://raw.githubusercontent.com/Avaiga/taipy/master/README.md,"Taipy is an open-source Python library for application development that integrates data and AI algorithms into production-ready web apps. It features what-if analyses, smart pipeline execution, built-in scheduling, and deployment tools. Designed for data scientists and machine learning engineers, Taipy enables building full-stack apps without the need for learning additional languages or frameworks. It includes a Python-based UI framework, pre-built components for data pipelines, scenario and data management features, version management, and pipeline orchestration tools. The tool allows users to easily create scenarios and manage data, such as filtering movie data based on selected genres. Taipy Cloud offers easy deployment options, and there are opportunities for contributing to the project.",Developing Production-Ready Web Apps with Taipy: Data and AI Algorithms in Action,"Taipy is an open-source Python library designed for easy, end-to-end application development, emphasizing what-if analyses, smart pipeline execution, built-in scheduling, and deployment tools. It enables data scientists and ML engineers to build full-stack apps without needing to learn new languages or frameworks, focusing on Data and AI algorithms. With features like Python-Based UI Framework, Pre-Built Components for Data Pipelines, Scenario and Data Management, and Version Management, Taipy simplifies the process of building production-ready web applications. Discover how to filter movie data based on genre using Taipy, create full-stack applications, and deploy your Taipy applications effortlessly with Taipy Cloud.","Learn how to leverage Taipy, an open-source Python library, to develop production-ready web applications with data and AI algorithms. Discover features like Python-Based UI Framework, Pre-Built Components for Data Pipelines, Scenario and Data Management, and Version Management. Explore a practical demo on filtering movie data by genre using Taipy and deployment options with Taipy Cloud.",Collaborative AI Framework.,"Python





        6,515





        410


        Built by

          









        361 stars today",https://github.com/Avaiga/taipy/raw/develop/readme_img/readme_demo_studio.gif; https://github.com/Avaiga/taipy/raw/develop/readme_img/readme_cloud_demo.gif,,6515,2022-02-18T15:55:45Z
2024-02-29,https://github.com/microsoft/unilm,https://raw.githubusercontent.com/microsoft/unilm/master/README.md,"The text provides information about large-scale self-supervised pre-training across different tasks, languages, and modalities. It includes details on hiring opportunities, foundation architectures like TorchScale, and various foundation models such as Foundation Transformers and Length-Extrapolatable Transformers. It also mentions model architectures like BitNet, RetNet, and LongNet, as well as applications in language understanding, generation, image analysis, speech, and multimodal tasks. Additionally, it highlights recent releases, model advancements, and links to relevant repositories and resources. For further details, you can refer to the full text or contact the provided email address for inquiries.",Revolutionizing Foundation Models and Architectures: A Deep Dive into TorchScale and New Innovations,"Discover the latest advancements in foundation models and architectures with TorchScale, a library focusing on modeling generality, stability, and efficiency. Explore cutting-edge technologies like DeepNet, Foundation Transformers, Length-Extrapolatable Transformers, and X-MoE. Uncover the groundbreaking BitNet, RetNet, and LongNet in the realm of model architecture revolution. Delve into the evolution of Multimodal LLM with models like Kosmos and MetaLM, enabling general-purpose modeling across various modalities.","Learn about the latest innovations in foundation models and architectures with TorchScale and explore technologies like DeepNet, Foundation Transformers, and X-MoE. Discover advancements such as BitNet, RetNet, and LongNet, and delve into the domain of Multimodal LLM with models like Kosmos and MetaLM.",Self-Supervised Learning Architecture.,"Python





        17,372





        2,261


        Built by

          









        95 stars today",,,17372,2019-07-23T04:15:28Z
2024-02-29,https://github.com/pytorch/examples,https://raw.githubusercontent.com/pytorch/examples/main/README.md,"The text provides information about the PyTorch Examples repository, showcasing various examples using PyTorch. The repository aims to offer curated, high-quality examples with minimal dependencies for diverse use cases. It includes models for image classification, natural language processing, generative models, reinforcement learning, neural style transfer, and more. Aside from the core examples within the repo, it also suggests external repositories for additional models. The text also mentions related resources like tutorials, model hub, production recipes, and support channels. It encourages contributions and provides guidelines for anyone willing to contribute examples or fix issues. The examples cover a wide range of applications and learning scenarios.",Explore PyTorch Examples for High-Quality Deep Learning Models,"`pytorch/examples` is a repository showcasing examples of using PyTorch with curated, high-quality models and diverse applications such as image classification, language modeling, and reinforcement learning. Find tutorials, ready-to-use models, and contributions guidelines for your own examples. Discover advanced techniques like generative adversarial networks and variational auto-encoders implemented in PyTorch.","Discover a curated repository of high-quality PyTorch examples showcasing deep learning models for various tasks like image classification, language modeling, and reinforcement learning. Explore tutorials, pre-trained models, and guidelines for contributing your own examples to the PyTorch community.",Deep Learning Platform,"Python





        21,466





        9,398


        Built by

          









        4 stars today",,,21466,2016-08-24T03:12:48Z
2024-02-29,https://github.com/qnguyen3/chat-with-mlx,https://raw.githubusercontent.com/qnguyen3/chat-with-mlx/main/README.md,"The text describes a repository featuring a Retrieval-augmented Generation (RAG) chat interface that supports various open-source models. It allows users to chat using different types of data like doc, pdf, txt, and YouTube videos. The installation can be done via Pip or manually through Git and Conda. Users can add their own models by configuring a .yaml file. The MLX framework mentioned supports machine learning research on Apple silicon, providing familiar APIs, lazy computation, dynamic graph construction, and more. The text acknowledges the Apple Machine Learning Research team, LangChain, ChromaDB, and other teams for their contributions. The repository's star history chart is also provided.",Native RAG on MacOS and Apple Silicon with MLX ðŸ§‘â€ðŸ’» | Chat with MLX,"This repository showcases a Retrieval-augmented Generation (RAG) chat interface with support for multiple open-source models. Chat with your Data: doc(x), pdf, txt and YouTube video via URL. Multilingual support. Easy integration with HuggingFace and MLX Compatible Open-Source Models. Installation and usage instructions for both Pip and Conda. Add your own models using provided solutions. Known issues and tips for using the MLX chat app. Reasoning behind MLX framework and acknowledgements.","Discover how to use Retrieval-augmented Generation (RAG) chat interface on MacOS and Apple Silicon using MLX. Chat with MLX supports various open-source models and allows easy integration with HuggingFace models. Learn how to install and use the chat app, add your own models, and understand the benefits of MLX framework. Find tips for resolving known issues and explore acknowledgements in the MLX community.",Natural Language Processing,"Python





        433





        37


        Built by

          







        180 stars today",https://raw.githubusercontent.com/qnguyen3/chat-with-mlx/main/assets/chat-w-mlx.gif,,433,2024-02-16T13:59:06Z
2024-02-29,https://github.com/NUS-HPC-AI-Lab/OpenDiT,https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/README.md,"OpenDiT is an open-source project focusing on enhancing the efficiency of training and inference for Diffusion Transformers (DiT) applications, such as text-to-video and text-to-image generation. It offers performance boosts through techniques like kernel optimization, hybrid parallelism, and FastSeq for large sequences. OpenDiT provides an easy-to-use platform with a complete pipeline for various tasks. The installation process involves setting up prerequisites, installing ColossalAI, and OpenDiT, along with optional speed-up libraries. The usage guide covers training and inference for both image and video tasks, with detailed commands and options. FastSeq, a novel sequence parallelism method, is introduced to optimize training for DiT models. Additionally, reproducibility results and acknowledgements are provided in the documentation. The codebase is available on GitHub for contributions, and citation information is provided for referencing the project.",OpenDiT: A High-Performance System for DiT Training and Inference,"OpenDiT is an open-source project designed to enhance the efficiency of training and inference for Diffusion Transformer applications, such as text-to-video and text-to-image generation. It offers performance boosts through techniques like speed optimizations, hybrid parallelism methods, and ease of use. The system also provides a complete pipeline for text-to-image and text-to-video generation, making it easy for researchers and engineers to adapt for real-world applications. Stay tuned for more features and updates!","Explore OpenDiT, an open-source project tailored for efficient training and inference of Diffusion Transformers. Discover techniques like speed optimizations and hybrid parallelism methods to enhance performance. Easily create text-to-image and text-to-video applications with OpenDiT's complete pipeline. Join us on GitHub and get ready for upcoming features!",Collaborative AI Framework.,"Python





        514





        22


        Built by

          









        154 stars today",https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/./figure/fastseq_overview.png; https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/./figure/fastseq_exp.png; https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/./figure/dit_results.png; https://raw.githubusercontent.com/NUS-HPC-AI-Lab/OpenDiT/master/./figure/dit_loss.png,,514,2024-02-17T08:40:35Z
2024-02-29,https://github.com/evo-design/evo,https://raw.githubusercontent.com/evo-design/evo/main/README.md,"Evo is a biological foundation model designed for long-context modeling and design, utilizing the StripedHyena architecture for sequence modeling at a single-nucleotide, byte-level resolution. It boasts 7 billion parameters and is trained on the OpenGenome dataset. To use Evo, one can install it via `pip` or GitHub source, after ensuring the correct PyTorch version. Various example scripts demonstrate Evo's capabilities, including generating sequences, folding proteins, and scoring log-likelihoods. Evo is integrated with HuggingFace and will soon be accessible through an API by TogetherAI. Please refer to the provided citation when mentioning Evo.",Sequence Modeling and Design with Evo: A Molecular to Genome Scale Approach,"Evo is a revolutionary biological foundation model with 7 billion parameters trained on the OpenGenome dataset. It enables long-context modeling and design using the StripedHyena architecture, offering near-linear scaling of compute and memory relative to context length. The model comes with checkpoints like 'evo-1-8k-base' and 'evo-1-131k-base' for different tasks, providing users with flexibility and efficiency. Evo supports various operations from sequence generation to scoring log-likelihoods, making it a versatile tool for molecular and genome-scale applications. Explore Evo's capabilities through HuggingFace integration and stay tuned for its upcoming availability via the TogetherAI API.","Discover Evo, a cutting-edge biological foundation model designed for sequence modeling and design from molecular to genome scale. Learn about the model's 7 billion parameters, training on OpenGenome data, and use of the StripedHyena architecture. Explore checkpoints like 'evo-1-8k-base' and 'evo-1-131k-base', along with examples of using Evo for tasks such as sequence generation and log-likelihood scoring. Integrated with HuggingFace and soon available through the TogetherAI API, Evo offers a wide range of functionalities for biological research and application.",Language Models,"Python





        332





        22


        Built by

          








        123 stars today",https://raw.githubusercontent.com/evo-design/evo/main/evo.jpg,,332,2024-02-17T19:11:33Z
2024-02-29,https://github.com/pygments/pygments,https://raw.githubusercontent.com/bruin-data/ingestr/main/README.md,"Ingestr is a command-line application that simplifies data ingestion from any source to any destination without writing code. It offers features such as copying data, incremental loading options (append, merge, delete+insert), and single-command installation. Users can easily transfer data from sources like Postgres, BigQuery, Snowflake, Redshift, Databricks, and more to various destinations. The tool eliminates backend management complexity and coding requirements, allowing users to run commands to move data efficiently. To get started, users can install Ingestr via pip and follow a quickstart guide to ingest data from a source to a destination effortlessly. Furthermore, the Ingestr project acknowledges the contributions of the SQLAlchemy and dlt teams for their support in connecting to different data sources and destinations. More details are available in the documentation and the community Slack channel.",Ingest Data Easily with Ingestr: Code-Free Data Transfer Tool,"Ingestr is a command-line application that simplifies data ingestion from any source to any destination without the need for writing code. With features like incremental loading and single-command installation, Ingestr streamlines the data transfer process. Simply install Ingestr using 'pip install ingestr' and follow a quickstart command to start ingesting data effortlessly.","Discover Ingestr, a code-free data ingestion tool that allows seamless copying of data between various sources and destinations. Learn about the easy installation process and how to use Ingestr for your data transfer needs.",Data Ingestion Tool.,"Python





        1,665





        604


        Built by

          









        15 stars today",,,1180,2019-08-31T15:46:03Z
2024-02-29,https://github.com/bruin-data/ingestr,https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/README.md,"HelloGitHub is a platform that shares interesting and beginner-friendly open-source projects on GitHub through monthly updates in a magazine format. The content includes interesting projects, open-source books, practical projects, enterprise-level projects, and more to help people experience the charm of open-source and develop a love for it quickly. Readers can enjoy a better reading experience on the official website or HelloGitHub public account. The platform welcomes project recommendations to become contributors. It is also sponsored by various companies to support its activities. The text is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",Discover Interesting Open Source Projects with HelloGitHub,"HelloGitHub is a platform that shares interesting and beginner-friendly open source projects on GitHub, updated monthly on the 28th. It includes a variety of content such as fun projects, open source books, practical projects, and enterprise-level projects, allowing you to quickly experience the charm of open source and fall in love with it. For a better reading experience, visit the official website or check out HelloGitHub's public account.","Explore the world of open source projects with HelloGitHub. Get insights into fun and beginner-friendly projects, open source books, practical projects, and more. Experience the joy of open source on this platform that updates monthly on the 28th.",Open Source Community,"Python





        1,180





        11


        Built by

          





        482 stars today",,,82566,2024-02-12T23:00:36Z
2024-02-29,https://github.com/521xueweihan/HelloGitHub,https://raw.githubusercontent.com/speechbrain/speechbrain/main/README.md,"The text provides an overview of SpeechBrain, an open-source PyTorch toolkit for developing Conversational AI technologies like speech assistants, chatbots, and large language models. It offers a holistic toolkit that supports various technologies for complex Conversational AI systems, spanning speech recognition, dialogue, language modeling, and more. SpeechBrain provides over 200 training recipes on 40 datasets, supporting 20 speech and text processing tasks. It integrates with HuggingFace for pretrained models and offers features like training orchestration, hyperparameter management, dynamic batching, GPU training, and more. The project encourages contributions and provides a roadmap for future development.",Accelerate Conversational AI Development with SpeechBrain: A Holistic Toolkit Overview,"SpeechBrain is an open-source PyTorch toolkit designed to accelerate Conversational AI development, including technologies for speech assistants, chatbots, and large language models. Built for easy creation of advanced technologies for speech and text processing, SpeechBrain offers a holistic toolkit that supports various technologies like speech recognition, speaker recognition, speech enhancement, speech separation, language modeling, and dialogue. With over 200 competitive training recipes on different datasets and tasks, SpeechBrain supports training from scratch and fine-tuning pretrained models. The toolkit also integrates with Hugging Face for access to over 100 pretrained models and provides prebuilt functionalities for training orchestration, hyperparameter management, efficient data reading, GPU training, mixed-precision training, and more.","Discover how SpeechBrain, an open-source PyTorch toolkit, simplifies Conversational AI development with technologies like speech recognition, speaker recognition, and speech enhancement. Learn about the toolkit's support for over 200 training recipes, integration with Hugging Face, and features for training orchestration, hyperparameter management, and efficient data reading.",Collaborative AI Framework.,"Python





        82,566





        9,361


        Built by

          









        37 stars today",,,7446,2016-05-04T06:24:11Z
2024-02-29,https://github.com/speechbrain/speechbrain,https://raw.githubusercontent.com/sdv-dev/SDV/main/README.md,"The text provides an overview of the Synthetic Data Vault (SDV) project, a Python library for generating tabular synthetic data. It uses various machine learning algorithms to learn patterns from real data and replicate them in synthetic data. The SDV offers features such as creating synthetic data, evaluating and visualizing data, preprocessing, anonymizing, and defining constraints. Users can install the SDV using pip or conda, then use it to synthesize data from real datasets. The library also allows for evaluating the quality of synthetic data by comparing it to real data and provides various visualization tools. The text concludes with credits to contributors and a citation recommendation.",Demystifying Synthetic Data Generation with the SDV Python Library,"The Synthetic Data Vault (SDV) is a powerful Python library designed to simplify the process of creating synthetic tabular data. Using various machine learning algorithms, SDV can learn patterns from real data and replicate them in synthetic datasets. From creating data using machine learning to evaluating and visualizing the data along with the ability to preprocess, anonymize, and define constraints, SDV offers a comprehensive solution for synthetic data generation. Whether you need single table data or interconnected tables, SDV can handle it all with ease. Learn how to get started with SDV, generate synthetic data, evaluate its quality, and explore the various features it offers.","Discover how the Synthetic Data Vault (SDV) Python library makes synthetic data generation effortless. From creating synthetic tabular data using machine learning to evaluating data quality and defining constraints, SDV offers a complete solution. Learn about its features, including preprocessing, anonymization, and visualization tools.",Python Libraries Collection,"Python





        7,446





        1,226


        Built by

          









        106 stars today",https://github.com/sdv-dev/SDV/blob/stable/docs/images/Single-Table-Metadata-Example.png; https://github.com/sdv-dev/SDV/blob/stable/docs/images/Real-vs-Synthetic-Evaluation.png,,1964,2020-04-28T17:48:45Z
2024-02-29,https://github.com/sdv-dev/SDV,https://raw.githubusercontent.com/jianchang512/pyvideotrans/main/README.md,"The text provides an overview of a video translation and dubbing tool. It describes features like translating videos to specified languages, generating subtitles, and adding dubbing automatically. The tool supports various language translations and voice synthesis methods. It allows for tasks like extracting and translating subtitles, merging subtitles and videos, creating dubbing for subtitles, separating voice and background music in videos, and downloading videos from YouTube. The text also includes information on using CUDA acceleration, configuring the tool through a command-line interface, and offers advanced settings through configuration files. Additionally, it lists related projects by the same author and provides contact details for support or donations.",Video Translation and Dubbing Tool,"This is a video translation and dubbing tool that can translate videos from one language to another, automatically generate and add subtitles and dubbing in the specified language. The tool uses faster-whisper and openai-whisper offline models for speech recognition. It supports translation services from Microsoft, Google, Baidu, Tencent, ChatGPT, Azure, Gemini, DeepL, DeepLX, and offline translation services. Voice synthesis supports Microsoft Edge TTS, Openai TTS-1, Elevenlabs TTS, and custom TTS server APIs, with the option to clone voices using 'clone-voice' to replicate the original voice. The tool allows for background music retention and supports various languages such as Chinese, English, Korean, Japanese, Russian, French, German, Italian, Spanish, Portuguese, Vietnamese, Thai, Arabic, Turkish, Hungarian, and Indian languages.","Explore a video translation and dubbing tool that transforms videos from one language to another, automatically adding subtitles and dubbing in the desired language. Support for various languages and services including speech recognition, translation, and voice synthesis methods.",Video Translation Tool.,"Python





        1,964





        271


        Built by

          









        15 stars today",https://raw.githubusercontent.com/jianchang512/pyvideotrans/main/./images/p2.png,,4348,2018-05-11T15:56:50Z
2024-02-29,https://github.com/Fanghua-Yu/SUPIR,https://raw.githubusercontent.com/ultralytics/yolov5/master/README.md,"The text provided introduces the YOLOv5, YOLOv8, and their various features, functionalities, and applications. It details their use in vision AI, including segmentation, classification, and object detection tasks. The text also mentions the availability of pre-trained models, training tutorials, and deployment options. It further highlights the different environments where you can quickly get started with YOLOv5 and how you can contribute to the project. Licensing options for AGPL-3.0 and Enterprise License are explained, along with contact information for bug reports, feature requests, and community interactions.",Introducing YOLOv8: The State-of-the-Art Object Detection Model,"YOLOv8 ðŸš€ is the world's most loved vision AI, representing Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. We are thrilled to announce the launch of Ultralytics YOLOv8 ðŸš€, our NEW cutting-edge, state-of-the-art (SOTA) model released at github.com/ultralytics/ultralytics. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection, image segmentation and image classification tasks. See the YOLOv8 Docs for details and get started with: PyPI version, Downloads, pip install ultralytics. Visit Ultralytics for more details and resources.","Introducing YOLOv8 ðŸš€, the latest state-of-the-art object detection model from Ultralytics. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection, image segmentation, and image classification tasks. Check out YOLOv8 Docs for details and resources. Purchase an Enterprise License at Ultralytics Licensing for commercial use.",Computer Vision Platform,"Python





        1,874





        131


        Built by

          





        69 stars today",https://user-images.githubusercontent.com/26833433/203113421-decef4c4-183d-4a0a-a6c2-6435b33bc5d3.jpg; https://user-images.githubusercontent.com/26833433/203113416-11fe0025-69f7-4874-a0a6-65d0bfe2999a.jpg,https://www.youtube.com/watch?v=LNwODJXcvt4,45377,2023-12-21T11:23:35Z
2024-02-29,https://github.com/ultralytics/yolov5,https://raw.githubusercontent.com/yerfor/GeneFacePlusPlus/main/README.md,"GeneFace++ is a real-time 3D talking face generation model implemented with Pytorch, focusing on high lip-sync, video-reality, and system efficiency. The official repository provides a guide for quick start, including environment setup, dataset download, and model checkpoints. The implementation includes features like eye blink control and an experimental audio-to-motion model. Users can interact with the model through provided scripts or a Gradio WebUI. Training GeneFace++ with custom videos is possible, with guidelines provided in the documentation. The authors invite citations for their work and plan future releases and enhancements.",GeneFace++: Generalized and Stable Real-Time 3D Talking Face Generation,"This blog post introduces GeneFace++, the official implementation that enables high lip-sync, high video-reality, and high system-efficiency 3D talking face generation using Pytorch. It provides a guide for a quick start in GeneFace++, including steps to prepare the environment, download datasets, and use pre-trained models. The post also covers FAQs, trainings, and citations related to GeneFace++. Explore the post for detailed information and resources.","Learn about GeneFace++, an implementation for high-quality 3D talking face generation, offering a quick start guide, FAQs, training details, and citations. Explore how GeneFace++ enables stable real-time generation of talking faces with high fidelity and efficiency.",Deep Learning Platform.,"Python





        45,377





        15,285


        Built by

          









        35 stars today",,,485,2020-05-18T03:45:11Z
2024-02-29,https://github.com/KillianLucas/open-interpreter,https://raw.githubusercontent.com/Sinaptik-AI/pandas-ai/main/README.md,"PandasAI is a Python library that utilizes generative AI to simplify data exploration, cleaning, and analysis through natural language queries. It offers easy installation via pip or poetry and interactive demos using Colab notebooks. The tool supports various deployment methods, including Jupyter notebooks or as a REST API with FastAPI or Flask. Users can ask questions, visualize data, and work with multiple dataframes. Privacy and security are prioritized through anonymization techniques. The software is available under the MIT license, with options for a managed cloud service or self-hosted enterprise offering. Contributions to the project are encouraged, and documentation, examples, and community discussions are available.",Exploring Data with PandasAI: Natural Language AI for Data Analysis,"PandasAI is a Python library that utilizes generative AI to enable users to interact with their data in natural language. With PandasAI, you can explore, clean, and analyze your data effortlessly using AI-powered tools. The library allows users to ask questions and generate visualizations quickly and effectively. Whether you're a beginner or an expert, PandasAI simplifies the data analysis process and provides valuable insights for decision-making purposes. Discover the power of PandasAI and revolutionize how you interact with your datasets today!","Learn how PandasAI, a Python library powered by generative AI, helps you explore, clean, and analyze data through natural language interactions. Simplify your data analysis process using PandasAI's powerful tools and get valuable insights effortlessly. Whether you're a beginner or an expert, PandasAI revolutionizes the way you work with datasets.",Natural Language Processing.,"Python





        41,238





        3,598


        Built by

          









        50 stars today",https://raw.githubusercontent.com/Sinaptik-AI/pandas-ai/main/images/logo.png,,9933,2023-07-14T07:10:44Z
2024-02-29,https://github.com/Sinaptik-AI/pandas-ai,https://raw.githubusercontent.com/airbytehq/airbyte/master/README.md,"The text describes Airbyte, an open-source data integration platform for ELT pipelines. It aims to cover a wide range of data sources and empower data engineers to customize connectors. Airbyte offers over 300 connectors for APIs, databases, data warehouses, and data lakes. Users can deploy Airbyte Open Source or use Airbyte Cloud to centralize data, create connectors easily, explore tutorials, and orchestrate data syncs. Various tools like Airflow, Prefect, and SQL can be used with Airbyte. The community can engage through Slack, forums, and office hours. Security concerns should be reported to `security@airbyte.io`. Airbyte also offers dedicated support and an Enterprise version with additional features.",Airbyte - Open Source Data Integration Platform,"We believe that only an open-source solution to data movement can cover the long tail of data sources while empowering data engineers to customize existing connectors. Our ultimate vision is to help you move data from any source to any destination. Airbyte already provides the largest catalog of 300+ connectors for APIs, databases, data warehouses, and data lakes. Getting Started: Deploy Airbyte Open Source or set up Airbyte Cloud to start centralizing your data. Create connectors in minutes with our no-code Connector Builder or low-code CDK. Join the Airbyte Community in our Slack, Forum, or Office Hours. Airbyte takes security issues seriously, please email security@airbyte.io for vulnerabilities.","Learn about Airbyte, an open-source data integration platform that offers a wide range of connectors for data movement. Explore how to get started with deploying Airbyte, building connectors, and engaging with the Airbyte community. Find out about Airbyte's security practices and enterprise features. Discover how to contribute to Airbyte and its commitment to open source. Read more on Airbyte's license, security information, and thank you page.",Data Ingestion Tool.,"Python





        9,933





        865


        Built by

          









        26 stars today",,,13370,2023-04-22T12:58:01Z
2024-02-29,https://github.com/airbytehq/airbyte,https://raw.githubusercontent.com/microsoft/sample-app-aoai-chatGPT/main/README.md,"The text describes a sample chat web application that incorporates Azure OpenAI. It includes prerequisites for deploying the app, such as having an Azure OpenAI resource and options for connecting to different data sources. The deployment process is explained, including using Azure Developer CLI, one-click Azure deployment, and deploying from a local machine. Different setups are detailed, such as basic chat experience, chat with your data, enabling chat history, and enabling message feedback. Additionally, information on adding an identity provider, customization scenarios, scalability, debugging the deployed app, configuring vector search, and changing citation display is provided. Best practices and contributing guidelines are also mentioned.",Building a Chat App with Azure OpenAI: Step-By-Step Guide,"This blog post provides a detailed guide on building a chat webapp that integrates with Azure OpenAI. It covers prerequisites such as having an existing Azure OpenAI resource and model deployment, and using Azure OpenAI on different data sources. The post also walks you through deploying the app using Azure Developer CLI, one-click Azure deployment, and deploying from your local machine. Additionally, it includes instructions on enabling chat history, message feedback, and authentication support in your app.","Learn how to build a chat webapp integrating Azure OpenAI with this comprehensive guide. Find steps for deploying the app using Azure Developer CLI, one-click Azure deployment, and deploying from your local machine. Discover how to enable chat history, message feedback, and authentication support.",Collaborative AI Framework.,"Python





        13,370





        3,469


        Built by

          









        75 stars today",,,1073,2020-07-27T23:55:54Z
2024-02-29,https://github.com/microsoft/sample-app-aoai-chatGPT,https://raw.githubusercontent.com/huggingface/diffusers/main/README.md,"The text provides an overview of the ðŸ¤— Diffusers library, a collection of state-of-the-art pretrained diffusion models for generating images, audio, and 3D structures. It emphasizes usability, simplicity, and customizability. The library offers diffusion pipelines, noise schedulers, and pretrained models for creating end-to-end diffusion systems. The installation process for PyTorch and Flax is explained. It also provides a quickstart guide for generating outputs using Diffusers. The text includes links to the documentation, tutorials, optimization guides, and training resources. It encourages contributions from the open-source community and lists popular tasks, pipelines, libraries, credits, and a citation.","ðŸ¤— Diffusers: State-of-the-art diffusion models for Images, Audio, and 3D Structures","ðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, ðŸ¤— Diffusers is a modular toolbox that supports both.","Explore ðŸ¤— Diffusers, the library offering state-of-the-art diffusion pipelines, noise schedulers, and pretrained models for generating images, audio, and 3D structures. Learn about installation, quickstart guide, documentation navigation, contributions, popular tasks & pipelines, libraries using ðŸ§¨ Diffusers, credits, and citation.",Deep Learning Platform.,"Python





        1,073





        1,523


        Built by

          









        9 stars today",,,21299,2023-04-06T21:16:41Z
2024-03-03,https://github.com/kyegomez/BitNet,https://raw.githubusercontent.com/kyegomez/BitNet/main/README.md,"The text is about a PyTorch implementation called BitNet, which is based on the ""BitNet: Scaling 1-bit Transformers for Large Language Models"" paper. The BitNet architecture replaces linear projections in Transformers with BitLinear modules. The implementation is simple and involves the transformation of tensor data. BitNet includes features such as BitNetTransformer, BitAttention, BitFeedForward, and BitNetInference for various applications. The text also mentions training on the enwiki8 dataset, new optimizations like BitMGQA, and collaboration in the Agora Discord community. It provides installation instructions, usage examples, and a Huggingface integration guide. Future plans include implementing new versions like BitNet1.5b in Cuda.",BitNet: Scaling 1-bit Transformers for Large Language Models - Implementation and Optimization,"The BitNet architecture offers a simple implementation of 1-bit Transformers for large language models. By replacing linear projections with BitLinear modules, BitNet provides a scalable solution for efficient model training and deployment. The latest iteration of BitNet introduces optimizations such as Bit Attention 'BitMGQA,' which leverages Multi Grouped Query Attention for improved decoding and context handling. With easy-to-use implementations and new features like Bit FeedForward, BitNet opens up possibilities for diverse applications beyond text processing.","Explore the implementation and optimization of BitNet, a revolutionary approach to scaling 1-bit Transformers for large language models. Learn how BitNet simplifies model architecture by introducing BitLinear modules and discover new enhancements like Bit Attention 'BitMGQA.' Dive into the world of efficient model training and deployment with BitNet's innovative features and capabilities.",Language Models,"Python





        819





        68


        Built by

          








        175 stars today",https://raw.githubusercontent.com/kyegomez/BitNet/main/agorabanner.png; https://raw.githubusercontent.com/kyegomez/BitNet/main/bitnet.png,,819,2023-10-18T16:19:06Z
2024-03-03,https://github.com/python-poetry/poetry,https://raw.githubusercontent.com/python-poetry/poetry/main/README.md,"Poetry is a tool for easy Python packaging and dependency management. It simplifies declaring, managing, and installing dependencies for Python projects, ensuring a consistent stack. With Poetry, you can streamline your project setup by replacing multiple files like `setup.py` and `requirements.txt` with a single `pyproject.toml` file. This file specifies project details, dependencies, and optional configurations. Poetry supports various dependency types, including standard, version-specific, git-based, and optional dependencies. Additionally, it organizes dependencies into groups for better management. The tool offers multiple installation methods and comprehensive documentation for users, as well as opportunities for contribution to the project.",Demystifying Python Packaging and Dependency Management with Poetry,"Poetry helps you declare, manage, and install dependencies of Python projects, ensuring you have the right stack everywhere. It replaces `setup.py`, `requirements.txt`, `setup.cfg`, `MANIFEST.in`, and `Pipfile` with a simple `pyproject.toml` based project format. Poetry supports multiple installation methods, including a simple script found at install.python-poetry.org. For full installation instructions, including advanced usage of the script, alternate install methods, and CI best practices, see the full installation documentation.","Learn how Poetry simplifies Python packaging and dependency management, making it easier to manage dependencies, declare projects, and install the right stack everywhere. Discover multiple installation methods, including a simple script at install.python-poetry.org. Find detailed installation instructions and best practices in the full installation documentation.",Python Libraries Collection.,"Python





        28,793





        2,161


        Built by

          









        12 stars today",https://raw.githubusercontent.com/python-poetry/poetry/master/assets/install.gif,,28793,2018-02-28T15:23:47Z
2024-03-03,https://github.com/embedchain/embedchain,https://raw.githubusercontent.com/embedchain/embedchain/main/README.md,"Embedchain is an open-source RAG Framework designed to simplify the creation and deployment of AI applications by offering a *""Conventional but Configurable""* approach. It aids software and machine learning engineers in managing unstructured data efficiently by segmenting it, generating embeddings, and storing them in a vector database for optimized retrieval. With diverse APIs, users can extract contextual information, find answers, and engage in chat conversations tailored to their data. The framework supports easy installation via Python API and showcases a live demo called ""Chat with PDF."" Extensive documentation, community engagement options, and guidelines for contributing are available. Anonymous telemetry data is collected to enhance the package's quality and user experience.",Introducing Embedchain: Open Source RAG Framework for Creating AI Apps,"Embedchain is an Open Source RAG Framework that simplifies the creation and deployment of AI apps. It is designed to be 'Conventional but Configurable' to cater to both software and machine learning engineers. The framework facilitates the development of Retrieval-Augmented Generation (RAG) applications, allowing users to efficiently manage unstructured data, generate embeddings, and optimize retrieval. With a range of APIs, Embedchain empowers users to extract contextual information, find precise answers, and engage in chat conversations tailored to their data.","Discover Embedchain, the Open Source RAG Framework that streamlines AI app development. Learn how Embedchain simplifies managing unstructured data, generating embeddings, and enabling precise answers through diverse APIs.",Collaborative AI Framework.,"Python





        7,962





        944


        Built by

          









        14 stars today",https://raw.githubusercontent.com/embedchain/embedchain/main/docs/images/cover.gif,,7963,2023-06-20T08:58:36Z
2024-03-03,https://github.com/allenai/fm-cheatsheet,https://raw.githubusercontent.com/allenai/fm-cheatsheet/main/README.md,"The text is about ""The Foundation Model Development Cheatsheet,"" providing resources and recommendations for best practices in developing and releasing models. To contribute to the cheatsheet, resources need to meet certain criteria including helpfulness, documentation quality, and value to the development process. They primarily focus on tools like data catalogs, search/analysis tools, and evaluation repositories. Contributors can use an upload form or create a pull request to add resources. For questions, contact slongpre@media.mit.edu. A citation will be available soon. The cheatsheet aims to support responsible development practices and improve the model development process with valuable insights and resources.",Foundation Model Development: Best Practices and Resources,Explore resources and recommendations for best practices in developing and releasing models through our comprehensive cheatsheet. Contribute your own resources by following the provided guidelines and criteria for inclusion. Get involved by using the upload form or creating a pull request on the repository. Contact us for any questions or inquiries regarding this valuable resource.,Discover a cheatsheet with resources and best practices for developing and releasing foundation models. Learn how to contribute resources and get involved in responsible development practices. Contact us for any questions and find out more about citations for the provided information.,Data Science Resources.,"Python





        113





        11


        Built by

          







        27 stars today",https://raw.githubusercontent.com/allenai/fm-cheatsheet/main/app/resources/logos/cheatsheet-0.png,,113,2023-12-01T19:05:20Z
2024-03-03,https://github.com/maszhongming/Multi-LoRA-Composition,https://raw.githubusercontent.com/maszhongming/Multi-LoRA-Composition/main/README.md,"The text discusses a project called Multi-LoRA Composition for Image Generation that introduces two training-free methods, LoRA Switch and LoRA Composite, to integrate various elements into images through multi-LoRA composition. It compares these methods to traditional techniques like LoRA Merge. The project provides guidelines for setting up the environment, downloading pre-trained LoRAs, and generating images using these methods. Additionally, it includes experiments on ComposLoRA and comparative evaluations using GPT-4V. The work also involves human evaluations for assessing composition and image quality. To know more, visit their website or check out the paper on arXiv (https://arxiv.org/abs/2402.16843).",Multi-LoRA Composition for Image Generation: Enhancing Image Creativity with LoRA Switch and LoRA Composite,"Low-Rank Adaptation (LoRA) techniques have revolutionized text-to-image models, allowing for precise rendering of unique elements in generated images. Our project introduces two innovative, training-free methods called LoRA Switch and LoRA Composite. These methods enable multi-LoRA composition, showcasing significant improvements over traditional LoRA Merge approaches. By integrating any number of elements seamlessly into images, our techniques spark creativity and offer a wide range of possibilities for image generation. Explore our step-by-step guide for setting up the environment, downloading pre-trained LoRAs, and utilizing ComposLoRA for generating captivating images with our cutting-edge multi-LoRA Composition methods.","Discover the power of multi-LoRA Composition techniques in the realm of image generation with our innovative LoRA Switch and LoRA Composite methods. Unleash your creativity by seamlessly integrating diverse elements into images, creating visually stunning compositions. Follow our comprehensive guide to set up the environment, download pre-trained LoRAs, and leverage ComposLoRA for image generation that transcends traditional boundaries. Enhance your image creation process with our groundbreaking approaches to multi-LoRA Composition.",Image Generation Platform.,"Python





        226





        23


        Built by

          





        14 stars today",https://raw.githubusercontent.com/maszhongming/Multi-LoRA-Composition/main/images/tangram.png; https://raw.githubusercontent.com/maszhongming/Multi-LoRA-Composition/main/images/intro_fig.png; https://raw.githubusercontent.com/maszhongming/Multi-LoRA-Composition/main/images/merge_example.png; https://raw.githubusercontent.com/maszhongming/Multi-LoRA-Composition/main/images/switch_example.png; https://raw.githubusercontent.com/maszhongming/Multi-LoRA-Composition/main/images/composite_example.png,,226,2024-02-20T20:43:36Z
2024-03-03,https://github.com/PaddlePaddle/PaddleNLP,https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/main/README.md,"PaddleNLP is a user-friendly and powerful natural language processing and large language model (LLM) development library. It provides high-quality pre-trained models and a seamless development experience, covering a wide range of NLP scenarios with industry-specific model libraries and practical examples to meet developers' needs for customization.

The latest release, **PaddleNLP v2.7**, includes significant upgrades for large model experiences, unified tools for large model training, fine-tuning, compression, inference, and deployment. It introduces a unified checkpoint storage mechanism, LoRA efficient fine-tuning support, and algorithms like QLoRA.

Previous release, **PaddleNLP v2.6**, introduced a full-process large model toolchain covering pre-training, fine-tuning, compression, inference, and deployment. It offers optimized algorithms like LoRA and Prefix Tuning, INT8/INT4 quantization, and supports major large models.

PaddleNLP features a variety of Chinese pre-trained models, a comprehensive collection of application examples, end-to-end industry system examples, and high-performance distributed training and inference capabilities.

For more information and details on installation, features, examples, and community interactions, please refer to the complete text provided above.",PaddleNLP: Easy-to-Use Natural Language Processing Library,"PaddleNLP is a simple yet powerful natural language processing and large language model (LLM) development library. It aggregates industry-quality pre-trained models and provides an out-of-the-box development experience, covering a model library for NLP in various scenarios with industrial practice examples to meet developers' flexible customization needs.","Discover PaddleNLP, a comprehensive library for natural language processing and large language model development. Learn about industry-quality pre-trained models, easy-to-use development tools, and practical examples for various NLP scenarios.",Natural Language Processing,"Python





        11,072





        2,753


        Built by

          









        6 stars today",https://user-images.githubusercontent.com/1371212/175816733-8ec25eb0-9af3-4380-9218-27c154518258.png; https://user-images.githubusercontent.com/11793384/159693816-fda35221-9751-43bb-b05c-7fc77571dd76.gif; https://user-images.githubusercontent.com/11793384/168514909-8817d79a-72c4-4be1-8080-93d1f682bb46.gif; https://user-images.githubusercontent.com/11793384/168514868-1babe981-c675-4f89-9168-dd0a3eede315.gif; https://user-images.githubusercontent.com/11793384/168407260-b7f92800-861c-4207-98f3-2291e0102bbe.png; https://user-images.githubusercontent.com/16698950/168589100-a6c6f346-97bb-47b2-ac26-8d50e71fddc5.png; https://user-images.githubusercontent.com/11793384/168407921-b4395b1d-44bd-41a0-8c58-923ba2b703ef.png; https://user-images.githubusercontent.com/11793384/168407831-914dced0-3a5a-40b8-8a65-ec82bf13e53c.gif; https://user-images.githubusercontent.com/11793384/168515134-513f13e0-9902-40ef-98fa-528271dcccda.png; https://user-images.githubusercontent.com/11987277/245085922-0aa68d24-00ff-442e-9c53-2f1e898151ce.png,,11072,2021-02-05T13:07:42Z
2024-03-03,https://github.com/stanfordnlp/dspy,https://raw.githubusercontent.com/stanfordnlp/dspy/main/README.md,"The text provides information about DSPy, a framework for optimizing language model prompts and weights. DSPy separates the program flow from prompt parameters and introduces optimizers to tune the prompts and weights of language model calls. It aims to improve the quality and cost efficiency of using language models for complex tasks. The framework offers tutorials, documentation, and examples to help users get started. DSPy offers a new paradigm where language models and prompts are optimized pieces of a larger system. It focuses on less prompting, higher scores, and a systematic approach to solving challenging tasks using language models.",DSPy: Programming Foundation Models for Algorithmic Optimization,"**DSPy** is a framework for algorithmically optimizing LM prompts and weights. It separates the flow of your program from the parameters of each step and introduces new optimizers that can tune the prompts and/or the weights of your LM calls. **DSPy** can help teach powerful models like GPT-3.5 or GPT-4 to be more reliable at tasks, leading to higher quality and a more systematic approach to solving hard tasks with LMs.","Learn about how DSPy helps optimize LM prompts and weights, teaches powerful models like GPT-3.5 or GPT-4 to be more reliable, and offers a systematic approach to task-solving with LMs.",Natural Language Processing,"Python





        7,378





        497


        Built by

          









        51 stars today",https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/images/DSPy8.png,https://www.youtube.com/watch?v=Dt3H2ninoeY; https://www.youtube.com/watch?v=im7bCLW2aM4; https://www.youtube.com/watch?v=41EfOY0Ldkc; https://www.youtube.com/watch?v=ycfnKPxBMck; https://www.youtube.com/watch?v=CDung1LnLbY; https://www.youtube.com/watch?v=CEuUG4Umfxs,7378,2023-01-09T21:01:51Z
2024-03-03,https://github.com/bigcode-project/starcoder2,https://raw.githubusercontent.com/bigcode-project/starcoder2/main/README.md,"StarCoder2 is a collection of code generation models trained on over 600 programming languages from The Stack v2 dataset, as well as natural language text sources like Wikipedia, Arxiv, and GitHub issues. The models use Grouped Query Attention and have significant token capacities. The 3B and 7B models were trained on trillions of tokens, while the 15B model was trained on more than 4 trillion tokens. These models are designed for code completion tasks and are not suitable for instruction-based tasks. The models can be fine-tuned using various techniques such as Low-Rank Adaptation training and quantization. To explore more details, refer to the provided paper link.",Introducing StarCoder2: Advanced Code Generation Models for Programming Languages,"StarCoder2 is a family of code generation models (3B, 7B, and 15B), trained on 600+ programming languages from The Stack v2 and some natural language text such as Wikipedia, Arxiv, and GitHub issues. These models use Grouped Query Attention and have impressive memory footprints, offering various functionalities for code generation and fine-tuning. Learn how to install, load models, generate code, and fine-tune StarCoder2 models efficiently.","Discover StarCoder2, a new family of code generation models trained on diverse programming languages and natural language text. Explore how to install, load, and use these models for code generation, inference, and fine-tuning. Check out this comprehensive guide to understand more about StarCoder2's architecture, memory footprint, and advanced functionalities.",Natural Language Processing,"Python





        522





        43


        Built by

          






        95 stars today",,,522,2023-12-08T08:46:25Z
2024-03-03,https://github.com/MooreThreads/Moore-AnimateAnyone,https://raw.githubusercontent.com/MooreThreads/Moore-AnimateAnyone/master/README.md,"The text provides an update on the release of training codes for AnimateAnyone models, a link to the demo, and details on the repository that reproduces AnimateAnyone. It mentions current limitations and improvements planned for the future. The text includes information on release plans, examples generated, installation instructions, downloading weights, and training and inference details. Additionally, it highlights a Gradio Demo, community contributions, use on the Mobi MaLiang platform, a disclaimer, and acknowledgments to related repositories and contributors. The text also encourages feedback and ideas from the community for further development.",Enhancing Moore-AnimateAnyone: Training Codes Released and Demo Updated,"We have released the training codes for our AnimateAnyone models, allowing users to train their own models. In addition, we have updated the HuggingFace Spaces demo of Moore-AnimateAnyone. Our repository aims to reproduce AnimateAnyone results with various methods, although some differences may exist from the original paper. While our current version approximates 80% of the performance shown in AnimateAnyone, we acknowledge certain limitations such as background artifacts and scale mismatches. Future improvements will address these issues. Instructions for installation, downloading weights, training, and inference are provided in detail.","Explore the latest updates on Moore-AnimateAnyone with the release of training codes and an updated HuggingFace Spaces demo. Reproducing AnimateAnyone results with our repository while addressing limitations. Installation guidance, weight downloading instructions, training, and inference details included for users' convenience.",Image Generation Platform.,"Python





        2,243





        171


        Built by

          







        21 stars today",,https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/9c4d852e-0a99-4607-8d63-569a1f67a8d2; https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/722c6535-2901-4e23-9de9-501b22306ebd; https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/17b907cc-c97e-43cd-af18-b646393c8e8a; https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/86f2f6d2-df60-4333-b19b-4c5abcd5999d,2243,2024-01-12T07:55:21Z
2024-03-03,https://github.com/freqtrade/freqtrade,https://raw.githubusercontent.com/freqtrade/freqtrade/master/README.md,"Freqtrade is an open-source crypto trading bot written in Python, supporting major exchanges and controllable via Telegram or webUI. It offers backtesting, plotting, and money management tools, with strategy optimization through machine learning. The bot provides features like persistence, dry-run mode, strategy optimization, adaptive prediction modeling, position sizing, whitelist/blacklist cryptocurrencies, a built-in WebUI, Telegram management, and performance reporting. It runs on Python 3.9+ and utilizes sqlite for persistence. Freqtrade is best used for educational purposes, emphasizing the need for understanding before investing real money. Detailed documentation and development branches are available for further exploration and community support. Required resources include an accurate clock, minimum hardware specifications, Python 3.9+, TA-Lib, virtualenv, and Docker.",Discover Freqtrade: Free Open Source Crypto Trading Bot in Python,"Freqtrade is a free and open source crypto trading bot written in Python, designed to support all major exchanges and controlled via Telegram or webUI. It offers features like backtesting, money management tools, and strategy optimization via machine learning. Please start by running the bot in Dry-run mode and ensure you have coding and Python knowledge before engaging money. Join the Freqtrade community through Discord for help, to report bugs, or make feature requests.","Explore Freqtrade, a free and open source Python-based crypto trading bot supporting major exchanges. Learn about its features, community support, and requirements for running the bot efficiently. Join the Discord server for help, bug reporting, and feature requests.",Crypto Trading Bot,"Python





        24,677





        5,384


        Built by

          









        130 stars today",https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade-screenshot.png,,24677,2017-05-17T23:48:53Z
2024-03-03,https://github.com/jhao104/proxy_pool,https://raw.githubusercontent.com/jhao104/proxy_pool/master/README.md,"The text provides information about ProxyPool, a web crawler proxy IP pool project. It includes details on the functionality, usage methods (API and CLI), supported Python versions, running the project, using Docker, extending proxy sources, and free proxy sources. The text also offers guidance on contributing to the project, reporting issues, and using the proxies in web scraping. It mentions contributors and provides a link to the changelog. The project aims to collect free proxies, verify their validity, and offer them through an API. You can find more detailed information in the complete text.",ProxyPool: A Comprehensive Guide to Building and Managing Proxy IP Pools,"ProxyPool project focuses on collecting and verifying free proxies from various sources. It ensures the availability of proxies by regular validation and offers API and CLI interfaces. Users can extend proxy sources to enhance the quality and quantity of the proxy pool IP addresses. This blog provides instructions on running the project, setting up, using Docker images, and expanding proxy sources for better performance.","Discover how ProxyPool project can help you in managing and validating proxy IP pools effectively. Learn about the features, setup instructions, Docker Image usage, and how to add new proxy sources for better performance.",Open Source Tool,"Python





        19,776





        4,888


        Built by

          









        6 stars today",,,19776,2016-11-25T13:49:07Z
2024-03-03,https://github.com/fluencelabs/dev-rewards,https://raw.githubusercontent.com/fluencelabs/dev-rewards/main/README.md,"The text provides guidelines on how to generate proof for Fluence Developer Rewards using different methods. 

For docker image proof generation, you need to build the image and run the script with your ssh keys directory path specified.

For local proof generation using shell script, you should install dependencies and run the shell script provided.

For local proof generation using python script, you need to install Python, create a virtual environment, install dependencies, and run the Python script.

These steps involve setting up docker images, running local shell scripts, and running Python scripts to generate proof for Fluence Developer Rewards.",Fluence Developer Rewards: Generating Proof via Docker and Local Scripts,"Learn how to generate proof for Fluence developer rewards using Docker and local scripts. Docker image can be built using `docker build -t dev-reward-script .` command. If your ssh keys are in different directories, customize the Docker run command accordingly. For generating proof using local scripts, dependencies need to be installed with `./install.sh` and the respective scripts need to be run as described.",Discover the process of generating proof for Fluence developer rewards with Docker and local scripts. Follow the instructions to build a Docker image and run scripts locally. Install dependencies and execute the necessary scripts to claim your developer rewards efficiently.,Crypto Tools & Guides.,"Python





        147





        116


        Built by

          









        16 stars today",,,147,2024-02-27T12:12:28Z
2024-03-03,https://github.com/kijai/ComfyUI-SUPIR,https://raw.githubusercontent.com/kijai/ComfyUI-SUPIR/main/README.md,"The text provides information about the ComfyUI SUPIR upscaler wrapper node, which is a work in progress. Users can install the node by managing and installing from git or cloning the repo to custom_nodes and running specific commands. Additional instructions on installing necessary dependencies are provided. The text also mentions memory requirements and system specifications for running the node efficiently. Furthermore, it includes links to access the SUPIR models and provides warnings about downloading large models. The text also shares links for original models, associated BibTeX information, contact details, and guidelines for non-commercial use.",ComfyUI SUPIR Upscaling Node Guide,"ComfyUI SUPIR upscaler wrapper node is an essential tool for enhancing image resolution and quality. To install, either manage and install from Git or clone the repo to custom_nodes and run `pip install -r requirements.txt`. Memory requirements are directly related to input image resolution. Additionally, you can find the SUPIR model(s) and SDXL model in the `ComfyUI/models/checkpoints` folder. For testing, you can conduct video upscale tests and image upscales with provided links. Check the blog for more details.",Learn how to use the ComfyUI SUPIR upscaling node to enhance image quality and resolution. Install the node with provided instructions and explore various model options. Test the node's performance with upscale tests. Find the SUPIR model(s) and SDXL model in the designated folder. Discover more about image enhancement with this comprehensive guide.,Custom Node Framework.,"Python





        280





        13


        Built by

          






        33 stars today",,,280,2024-02-28T19:14:40Z
2024-03-03,https://github.com/speechbrain/speechbrain,https://raw.githubusercontent.com/Z4nzu/hackingtool/master/README.md,"The text describes a comprehensive hacking tool for hackers with various features and updates. It includes installation instructions for Linux and Docker, along with a list of hacking tools categorized under different sections such as Anonymously Hiding Tools, Information Gathering Tools, Wordlist Generator, Wireless Attack Tools, SQL Injection Tools, Phishing Attack Tools, Web Attack Tools, Post Exploitation Tools, etc. The tool comes with a menu listing different tool categories and provides links to access them on GitHub. The text emphasizes using the tool responsibly and provides social media links for the original author. Additionally, it offers future updates and encourages feedback and suggestions for improvements.",The Ultimate Hacking Tool Collection for Hackers | All-in-One Toolkit,"Discover the ultimate hacking toolkit that includes tools for anonymity, information gathering, wordlist generation, wireless attacks, SQL injections, phishing, web attacks, post-exploitation, forensic analysis, payload creation, exploit frameworks, reverse engineering, DDoS attacks, RAT tools, XSS attacks, steganography, and more. This comprehensive resource also covers social media bruteforcing, Android hacking, IDN homograph attacks, email verification, hash cracking, WiFi deauthentication, social media reconnaissance, payload injection, web crawling, and a mix of other helpful tools. Ensure you use this toolkit responsibly, and explore various tools for ethical hacking and cybersecurity testing.","Explore the ultimate hacking toolkit featuring tools for anonymity, information gathering, SQL injections, phishing, web attacks, post-exploitation, payload creation, and more. Discover tools for social media bruteforcing, Android hacking, steganography, and more. Use this toolkit responsibly for ethical hacking and cybersecurity testing.",Cybersecurity Tool,"Python





        7,493





        1,230


        Built by

          









        25 stars today",https://github.com/Z4nzu/hackingtool/blob/master/images/A00.png; https://github.com/Z4nzu/hackingtool/blob/master/images/A0.png; https://github.com/Z4nzu/hackingtool/blob/master/images/A1.png; https://github.com/Z4nzu/hackingtool/blob/master/images/A2.png; https://github.com/Z4nzu/hackingtool/blob/master/images/A4.png,https://www.youtube.com/watch?v=BsFhpIDcd9I,42145,2020-04-28T17:48:45Z
2024-03-03,https://github.com/layerdiffusion/sd-forge-layerdiffusion,https://raw.githubusercontent.com/layerdiffusion/sd-forge-layerdiffusion/main/README.md,"The text provides updates and details about the ""sd-forge-layerdiffusion"" repository, focusing on transparent image layer diffusion using latent transparency. It mentions work-in-progress extensions, image generation functionalities, and model releases for SDXL. The text includes specific prompts for users to check the transparency diffusion process, details on released models, and instructions for background and foreground conditions. It also addresses potential upcoming releases, model improvements, and emphasizes reproducing results accurately. The repository aims to provide a dynamic code base for generating transparent images and layers using innovative techniques, benefiting professional content creation studios.",,,,Image Generation Platform.,"Python





        1,085





        85


        Built by

          





        301 stars today",,,1085,2024-03-01T06:41:32Z
2024-03-03,https://github.com/naver/dust3r,https://raw.githubusercontent.com/naver/dust3r/main/README.md,"The text provides information about the implementation of DUSt3R, a tool for geometric 3D vision. It includes details like the project page, arXiv link, and a BibTeX citation. Installation steps are outlined, along with checkpoints for pre-trained models. An interactive demo is described for reconstructing scenes, along with code snippets for usage examples. Training instructions are also included, with hyperparameters and steps for training the DUSt3R models. Overall, the text covers various aspects of DUSt3R, from its features to practical usage and training guidance.",DUSt3R: Geometric 3D Vision Made Easy | Implementation and Usage Guide,"DUSt3R offers an official implementation of 'DUSt3R: Geometric 3D Vision Made Easy'. The blogpost provides comprehensive information on the installation process, checkpoints, interactive demo, usage guidelines, as well as training insights. Learn how to clone DUSt3R, create the environment, compile cuda kernels, download pre-trained models, run interactive demos, perform global alignment, visualize reconstructions, and more. Dive into the world of 3D vision with DUSt3R!","Discover the official implementation of 'DUSt3R: Geometric 3D Vision Made Easy' along with installation guides, checkpoints information, interactive demo instructions, usage insights, and training details. Learn how to use DUSt3R for 3D reconstruction and enhance your understanding of 3D vision. Get started with DUSt3R today!",Computer Vision Platform.,"Python





        894





        63


        Built by

          






        198 stars today",https://raw.githubusercontent.com/naver/dust3r/main/assets/pipeline1.jpg; https://raw.githubusercontent.com/naver/dust3r/main/assets/dust3r_archi.jpg; https://raw.githubusercontent.com/naver/dust3r/main/assets/demo.jpg; https://raw.githubusercontent.com/naver/dust3r/main/assets/matching.jpg,,894,2024-02-21T07:13:14Z
2024-03-03,https://github.com/AUTOMATIC1111/stable-diffusion-webui,https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/README.md,"The text describes a web interface called Stable Diffusion web UI, developed using the Gradio library. The interface offers various features, including img2img and txt2img modes, outpainting, inpainting, color sketch, and more. Users can adjust attention to specific text parts, run img2img processing multiple times, and create 3D image plots. Additional functionalities include the ability to run arbitrary Python code, mouseover hints, and support for various neural network tools. The text also provides detailed installation instructions for different platforms and highlights the contributions from various sources. The interface supports multiple extensions and integrations, making it a comprehensive tool for visual generation tasks.","Enhancing Visual Generation with Stable Diffusion Web UI: Features, Installation, and Contributions","Stable Diffusion Web UI is a powerful tool that offers various features such as original txt2img and img2img modes, outpainting, inpainting, color sketch, prompt matrix, stable diffusion upscale, and much more. Users can enjoy live previews, custom scripts, tiling support, highres fix, and API support. The installation process varies for different platforms, including NVidia GPUs, AMD GPUs, Intel CPUs and GPUs, and Apple Silicon. The project welcomes contributions and provides comprehensive documentation on its wiki.","Discover the diverse features of Stable Diffusion Web UI for visual generation, learn how to install it on different platforms, and explore opportunities to contribute. Check out the detailed installation guides for NVidia GPUs, AMD GPUs, Intel CPUs, and Apple Silicon. Get started with this innovative tool today!",Computer Vision Platform.,"Python





        123,701





        24,154


        Built by

          









        142 stars today",https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png,,123701,2022-08-22T14:05:26Z
2024-03-03,https://github.com/allenai/OLMo,https://raw.githubusercontent.com/allenai/OLMo/main/README.md,"OLMo, short for Open Language Model, is a repository dedicated to training and utilizing AI2's advanced open language models, developed by scientists specifically for scientific purposes. It offers models like OLMo 1B, OLMo 7B, and OLMo 7B Twin 2T, trained on the Dolma dataset. Users can install OLMo via PyPI or from the source code. The text details installation instructions, various models in the OLMo family, how to run inference using Hugging Face integration, finetuning checkpoints, quantization, reproducibility tips for training, inspecting training data, finetuning instructions, evaluation tools, and a citation reference for OLMo research.","OLMo: Open Language Model - Overview, Installation, Models, and Inference","OLMo is a repository for training and using AI2's state-of-the-art open language models. Built by scientists, for scientists, OLMo offers models like OLMo 1B, OLMo 7B, and OLMo 7B Twin 2T trained on the Dolma dataset. The blogpost covers installation instructions for PyTorch, models overview, inference using Hugging Face integration, quantization details, reproducibility with training and inspecting data, fine-tuning guidance, evaluation tools, and citation information.","Learn about OLMo, a repository for open language models like OLMo 1B, OLMo 7B, and OLMo 7B Twin 2T. Explore installation guides, models overview, inference methods with Hugging Face, quantization, reproducibility steps, fine-tuning instructions, evaluation tools, and citation details for OLMo. Discover how OLMo accelerates the science of language models.",Language Models,"Python





        3,264





        283


        Built by

          









        60 stars today",https://allenai.org/olmo/olmo-7b-animation.gif,,3264,2023-02-20T22:29:43Z
2024-03-03,https://github.com/mini-sora/minisora,https://raw.githubusercontent.com/mini-sora/minisora/main/README.md,"Mini Sora is an open-source community organized by students aiming to explore the implementation path of Sora and its future development direction. They conduct roundtable discussions with the community to explore possibilities, and discuss existing technologies for video generation. The community also has a paper reproduction group focusing on GPU-friendly, training-efficient, and inference-efficient targets. They have ongoing discussions, reading plans, and various related works in the field of diffusion models, transformers, video generation, and more. The community welcomes contributions and has a dedicated group of contributors. You can learn more and contribute by visiting their GitHub repository.","Exploring the World of Sora: Latest Updates, Research, and Community Discussions","Join the Mini Sora open-source community, a place for exploring Sora's implementation paths and future directions. Engage in roundtable discussions, explore video generation technologies, and delve into diffusion models for visual computing. Stay updated with the latest research, papers, and shared insights within the community.","Discover the Mini Sora open-source community, where you can participate in roundtable discussions, explore video generation technologies, and dive into diffusion models. Stay up-to-date with the latest research and community insights on Sora's implementation paths and future directions.",Collaborative AI Framework.,"Python





        310





        40


        Built by

          









        51 stars today",https://raw.githubusercontent.com/mini-sora/minisora/main/assets/logo.jpg; https://raw.githubusercontent.com/mini-sora/minisora/main/assets/qrcode.png,,310,2024-02-21T13:50:34Z
2024-03-03,https://github.com/liguodongiot/llm-action,https://raw.githubusercontent.com/liguodongiot/llm-action/main/README.md,"The text provides an overview of various topics related to large language models (LLMs). It covers different aspects such as model training, inference, compression, algorithm architecture, AI compilers, AI infrastructure, model deployment on servers, and common tools used in these domains. The text includes information on specific LLM training practices, optimization techniques like knowledge distillation and low-rank decomposition, as well as tools and frameworks commonly used in LLM development. It also mentions learning and discussion groups dedicated to LLMs and provides links to relevant resources and materials. Additionally, there is a section on the history of stars the project has gathered over time.","Exploring Large Language Models: Training, Inference, and Compression","The blog post dives into different aspects of large language models (LLMs) including training techniques, inference frameworks, and model compression methods. It covers topics like distributed training, LLM fine-tuning technologies, knowledge distillation, low-rank decomposition, model localization, AI compilers, and more. The post also provides insights on LLM applications, AI infrastructure setup, and commonly used tools. Join the LLM learning and discussion groups, and follow the companion WeChat Official Account for the latest updates on AI engineering practices.","Explore various facets of large language models (LLMs) from training to inference and model compression. Learn about distributed training techniques, LLM fine-tuning methods, knowledge distillation, low-rank decomposition, AI compilers, AI infrastructure setup, and essential tools. Join learning groups and follow the WeChat Official Account for AI engineering insights.",Language Models,"Python





        3,711





        346


        Built by

          





        154 stars today",https://github.com/liguodongiot/llm-action/blob/main/pic/llm-action-v3.png; https://github.com/liguodongiot/llm-action/blob/main/pic/wx.jpg; https://github.com/liguodongiot/llm-action/blob/main/pic/wx-gzh.png,,3711,2023-05-23T05:29:16Z
2024-03-03,https://github.com/lllyasviel/Fooocus,https://raw.githubusercontent.com/tinygrad/tinygrad/master/README.md,"tinygrad is a deep learning framework maintained by tiny corp. It aims to be a simple framework to add new accelerators to, supporting both inference and training. The framework is still in alpha stage but has received funding for further development. Key features include LLaMA and Stable Diffusion, as well as laziness for fused operations. Neural networks can be easily implemented with tinygrad, along with support for various accelerators such as GPU, C Code, LLVM, METAL, CUDA, and HIP. Installation is recommended from source. Contributions are welcomed, with guidelines provided for submitting PRs. Tests can be run locally using pytest.",Exploring tinygrad: A Simple yet Powerful Deep Learning Framework,"tinygrad is a deep learning framework maintained by tiny corp, sitting between PyTorch and micrograd. Despite its simplicity, it aims to be versatile and easy to extend with new accelerators for both inference and training purposes. With features like LLaMA and Stable Diffusion, laziness for efficient kernel fusion, and neural network capabilities, tinygrad offers a promising tool for machine learning enthusiasts. The framework already supports various accelerators like GPU, C Code, LLVM, METAL, CUDA, and HIP, with the flexibility to add more. Installation is recommended from the source code, and documentation can be found in the docs directory.","Discover the simplicity and power of tinygrad, a deep learning framework between PyTorch and micrograd. Learn about its features, including LLaMA and Stable Diffusion, laziness for efficient kernel fusion, and neural network capabilities. Explore how tinygrad supports various accelerators like GPU, C Code, LLVM, METAL, CUDA, and HIP, with the flexibility to add more. Installation is recommended from the source code, and documentation can be found in the docs directory.",Deep Learning Framework,"Python





        32,053





        3,665


        Built by

          









        70 stars today",https://raw.githubusercontent.com/tinygrad/tinygrad/master/docs/logo.png,,22567,2023-08-09T18:43:40Z
2024-03-03,https://github.com/freqtrade/freqtrade,https://raw.githubusercontent.com/roboflow/supervision/main/README.md,"The text provides information about a computer vision toolkit called Supervision. It offers tools for loading datasets, drawing detections on images or videos, and counting detections in a zone. The toolkit supports various models and provides customizable annotators and dataset utilities. Users can install the Supervision package using pip in a Python environment. The text includes code examples for working with detection models and annotating images. Additionally, it mentions tutorials on vehicle tracking and traffic analysis. The text also encourages contributions and provides links to documentation and resources related to Supervision.",Computer Vision Tools: Building Reusable Models with Supervision,"Discover how Supervision provides reusable computer vision tools for loading datasets, drawing detections, and counting detections. Pip install the supervision package in a Python environment to get started. Use Supervision for model-agnostic tasks like classification, detection, and segmentation. Explore various connectors and annotators to enhance your computer vision projects. Learn about the dataset utilities provided by Supervision for loading, splitting, merging, and saving datasets.","Learn about the reusable computer vision tools offered by Supervision for loading datasets, drawing detections, and counting detections. Follow a quick installation guide by pip installing the supervision package in Python environment. Dive into model-agnostic applications for classification, detection, and segmentation tasks. Explore connectors and annotators provided by Supervision to enhance your computer vision projects. Discover the dataset utilities for loading, splitting, merging, and saving datasets.",Computer Vision Platform.,"Python





        24,695





        5,387


        Built by

          









        40 stars today",https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png,https://www.youtube.com/watch?v=uWP6UjDeZvY; https://www.youtube.com/watch?v=uWP6UjDeZvY; https://www.youtube.com/watch?v=4Q3ut7vqD5o; https://www.youtube.com/watch?v=4Q3ut7vqD5o,9780,2017-05-17T23:48:53Z
2024-03-03,https://github.com/microsoft/unilm,https://raw.githubusercontent.com/ltdrdata/ComfyUI-Impact-Pack/master/README.md,"The text provides detailed information about a custom node pack called ComfyUI-Impact-Pack. It includes various custom nodes such as Detectors, Detailers, SEGS Manipulation nodes, Pipe nodes, Image Utils, Switch nodes, Wildcards, and more. The pack offers features like controlling nodes, applying sampling methods, working with detectors and classifiers, and handling image and mask manipulation. Additionally, the text mentions explanations on Ultralytics models, installation instructions, and package dependencies. Users can apply these nodes to enhance image quality, process segmentation, work with detectors, and manage conditional logic within ComfyUI workflows effectively.",ComfyUI-Impact-Pack Custom Nodes for Enhancing Images and Models,"The ComfyUI-Impact-Pack is a collection of custom nodes designed to enhance images through various detectors, detailers, upscalers, and more. Recent compatibility patches and feature updates ensure smooth performance, but users are advised to update to the latest versions for optimal functionality. Various custom nodes such as Detectors, SAMLoader, Detailers, and SEGS Manipulation nodes are included to streamline image enhancement workflows. Installing the Impact Pack allows access to Ultralytics models for face, people, and clothing detection, along with NSFW content models.","Discover the capabilities of the ComfyUI-Impact-Pack, a collection of custom nodes designed to enhance images through detectors, detailers, and upscalers. Stay up-to-date with recent compatibility patches and feature updates to ensure optimal performance. Update to the latest versions for seamless workflow integration and access to Ultralytics models for various detections, including face, people, and clothing models.",Custom Node Pack,"Python





        17,587





        2,302


        Built by

          









        132 stars today",https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/simple.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/simple-original.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/simple-refined.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/2pass-simple.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/2pass-original.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/2pass-1pass.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/2pass-2pass.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/combination.jpg; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/combination-original.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/combination-refined.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/upscale-workflow.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/upscale-original.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/upscale-3x.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/SAMDetector-menu.png; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/SAMDetector-dialog.jpg; https://github.com/ltdrdata/ComfyUI-extension-tutorials/raw/Main/ComfyUI-Impact-Pack/images/SAMDetector-result.jpg,https://www.youtube.com/watch?v=AccoxDZIg3Y; https://www.youtube.com/watch?v=AccoxDZIg3Y,959,2019-07-23T04:15:28Z
2024-03-03,https://github.com/ltdrdata/ComfyUI-Impact-Pack,https://raw.githubusercontent.com/BatsResearch/bonito/main/README.md,"Bonito is an open-source model designed for creating task-specific training datasets from unannotated text for instruction tuning. It allows for easy generation of synthetic datasets using the Hugging Face `transformers` and `vllm` libraries. The tool supports various task types such as question answering, sentiment analysis, summarization, and more. To use Bonito, you can create a synthetic instruction tuning dataset by providing unannotated text and defining the task type. The model is accessible through the `bonito-v1` repository and the `ctga-v1` dataset. To reproduce experiments, refer to the `nayak-arxiv24-code` GitHub repository. If using Bonito in research, remember to cite the provided paper.",Bonito: An Open-Source Model for Instruction Tuning Dataset Generation,"Bonito is an open-source model designed to convert unannotated text into task-specific training datasets for instruction tuning. This blog introduces Bonito as a lightweight library that allows for the easy creation of synthetic datasets using Hugging Face `transformers` and `vllm` libraries. Users can follow specific installation and usage instructions to generate synthetic instruction tuning datasets across various supported task types. Additionally, the post includes information on citation guidelines for referencing Bonito in research.","Learn about Bonito, an open-source model for generating instruction tuning datasets from unannotated text. Discover how to easily create synthetic datasets with Bonito using Hugging Face libraries. Explore the installation steps and basic usage instructions for generating synthetic datasets for various supported task types.",Language Models.,"Python





        959





        92


        Built by

          









        3 stars today",https://raw.githubusercontent.com/BatsResearch/bonito/main/assets/workflow.png,,162,2023-03-30T13:54:51Z
2024-03-03,https://github.com/BatsResearch/bonito,https://raw.githubusercontent.com/huggingface/alignment-handbook/main/README.md,"The text is a handbook called ""The Alignment Handbook"" that focuses on aligning language models with human and AI preferences. It discusses techniques like supervised fine-tuning, reward modeling, rejection sampling, and direct preference optimization. The handbook provides robust training recipes and highlights recent releases and news related to aligning language models. It includes links to models, datasets, and demos, along with navigation tips for the project structure. The text also mentions installation instructions and encourages readers to cite the handbook if found useful. Overall, the handbook aims to be a resource for the ML community on training language models to better align with user preferences.",The Alignment Handbook: Robust Training Recipes for Language Model Alignment,"Robust recipes to align language models with human and AI preferences. Just one year ago, chatbots were out of fashion and most people hadn't heard about techniques like Reinforcement Learning from Human Feedback (RLHF) to align language models with human preferences. The Alignment Handbook aims to fill the gap by providing robust training recipes that span the entire pipeline, including supervised fine-tuning, reward modeling, rejection sampling, and direct preference optimization (DPO). Explore how to replicate models like Zephyr 7B, navigate the project, and install dependencies.","Discover robust training recipes in The Alignment Handbook for aligning language models with human and AI preferences. Learn about supervised fine-tuning, reward modeling, rejection sampling, and direct preference optimization (DPO). Explore project navigation and installation instructions for running the code efficiently.",Natural Language Processing.,"Python





        162





        8


        Built by

          






        38 stars today",https://raw.githubusercontent.com/huggingface/alignment-handbook/main/assets/handbook.png,,3331,2024-02-25T13:33:33Z
2024-03-03,https://github.com/huggingface/alignment-handbook,https://raw.githubusercontent.com/alexta69/metube/master/README.md,"MeTube is a web GUI for youtube-dl that uses the yt-dlp fork, allowing you to download videos from various sites with playlist support. It can be run with Docker or docker-compose, and settings can be configured via environment variables. You can set options such as user and group IDs, download directories, themes, output file templates, and more. MeTube supports using browser cookies for restricted or private videos, browser extensions for easy downloads, an iOS Shortcut, and a bookmarklet. Running behind a reverse proxy is recommended for authentication and HTTPS support. Regular updates are necessary to keep up with changes in the yt-dlp engine.",A Complete Guide to MeTube: Web GUI for youtube-dl with Playlist Support,"MeTube is a powerful web GUI for youtube-dl that supports playlists from YouTube and many other sites. You can easily download videos by running it using Docker or docker-compose. Customize MeTube using various environment variables for settings like theme, download directories, output templates, and more. Additionally, learn how to use browser cookies with MeTube, browser extensions, iOS shortcuts, and bookmarklets for seamless video downloads. Discover tips for running MeTube behind a reverse proxy, updating yt-dlp, and building/running MeTube locally.","Explore the functionalities of MeTube, a web GUI for youtube-dl with playlist support. Learn how to download videos from YouTube and other platforms, customize settings via environment variables, use browser cookies, browser extensions, iOS shortcuts, and bookmarklets for efficient downloads. Get insights on running MeTube behind a reverse proxy, updating yt-dlp, and building/running MeTube locally for a seamless video download experience.",Data Ingestion Tool,"Python





        3,331





        248


        Built by

          









        12 stars today",https://github.com/alexta69/metube/raw/master/screenshot.gif,,2890,2023-08-25T11:35:34Z
2024-03-03,https://github.com/alexta69/metube,https://raw.githubusercontent.com/VikParuchuri/marker/master/README.md,"Marker is a tool that converts PDF, EPUB, and MOBI files to markdown, emphasizing speed and accuracy. It specializes in handling PDF documents such as books and scientific papers. The conversion process involves advanced deep learning models for tasks like text extraction, layout detection, and formatting. Marker outperforms a similar tool named Nougat in speed and has lower hallucination risks due to its focused algorithm. The tool supports multiple languages and provides customizable settings. Marker is suitable for academic and digital PDFs, offering features like equation conversion to LaTeX and code block formatting. It is optimized for GPU, CPU, or MPS usage. Marker's limitations include occasional issues with equations and language support. Installation instructions and usage guidelines are provided for Linux and Mac systems. Performance benchmarks showcase Marker's speed and accuracy, comparing it with Nougat and naive text extraction methods. Commercial usage is restricted due to licensing of certain dependencies, but a commercial version is being developed. The tool acknowledges various open-source models and datasets that have contributed to its development.",Introducing Marker: A Fast and Accurate PDF to Markdown Converter,"Marker is a powerful tool that efficiently converts PDF, EPUB, and MOBI files to markdown format. It boasts a 10x speed improvement over similar tools like nougat and offers greater accuracy, especially on complex documents. With features like support for various PDF document types, equation conversion to Latex, code block and table formatting, and multi-language support, Marker is a versatile solution. Its deep learning pipeline ensures efficient text extraction, page layout detection, and post-processing, resulting in high-quality markdown output.","Discover Marker, a revolutionary tool for converting PDF files to markdown at lightning speed. Learn about its features, speed, and accuracy compared to existing solutions. See how Marker's deep learning models ensure precise text extraction and formatting, making it a standout choice for your PDF conversion needs.",Document Conversion Tool,"Python





        2,890





        198


        Built by

          









        128 stars today",https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/overall.png; https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/per_doc.png,,6974,2019-11-29T17:24:51Z
2024-03-03,https://github.com/VikParuchuri/marker,https://raw.githubusercontent.com/majacinka/crewai-experiments/main/README.md,"The text describes experiments using CrewAI, focusing on different AI projects. The projects involve tasks like examining startup ideas, creating AI newsletters from Google SERP and Reddit Scraping, and developing an email classifier. Various AI models like GPT-4, Gemini Pro, Mistral, Open Chat, Nous Hermes, among others, were used for the experiments. Results varied, with some models producing generic content, lacking coherence, or failing to use appropriate tools. The Llama 2 13B model showed better understanding but lacked coherence for newsletter content. Some models like Llama 2 13B chat and text failed to produce any output, indicating limitations in understanding and task completion.",Unveiling My CREWAI Experiments: Leveraging AI Models with Varied Results,"In my recent experiments with CREWAI, I delved into trying 3 diverse projects, ranging from straightforward to intricate tasks. The main goal was to assign these tasks to a team of AI agents, each aimed at different objectives including examining a startup idea, building AI newsletters using Google SERP and Reddit Scraper, alongside developing an Email classifier. Through this venture, I explored numerous AI models, both through API calls like GPT-4 and Gemini Pro, as well as local models via Ollama, assessing their performance and output quality diligently.",Explore my journey with CREWAI experiments where I tasked AI agents with diverse projects like analyzing startup ideas and crafting AI newsletters. Discover the varied performance and output quality of different AI models tested through API calls and locally via Ollama. Dive into the world of AI experimentation with insights on model behavior and task understanding.,Collaborative AI Framework.,"Python





        6,974





        250


        Built by

          






        50 stars today",,,460,2023-10-30T20:14:08Z
2024-03-03,https://github.com/majacinka/crewai-experiments,https://raw.githubusercontent.com/smicallef/spiderfoot/master/README.md,"SpiderFoot is an open-source intelligence automation tool designed to analyze data from various sources. It features over 200 modules, supports a YAML-configurable correlation engine, and offers export options like CSV and JSON. Users can access it through a web-based UI or CLI. It allows for the extraction of diverse data types, including IP addresses, email addresses, social media accounts, and more. SpiderFoot integrates with external tools like SHODAN, HaveIBeenPwned, and Nmap. It supports Python 3 and is MIT-licensed. SpiderFoot can be used for offensive (red team exercises, pen testing) or defensive purposes. Additional features are available in SpiderFoot HX, a cloud-based managed solution. You can learn more on their website or join their community on Discord.",SpiderFoot: Open Source Intelligence Automation Tool,"SpiderFoot is an open source intelligence (OSINT) automation tool designed to aggregate and analyze data from various sources. It features a web-based UI and CLI interface making it easy to navigate and utilize the data seamlessly. Written in Python 3 and MIT-licensed, SpiderFoot offers over 200 modules for comprehensive data extraction and analysis. Whether used offensively or defensively, SpiderFoot can target various entities like IP addresses, domains, and even social media accounts to provide in-depth insights and analysis.","Discover how SpiderFoot, an open source intelligence tool, uses Python 3 and over 200 modules to gather and analyze data from diverse sources. Explore its features, uses, and installation process. Join the SpiderFoot community for discussions, tutorials, and more. Follow SpiderFoot on Twitter for the latest updates.",Cybersecurity Tool.,"Python





        460





        105


        Built by

          





        12 stars today",https://www.spiderfoot.net/wp-content/themes/spiderfoot/img/spiderfoot-wide.png; https://www.spiderfoot.net/wp-content/uploads/2022/04/opensource-screenshot-v4.png,,11365,2024-01-13T17:00:01Z
2024-03-04,https://github.com/lining808/CS-Ebook,https://raw.githubusercontent.com/lining808/CS-Ebook/main/README.md,"This text introduces a repository that contains a curated list of high-quality books on computer science and technology. The collection is constantly updated and covers most software-related fields. The main categories include:

- **Computer Basics**: Introduces books on fundamental topics like computer introduction, architecture, operating systems, computer networks, data structures, and algorithms.
- **Programming Languages**: Features books on various languages, including C, C++, C#, Rust, Java, Go, Python, SQL, JavaScript, PHP, Ruby, Matlab, and Latex.
- **Software Engineering**: Offers resources for roles like product managers and provides insights into software architecture and debugging/testing.
- **Mathematical Tools**: Lists books on basic and applied mathematics.
- **Big Data**: Includes books on data analysis and data mining.
- **Artificial Intelligence**: Covers machine learning, deep learning, reinforcement learning, etc.
- **Survival Guide**: Shares tips on surviving in the field, including advice on certifications, job interviews, etc.

The guide ensures quality over quantity and aims to include classic books for each direction. The books are recommended for their latest editions, though downloading links are not directly provided; it refers readers to the Z-Library for accessing a wider range of books.

The text also mentions that the entire book recommendations come from online sources and are meant for personal learning and reference. Finally, it touches upon some computer fundamentals with specific book recommendations for deeper learning, but does not go into the details of all categories mentioned.",Ultimate Guide for Aspiring Computer Scientists: Top Book Recommendations,"Discover a curated repository of high-quality computer science and technology books to advance your learning journey. Covering most software-related fields, this list is continually updated and meticulously organized. Dive into computer fundamentals, programming languages, software engineering, mathematical tools, big data, artificial intelligence, survival guides, and much more. Whether you're a beginner, an expert, or somewhere in between, these recommendations promise to enrich your understanding and skill set in the ever-evolving world of computing.","Explore an expertly curated list of computer science and technology books, updated regularly. It spans topics from basics to AI, making it perfect for learners at any stage.",Data Science Resources,"Python





        1,028





        89


        Built by

          





        79 stars today",https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/bg.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/start.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºæ¦‚è®º/å¤§è¯è®¡ç®—æœº å·1-3.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºæ¦‚è®º/è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ç™¾ç§‘å…¨ä¹¦ (ç¬¬ä¸‰ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºæ¦‚è®º/è®¡ç®—æœºç§‘å­¦æ¦‚è®º (ç¬¬13ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºæ¦‚è®º/è®¡ç®—æœºç§‘å­¦å¯¼è®ºï¼ˆç¬¬4ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç»„æˆåŽŸç†/è®¡ç®—æœºç»„æˆ  ç»“æž„åŒ–æ–¹æ³•ï¼ˆåŽŸä¹¦ç¬¬6ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç»„æˆåŽŸç†/è®¡ç®—æœºç»„æˆä¸Žè®¾è®¡ ç¡¬ä»¶è½¯ä»¶æŽ¥å£ (ç¬¬5ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç»„æˆåŽŸç†/è®¡ç®—æœºç»„æˆä¸Žè®¾è®¡ï¼šç¡¬ä»¶è½¯ä»¶æŽ¥å£ï¼ˆARMç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç»„æˆåŽŸç†/è®¡ç®—æœºç»„æˆä¸Žè®¾è®¡ï¼šç¡¬ä»¶è½¯ä»¶æŽ¥å£ï¼ˆRISC-Vç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç»„æˆåŽŸç†/æ‰‹æŠŠæ‰‹æ•™ä½ è®¾è®¡CPU-RISC-Vå¤„ç†å™¨ç¯‡.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç»„æˆåŽŸç†/ç”µè„‘ç»„è£…ã€ç»´æŠ¤ã€ç»´ä¿®å…¨èƒ½ä¸€æœ¬é€š.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç»„æˆåŽŸç†/è®¡ç®—æœºç»„è£…ä¸Žç»´æŠ¤.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºç½‘ç»œ/è®¡ç®—æœºç½‘ç»œ (ç¬¬8ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºç½‘ç»œ/è®¡ç®—æœºç½‘ç»œ è‡ªé¡¶å‘ä¸‹æ–¹æ³• (ç¬¬ä¸ƒç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºç½‘ç»œ/ç½‘ç»œæ˜¯æ€Žæ ·è¿žæŽ¥çš„.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºç½‘ç»œ/TCP IPè¯¦è§£ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºç½‘ç»œ/è®¡ç®—æœºç½‘ç»œ ç³»ç»Ÿæ–¹æ³• (ç¬¬5ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºç½‘ç»œ/å›¾è§£HTTP.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è®¡ç®—æœºç½‘ç»œ/å›¾è§£TCPIPåè®®.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ“ä½œç³»ç»Ÿ/æ·±å…¥ç†è§£è®¡ç®—æœºç³»ç»Ÿï¼ˆåŽŸä¹¦ç¬¬3ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ“ä½œç³»ç»Ÿ/çŽ°ä»£æ“ä½œç³»ç»Ÿ (ç¬¬4ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ“ä½œç³»ç»Ÿ/æ“ä½œç³»ç»Ÿå¯¼è®º.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/ç®—æ³•å¯¼è®ºï¼ˆåŽŸä¹¦ç¬¬3ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/ç®—æ³•  (ç¬¬4ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/è®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯1.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/labuladongçš„ç®—æ³•å°æŠ„ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/LeetCode 101 (C++ Version).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/ç¼–ç¨‹ç çŽ‘.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/å¤§è¯æ•°æ®ç»“æž„ã€æº¢å½©åŠ å¼ºç‰ˆã€‘.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/ç®—æ³•å›¾è§£.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/æ¼«ç”»ç®—æ³• å°ç°çš„ç®—æ³•ä¹‹æ—….jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/æ•°æ®ç»“æž„ (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/æ•°æ®ç»“æž„ä¸Žç®—æ³•åˆ†æž Cè¯­è¨€æè¿°ï¼ˆåŽŸä¹¦ç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/æ•°æ®ç»“æž„ä¸Žç®—æ³•å›¾è§£.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/ç®—æ³•ç¬”è®°.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/ç®—æ³•ç²¾ç²¹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç®—æ³•ä¸Žæ•°æ®ç»“æž„/ç®—æ³•è®¾è®¡ä¸Žåˆ†æžåŸºç¡€ (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç¼–ç¨‹å·¥å…·/PyCharm ä¸­æ–‡æŒ‡å—ï¼ˆWinç‰ˆï¼‰v2.0.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç¼–ç¨‹å·¥å…·/VSCodeæƒå¨æŒ‡å—.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç¼–ç¨‹å·¥å…·/ç²¾é€šGit (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ±‡ç¼–/ç¼–è¯‘åŽŸç† (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/C/Cç¨‹åºè®¾è®¡è¯­è¨€ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/C/C Primer Plusï¼ˆç¬¬6ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/C/Cè¯­è¨€ç¨‹åºè®¾è®¡ çŽ°ä»£æ–¹æ³• (ç¬¬2ç‰ˆ.ä¿®è®¢ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/C/Cå’ŒæŒ‡é’ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++ Primer (ç¬¬5ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++ Primerä¹ é¢˜é›†ï¼ˆç¬¬5ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++ Primer Plus (ç¬¬6ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++æ ‡å‡†åº“ (ç¬¬2ç‰ˆ) .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++ç¨‹åºè®¾è®¡è¯­è¨€ï¼ˆç‰¹åˆ«ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++ç¨‹åºè®¾è®¡è¯­è¨€ ç¬¬1ï½ž3éƒ¨åˆ†ï¼ˆç¬¬4ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++ç¨‹åºè®¾è®¡è¯­è¨€ ç¬¬4éƒ¨åˆ†ï¼ˆç¬¬4ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++20é«˜çº§ç¼–ç¨‹ï¼ˆç¬¬5ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/Effective Modern C++.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/More Effective C++.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/æ˜Žè§£C++.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Cpp/C++ Templates (ç¬¬2ç‰ˆÂ·ä¸­æ–‡ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Csharp/æ·±å…¥ç†è§£Cï¼ˆç¬¬3ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Csharp/C å›¾è§£æ•™ç¨‹  (ç¬¬5ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/R/Rè¯­è¨€å®žæˆ˜ï¼ˆç¬¬3ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Rust/Rust ç¨‹åºè®¾è®¡ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Rust/ç²¾é€šRust(ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/Javaç¼–ç¨‹æ€æƒ³ (ç¬¬5ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/æ·±å…¥ç†è§£Javaè™šæ‹Ÿæœºï¼ˆç¬¬3ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/Javaæ ¸å¿ƒæŠ€æœ¯Â·å·Iï¼ˆåŽŸä¹¦ç¬¬12ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/Javaå®žæˆ˜ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/Effective Java (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/spring boot Vue3.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/spring bootå®žæˆ˜ï¼š.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/Spring Bootå®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/Springå®žæˆ˜ï¼ˆç¬¬6ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/Springå¾®æœåŠ¡å®žæˆ˜ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/Kafkaæƒå¨æŒ‡å—ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/æ·±å…¥ç†è§£Kafkaï¼šæ ¸å¿ƒè®¾è®¡ä¸Žå®žè·µåŽŸç†.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Java/MyBatisä»Žå…¥é—¨åˆ°ç²¾é€š.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Go/Goè¯­è¨€åœ£ç».jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Go/Goè¯­è¨€å­¦ä¹ ç¬”è®°.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Effect Python.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Flash Webå¼€å‘ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Flask Webå¼€å‘å®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Pandasæ•°æ®å¤„ç†ä¸Žåˆ†æž.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Python asyncio å¹¶å‘ç¼–ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Python Qt GUIä¸Žæ•°æ®å¯è§†åŒ–ç¼–ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Python3ç½‘ç»œçˆ¬è™«å¼€å‘å®žæˆ˜ ç¬¬2ç‰ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Pythonç¼–ç¨‹ï¼šä»Žå…¥é—¨åˆ°å®žè·µï¼ˆç¬¬3ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/PythonåŸºç¡€æ•™ç¨‹ (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Pythonè®©ç¹çå·¥ä½œè‡ªåŠ¨åŒ–.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Pythonç½‘ç»œçˆ¬è™«æƒå¨æŒ‡å— (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/Selenium3è‡ªåŠ¨åŒ–æµ‹è¯•å®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/SQLAlchemy Pythonæ•°æ®åº“å®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/æµç•…çš„ Pythonï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Python/æ˜Žè§£Python.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/MySQLåŸºç¡€æ•™ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/MySQLæ˜¯æ€Žæ ·è¿è¡Œçš„.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/SQLå¿…çŸ¥å¿…ä¼š (ç¬¬5ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/SQLåŸºç¡€æ•™ç¨‹ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/SQLè¿›é˜¶æ•™ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/é«˜æ€§èƒ½MYSQLï¼ˆç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/é«˜æ€§èƒ½MYSQLï¼ˆç¬¬å››ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/æ•°æ®åº“ç³»ç»Ÿæ¦‚å¿µ (ç¬¬6ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/Rediså¼€å‘ä¸Žè¿ç»´.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/Redisè®¾è®¡ä¸Žå®žçŽ°.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/MongoDBå®žæˆ˜  (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/SQL Serverä»Žå…¥é—¨åˆ°ç²¾é€š.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/æ”¶èŽ·ä¸æ­¢Oracle ç¬¬2ç‰ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/SQL/PostgreSQL æŠ€æœ¯å†…å¹•.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ­£åˆ™/æ­£åˆ™è¡¨è¾¾å¼å¿…çŸ¥å¿…ä¼š (ä¿®è®¢ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ­£åˆ™/æ­£åˆ™æŒ‡å¼•ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ±‡ç¼–/æ±‡ç¼–è¯­è¨€ (ç¬¬4ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Ruby/Rubyå…ƒç¼–ç¨‹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/ä½ ä¸çŸ¥é“çš„JavaScript.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/JavaScripté«˜çº§ç¨‹åºè®¾è®¡ (ç¬¬4ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/JavaScriptæƒå¨æŒ‡å— (ç¬¬7ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/vue.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/æ·±å…¥è§£æžCSS.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/CSSä¸–ç•Œ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/CSSæ–°ä¸–ç•Œ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/CSSé€‰æ‹©å™¨ä¸–ç•Œ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/æ·±å…¥æµ…å‡ºNode.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/å°ç¨‹åºå¼€å‘åŽŸç†ä¸Žå®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/jQueryå®žæˆ˜ï¼ˆç¬¬ä¸‰ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/TypeScriptç¼–ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/æ­ç§˜Angularï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/æ·±å…¥ReactæŠ€æœ¯æ ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/JavaScript/æ·±å…¥ç†è§£ES6.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/Linux UNIXç³»ç»Ÿç¼–ç¨‹æ‰‹å†Œ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/Linuxå¸¸ç”¨å‘½ä»¤è‡ªå­¦æ‰‹å†Œ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/Linuxå‘½ä»¤è¡Œä¸ŽShellè„šæœ¬ç¼–ç¨‹å¤§å…¨ (ç¬¬4ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/Linuxå‘½ä»¤è¡Œå¤§å…¨ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/Unix&Liunxå¤§å­¦æ•™ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/UNIXçŽ¯å¢ƒé«˜çº§ç¼–ç¨‹ (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/UNIXç¼–ç¨‹è‰ºæœ¯.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/UNIXç½‘ç»œç¼–ç¨‹ å·1 (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/UNIXç½‘ç»œç¼–ç¨‹ å·2 (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/æ·±å…¥Linuxå†…æ ¸æž¶æž„.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/é¸Ÿå“¥çš„Linuxç§æˆ¿èœ (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/é¸Ÿå“¥çš„Linuxç§æˆ¿èœ (ç¬¬4ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/Vimå®žç”¨æŠ€å·§ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/Ubuntu Linuxæ“ä½œç³»ç»Ÿï¼šå¾®è¯¾ç‰ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Linux/Linuxç½‘ç»œæ“ä½œç³»ç»Ÿé¡¹ç›®æ•™ç¨‹ï¼ˆRHEL 7.4CentOS 7.4ï¼‰ï¼ˆç¬¬3ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Latex/Latex Notes é›·å¤ªèµ«æŽ’ç‰ˆç³»ç»Ÿç®€ä»‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/PHP/Modern PHP  ä¸­æ–‡ç‰ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/MATLAB/MATLABä»Žå…¥é—¨åˆ°ç²¾é€š.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Swift/Swiftè¿›é˜¶.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Kotlin/Androidç¼–ç¨‹æƒå¨æŒ‡å—.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/Kotlin/kotlinå®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/äº§å“ç»ç†/äººäººéƒ½æ˜¯äº§å“ç»ç†2.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/å¤§è¯è®¾è®¡æ¨¡å¼.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/å‡¤å‡°æž¶æž„.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/æž¶æž„æ•´æ´ä¹‹é“.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/è®¾è®¡æ¨¡å¼ å¯å¤ç”¨é¢å‘å¯¹è±¡è½¯ä»¶çš„åŸºç¡€ï¼ˆå…¸è—ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/è®¾è®¡æ¨¡å¼çš„è‰ºæœ¯ï¼šè½¯ä»¶å¼€å‘äººå‘˜å†…åŠŸä¿®ç‚¼ä¹‹é“.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/è®¾è®¡æ¨¡å¼ä¹‹ç¾Ž.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/å›¾è§£è®¾è®¡æ¨¡å¼.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/å¾®æœåŠ¡æž¶æž„è®¾è®¡æ¨¡å¼.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/è½¯ä»¶å·¥ç¨‹ ï¼ˆç¬¬10ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/å†™ç»™å¤§å®¶çœ‹çš„è®¾è®¡ä¹¦ï¼ˆç¬¬4ç‰ˆï¼‰ï¼ˆå¹³è£…ï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/æ¸¸æˆå¼•æ“Žæž¶æž„ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æž¶æž„è®¾è®¡/ä»£ç éšæƒ³å½• å…«è‚¡æ–‡.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/K8S/Docker å®¹å™¨ä¸Žå®¹å™¨äº‘ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/K8S/Kubernetesä¿®ç‚¼æ‰‹å†Œ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/K8S/kubernetæƒå¨æŒ‡å—.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/K8S/æ·±å…¥å‰–æžKubernetes.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/K8S/æ·±å…¥æµ…å‡ºDocker.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æµ‹è¯•è¿ç»´/å…¨æ ˆæ€§èƒ½æµ‹è¯•ä¿®ç‚¼å®å…¸JMeterå®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æµ‹è¯•è¿ç»´/è½¯ä»¶è°ƒè¯•.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/K8S/DevOpså®žè·µæŒ‡å—.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/K8S/æŒç»­äº¤ä»˜  å‘å¸ƒå¯é è½¯ä»¶çš„ç³»ç»Ÿæ–¹æ³•.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/ç¨‹åºå‘˜çš„æ•°å­¦ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/ç¨‹åºå‘˜çš„æ•°å­¦ 2 æ¦‚çŽ‡ç»Ÿè®¡.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/ç¨‹åºå‘˜çš„æ•°å­¦ 3 çº¿æ€§ä»£æ•°.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/ç¨‹åºå‘˜æ•°å­¦.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/ä»Žé›¶å¼€å§‹ï¼šæœºå™¨å­¦ä¹ çš„æ•°å­¦åŽŸç†å’Œç®—æ³•å®žè·µ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/æ”¹å˜ä¸–ç•Œçš„17ä¸ªæ–¹ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/æœºå™¨å­¦ä¹ çš„æ•°å­¦.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/è®¡ç®—æœºç§‘å­¦ä¸­çš„æ•°å­¦ï¼šä¿¡æ¯ä¸Žæ™ºèƒ½æ—¶ä»£çš„å¿…ä¿®è¯¾.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/å…·ä½“æ•°å­¦ è®¡ç®—æœºç§‘å­¦åŸºç¡€ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/æ·±åº¦å­¦ä¹ çš„æ•°å­¦.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/æ•°å­¦ä¹‹ç¾Žï¼ˆç¬¬ä¸‰ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/ç»Ÿè®¡å­¦ä¹ æ–¹æ³• (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/åº”ç”¨æ•°å­¦/å´å†›æ•°å­¦é€šè¯†è®²ä¹‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/çº¯æ•°å­¦æ•™ç¨‹ (ç¬¬9ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/å¤åˆ†æž å¯è§†åŒ–æ–¹æ³•.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/æ¦‚çŽ‡å¯¼è®º (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/çº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨ (ç¬¬4ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/çº¿æ€§ä»£æ•°åº”è¯¥è¿™æ ·å­¦ (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/ç¦»æ•£æ•°å­¦åŠå…¶åº”ç”¨ï¼ˆåŽŸä¹¦ç¬¬8ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/ç»„åˆæ•°å­¦ (ç¬¬5ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/æ™®æž—æ–¯é¡¿æ¦‚çŽ‡è®ºè¯»æœ¬.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/æ™®æž—æ–¯é¡¿æ•°å­¦åˆ†æžè¯»æœ¬.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é«˜ç­‰æ•°å­¦/æ™®æž—æ–¯é¡¿å¾®ç§¯åˆ†è¯»æœ¬ (ä¿®è®¢ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ•°æ®åˆ†æž/Hadoopæƒå¨æŒ‡å—.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ•°æ®åˆ†æž/Pythonæ•°æ®ç§‘å­¦æ‰‹å†Œ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ•°æ®åˆ†æž/åˆ©ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æž åŽŸä¹¦ç¬¬2ç‰ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ•°æ®åˆ†æž/Pythoné‡‘èžå¤§æ•°æ®åˆ†æž (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ•°æ®æŒ–æŽ˜/æ•°æ®å¯†é›†åž‹åº”ç”¨ç³»ç»Ÿè®¾è®¡.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ•°æ®æŒ–æŽ˜/æ•°æ®æŒ–æŽ˜ æ¦‚å¿µä¸ŽæŠ€æœ¯ (ç¬¬3ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ•°æ®æŒ–æŽ˜/æ•°æ®æŒ–æŽ˜å¯¼è®º (å®Œæ•´ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /ç™¾é¢æœºå™¨å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /åŠ¨æ‰‹å­¦æœºå™¨å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /æœºå™¨å­¦ä¹  (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /æœºå™¨å­¦ä¹  å…¬å¼æŽ¨åˆ°ä¸Žä»£ç å®žçŽ°.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /æœºå™¨å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /æœºå™¨å­¦ä¹ Pythonå®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /æœºå™¨å­¦ä¹ ç¬”è®°(å´æ©è¾¾)v5.51.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /æœºå™¨å­¦ä¹ å…¬å¼è¯¦è§£.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /æœºå™¨å­¦ä¹ å®žæˆ˜ï¼šåŸºäºŽScikit-Learnã€Keraså’ŒTensorFlow (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /ç¾Žå›¢æœºå™¨å­¦ä¹ å®žè·µ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /å¯è§£é‡Šäººå·¥æ™ºèƒ½å¯¼è®º.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /äººå·¥æ™ºèƒ½ï¼šçŽ°ä»£æ–¹æ³•ï¼ˆç¬¬4ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /å®žç”¨æŽ¨èç³»ç»Ÿ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /é¸¢å°¾èŠ±ä¹¦.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æœºå™¨å­¦ä¹ /AIè¡Œä¸šæŠ¥å‘Š2023.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /Pythonæ·±åº¦å­¦ä¹ ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /Pytorch æ·±åº¦å­¦ä¹ å®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹  (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ·±åº¦å­¦ä¹ 500é—® .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ·±åº¦å­¦ä¹ åŽŸç†ä¸Žpytorchå®žæˆ˜ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /Pythonæ·±åº¦å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /Pytorch1.11.0å®˜æ–¹æ•™ç¨‹ä¸­æ–‡ç‰ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æŽå®æ¯…æ·±åº¦å­¦ä¹ æ•™ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ·±åº¦å­¦ä¹ ç¬”è®°(å´æ©è¾¾)v5.72.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ·±åº¦å­¦ä¹ åŽŸç†ä¸Žå®žè·µ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /Pythonç¥žç»ç½‘ç»œç¼–ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /TensorFlowæ·±åº¦å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ¨¡å¼è¯†åˆ«ä¸Žæœºå™¨å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ·±åº¦å­¦ä¹ é«˜æ‰‹ç¬”è®°.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /ç¥žç»ç½‘ç»œä¸Žæ·±åº¦å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /PyTorch æ·±åº¦å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /ç™¾é¢æ·±åº¦å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ·±åº¦å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ·±åº¦å­¦ä¹ æŽ¨èç³»ç»Ÿ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /å›¾ç¥žç»ç½‘ç»œ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/æ·±åº¦å­¦ä¹ /æ·±åº¦å­¦ä¹ å…¥é—¨2 è‡ªåˆ¶æ¡†æž¶.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/3dè®¡ç®—æœºè§†è§‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/OpenCVè½»æ¾å…¥é—¨ï¼šé¢å‘Python.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/æ·±åº¦å­¦ä¹ ä¸Žç›®æ ‡æ£€æµ‹ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/è§†è§‰SLAMåå››è®² (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/å›¾åƒå·¥ç¨‹ (ç¬¬4ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/OpenCVè®¡ç®—æœºè§†è§‰æ•™ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/æ·±åº¦å­¦ä¹ å…¥é—¨ åŸºäºŽPythonçš„ç†è®ºä¸Žå®žçŽ°.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/æ·±åº¦å­¦ä¹ ä¹‹PyTorchç‰©ä½“æ£€æµ‹å®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å›¾åƒå¤„ç†/æ•°å­—å›¾åƒå¤„ç†ï¼ˆç¬¬å››ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/bertåŸºç¡€æ•™ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/å¤§è§„æ¨¡è¯­è¨€æ¨¡åž‹ ä»Žç†è®ºåˆ°å®žè·µ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/ä¸€æœ¬ä¹¦è¯»æ‡‚AIGCï¼šChatGPTã€AIç»˜ç”»ã€æ™ºèƒ½æ–‡æ˜Žä¸Žç”Ÿäº§åŠ›å˜é©.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/çŸ¥è¯†å›¾è°±ä¸Žæ·±åº¦å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/è‡ªç„¶è¯­è¨€å¤„ç†å¯¼è®º.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/pytorchè‡ªç„¶è¯­è¨€å¤„ç†.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/æ·±åº¦å­¦ä¹ è¿›é˜¶ è‡ªç„¶è¯­è¨€å¤„ç†.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/çŸ¥è¯†å›¾è°±å¯¼è®º.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/è‡ªç„¶è¯­è¨€å¤„ç†ï¼šåŸºäºŽé¢„è®­ç»ƒæ¨¡åž‹çš„æ–¹æ³•.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è‡ªç„¶è¯­è¨€å¤„ç†/è‡ªç„¶è¯­è¨€å¤„ç†å®žæˆ˜.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å¼ºåŒ–å­¦ä¹ /Easy RLå¼ºåŒ–å­¦ä¹ æ•™ç¨‹.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å¼ºåŒ–å­¦ä¹ /åŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å¼ºåŒ–å­¦ä¹ /å¼ºåŒ–å­¦ä¹ ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/å¼ºåŒ–å­¦ä¹ /æ·±åº¦å¼ºåŒ–å­¦ä¹ .jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç”Ÿå­˜æŒ‡å—/ç¨‹åºå‘˜å¥åº·æŒ‡å—.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç”Ÿå­˜æŒ‡å—/è½¯æŠ€èƒ½ ç¬¬2ç‰ˆ.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/ç”Ÿå­˜æŒ‡å—/è½¯æŠ€èƒ½2.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é¢è¯•/ç¨‹åºå‘˜é¢è¯•é‡‘å…¸ï¼ˆç¬¬6ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é¢è¯•/ä»£ç æ•´æ´ä¹‹é“.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é¢è¯•/å‰‘æŒ‡OFFER  åä¼é¢è¯•å®˜ç²¾è®²å…¸åž‹ç¼–ç¨‹é¢˜  (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é¢è¯•/ä½ çœŸçš„ä¼šå†™ä»£ç å—.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é¢è¯•/é‡æž„ (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é¢è¯•/ç¨‹åºå‘˜ä¿®ç‚¼ä¹‹é“ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é¢è¯•/è®¡ç®—æœºç¨‹åºçš„æž„é€ å’Œè§£é‡Š (ç¬¬2ç‰ˆ).jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/é¢è¯•/å‰‘æŒ‡offerï¼ˆä¸“é¡¹çªç ´ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è½¯è€ƒ/åµŒå…¥å¼ç³»ç»Ÿè®¾è®¡å¸ˆæ•™ç¨‹ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è½¯è€ƒ/æ•°æ®åº“ç³»ç»Ÿå·¥ç¨‹å¸ˆæ•™ç¨‹ï¼ˆç¬¬3ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è½¯è€ƒ/ç½‘ç»œå·¥ç¨‹å¸ˆæ•™ç¨‹ï¼ˆç¬¬5ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è½¯è€ƒ/ç½‘ç»œç®¡ç†å‘˜æ•™ç¨‹ï¼ˆç¬¬5ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è½¯è€ƒ/ä¿¡æ¯å®‰å…¨å·¥ç¨‹å¸ˆæ•™ç¨‹ï¼ˆç¬¬2ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/è½¯è€ƒ/ä¿¡æ¯ç³»ç»Ÿé¡¹ç›®ç®¡ç†å¸ˆæ•™ç¨‹ï¼ˆç¬¬4ç‰ˆï¼‰.jpg; https://raw.githubusercontent.com/lining808/CS-Ebook/main/images/end.jpg,,1028,2023-03-25T08:10:33Z
2024-03-04,https://github.com/kcheng1021/GaussianPro,https://raw.githubusercontent.com/kcheng1021/GaussianPro/main/README.md,"GaussianPro introduces a novel method for enhancing 3D Gaussian Splatting (3DGS), a technique critical in neural rendering for producing high-quality renderings swiftly. The existing 3DGS process struggles with large-scale scenes, particularly when dealing with texture-less surfaces, due to its reliance on point clouds initialized by Structure-from-Motion (SfM) techniques which often fail to generate a sufficient number of points. GaussianPro addresses this by employing a progressive propagation strategy, inspired by classical multi-view stereo (MVS) techniques, to guide the densification of the 3D Gaussians. This strategy, leveraging scene geometry and patch matching, accurately positions and orients new Gaussians, thereby significantly improving rendering quality, as validated by experiments on both large and small-scale scenes, showing notable improvement over traditional 3DGS methods. The project has released a beta version of the code and demonstrated its effectiveness through various datasets and YouTube videos, indicating consistent enhancements across different scenarios.",Revolutionizing Neural Rendering with GaussianPro: A 3D Splatting Breakthrough,"GaussianPro introduces a groundbreaking approach to 3D Gaussian Splatting (3DGS) for neural rendering, addressing the limitations of Structure-from-Motion techniques in large-scale scenes. By incorporating a progressive propagation strategy, GaussianPro significantly improves the initialization process, producing high-quality renderings at real-time speeds. This method surpasses conventional 3DGS, particularly in texture-less surfaces, by utilizing reconstructed geometries and patch matching techniques. Validated by experiments on diverse scenes, GaussianPro achieves remarkable performance enhancements, marking a notable advancement in the field of neural rendering.","Discover how GaussianPro enhances neural rendering with an innovative 3D Gaussian Splatting approach, overcoming traditional challenges and leading to superior rendering quality and efficiency.",Deep Learning Platform,"Python





        257





        10


        Built by

          







        31 stars today",https://raw.githubusercontent.com/kcheng1021/GaussianPro/main/figs/comparison.gif; https://raw.githubusercontent.com/kcheng1021/GaussianPro/main/figs/pipeline.png; https://github.com/kcheng1021/GaussianPro/blob/main/figs/output.gif; https://github.com/kcheng1021/GaussianPro/blob/main/figs/output2.gif,,257,2024-02-21T01:57:53Z
2024-03-04,https://github.com/yt-dlp/yt-dlp,https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/README.md,"yt-dlp is a fork of youtube-dl, created to introduce new features and updates while staying aligned with the original project. Major enhancements include SponsorBlock integration to mark/remove sponsored sections in YouTube videos, advanced format sorting capabilities for favoring higher resolution and codecs over bitrate, and merging improvements from animelover1984/youtube-dl which adds features like writing comments, and support for various YouTube content types such as Clips, Stories, and Live. It introduces automatic cookie extraction from browsers, partial video downloads based on timestamps or chapters, multi-threaded fragment downloads, and use of aria2c for DASH and HLS formats. New extractors have been added, extractor issues fixed, and support for new MSOs and subtitle extraction from manifests is included. Enhancements to output templates, more path and template options, plugins support, self-updater, and nightly/master builds for the latest features are also notable additions. Differences from youtube-dl include updated default behavior in options, format selection, and storage locations to prioritize user convenience and performance.",Top New Features in yt-dlp: A Comprehensive Guide,"yt-dlp, a fork of youtube-dl, stands out by introducing an array of new features and updates, ensuring an enhanced video downloading experience. From SponsorBlock integration for skipping sponsor segments to advanced format sorting options prioritizing resolution and codecs, yt-dlp brings significant improvements to video downloading. Enhanced YouTube support, including downloading livestreams from the start and support for age-gated content without cookies, alongside multithreading for faster downloads, makes yt-dlp a go-to solution. Additionally, yt-dlp updates include new extractor support and portability in configuration, catering to a wide range of user preferences.","Discover the latest enhancements and features in yt-dlp, including SponsorBlock integration, improved format sorting, enhanced YouTube support, and faster downloads through multithreading. A must-read for efficient video downloading.",Document Conversion Tool,"Python





        67,184





        5,320


        Built by

          









        71 stars today",,https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKc,67184,2020-10-26T04:22:55Z
2024-03-04,https://github.com/ihmily/DouyinLiveRecorder,https://raw.githubusercontent.com/ihmily/DouyinLiveRecorder/main/README.md,"The DouyinLiveRecorder is a user-friendly live-stream recording tool developed by ihmily and collaborators, capable of capturing live streams across multiple platforms including Douyin, TikTok, Kuaishou, Huya, Douyu, YY, Bilibili, Xiaohongshu, Bigo, Blued, AfreecaTV, Netease CC, Qiandu Hotplay, PandaTV, and Missevan FM, with plans to support more. It utilizes FFmpeg for recording and is customizable through config files. Available on Windows and Linux, it supports Docker for easy deployment. Users can download the latest release, set up through config files, and run it to record live streams in high definition, with various additional features like proxy support for international platforms, quality settings, ongoing monitoring without frequent restarts to avoid IP bans, and detailed instructions for setup and use. The project is open-source, welcoming contributions and provides source code for further customization, Docker support for containerized deployment, and a contributor's list celebrating the community's involvement.",Ultimate Guide to Recording Live Streams Across Platforms with DouyinLiveRecorder,"Discover how to effortlessly record live streams from Douyin, TikTok, and more using the versatile DouyinLiveRecorder tool. This straightforward guide covers everything from initial setup to advanced recording features, ensuring you capture every moment in high quality. Whether you're a newbie or a seasoned user, learn how to configure live stream URLs, customize recording settings, and even pause recordings without losing data. Plus, get insights on new platform additions and updates to keep your recording capabilities at the cutting edge. Embrace the power of recording live content seamlessly across multiple platforms.","Learn how to use DouyinLiveRecorder to capture live streams from Douyin, TikTok, and other platforms. From setup to customization, this guide ensures you never miss a live moment again.",Open Source Tool,"Python





        2,155





        224


        Built by

          







        209 stars today",,,2155,2023-07-17T10:11:12Z
2024-03-04,https://github.com/python-telegram-bot/python-telegram-bot,https://raw.githubusercontent.com/ranaroussi/yfinance/main/README.md,"The document provides an overview of yfinance, an open-source Python library not affiliated with Yahoo, Inc., used for downloading market data from Yahoo! Finance's API for research and educational purposes. It emphasizes the need to adhere to Yahoo!'s terms of use regarding the data's usage. The library offers a Pythonic approach to access ticker data, historical market data, financials, and news among other features, with support for proxy servers. Installation instructions, quick start guides, and information on logging and smarter scraping to avoid spamming Yahoo's API are included. It also touches on managing multi-level column names, integrating yfinance with pandas_datareader for faster data download, and setting up a persistent cache store to reduce Yahoo data requests. The document calls for community contributions to the project and notes its distribution under the Apache Software License, reiterating its purpose for research and educational use only.",Harnessing Yahoo! Finance API for Market Data: A Comprehensive Guide,"Discover how to leverage the Yahoo! Finance API using yfinance for Python to download market data easily. This tutorial offers a Pythonic way to access stock information, historical market data, and more while adhering to Yahoo!'s terms of use. It's essential for research, educational purposes, and personal use, highlighting the community-driven enhancements for efficient data extraction.","Explore a step-by-step guide to downloading market data using the Yahoo! Finance API with yfinance. Ideal for research and educational purposes, learn how to access stock info, historical data, and much more while following Yahoo!'s usage terms.",Python Libraries Collection,"Python





        24,364





        4,970


        Built by

          









        6 stars today",,,11436,2015-07-07T15:30:39Z
2024-03-04,https://github.com/donnemartin/system-design-primer,https://raw.githubusercontent.com/ccxt/ccxt/master/README.md,"The CCXT library is a JavaScript / Python / PHP / C# library for cryptocurrency trading and e-commerce with support for many bitcoin/ether/altcoin exchange markets and merchant APIs. It is used to connect and trade with crypto exchanges and payment processing services globally. It offers quick access to market data for storage, analysis, visualization, indicator development, algorithmic trading, strategy backtesting, bot programming, and related software engineering. The library is designed for coders, developers, technically-skilled traders, data-scientists, and financial analysts for building trading algorithms.

Features include support for many cryptocurrency exchanges, fully implemented public and private APIs, optional normalized data for cross-exchange analytics and arbitrage, and an easy-to-integrate unified API. It works in Node.js, Python, PHP, C#, and browsers. The library is open source under the MIT license, meaning it's free to use in commercial and open-source projects. However, responsibilities and warranties are disclaimed. Installation involves using package managers like npm for JavaScript, pip for Python, or including a script tag for browsers.

The CCXT library comprises a public part accessible immediately upon installation, offering unrestricted access to public information without needing an account or API key. Conversely, private APIs require API keys for actions like account management, trading, and withdrawals.

Contributions are welcome but should follow the contributing guidelines detailed in the documentation. The development team also accepts donations and sponsorship to support and accelerate the library's development.",Maximize Your Crypto Trading with the CCXT Library: The Ultimate Guide,"Discover how the CCXT Library can transform your cryptocurrency trading experience. This ultimate guide covers its features, benefits, and how it supports multiple exchange markets for effective trading. Whether you're a developer, technically-skilled trader, or a financial analyst, CCXT provides the tools you need for market analysis, algorithmic trading, and more. Explore how CCXT can enhance your trading strategies today.","Unlock the potential of cryptocurrency trading with our comprehensive guide on the CCXT Library. Learn how it supports various exchanges, streamlines trading, and provides essential tools for market analysis and algorithmic trading. Start optimizing your trading strategies with CCXT now.",Crypto Trading Bot,"Python





        248,122





        42,583


        Built by

          









        99 stars today",https://user-images.githubusercontent.com/1294454/66755907-9c3e8880-eea1-11e9-846e-0bff349ceb87.png; https://user-images.githubusercontent.com/1294454/114340585-8e35fa80-9b60-11eb-860f-4379125e2db6.png; https://user-images.githubusercontent.com/1294454/132113722-007fc092-7530-4b41-b929-b8ed380b7b2e.png; https://user-images.githubusercontent.com/1294454/152720975-0522b803-70f0-4f18-a305-3c99b37cd990.png; https://user-images.githubusercontent.com/1294454/29604020-d5483cdc-87ee-11e7-94c7-d1a8d9169293.jpg; https://user-images.githubusercontent.com/1294454/117738721-668c8d80-b205-11eb-8c49-3fad84c4a07f.jpg; https://user-images.githubusercontent.com/1294454/117738721-668c8d80-b205-11eb-8c49-3fad84c4a07f.jpg; https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg; https://user-images.githubusercontent.com/1294454/195989417-4253ddb0-afbe-4a1c-9dea-9dbcd121fa5d.jpg; https://user-images.githubusercontent.com/1294454/129991357-8f47464b-d0f4-41d6-8a82-34122f0d1398.jpg; https://user-images.githubusercontent.com/51840849/76547799-daff5b80-649e-11ea-87fb-3be9bac08954.jpg; https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg; https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg; https://user-images.githubusercontent.com/1294454/31784029-0313c702-b509-11e7-9ccc-bc0da6a0e435.jpg; https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg; https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg; https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg; https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg; https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg; https://user-images.githubusercontent.com/1294454/150730761-1a00e5e0-d28c-480f-9e65-089ce3e6ef3b.jpg; https://user-images.githubusercontent.com/1294454/216908003-fb314cf6-e66e-471c-b91d-1d86e4baaa90.jpg; https://user-images.githubusercontent.com/1294454/187234005-b864db3d-f1e3-447a-aaf9-a9fc7b955d07.jpg; https://user-images.githubusercontent.com/1294454/112027508-47984600-8b48-11eb-9e17-d26459cc36c6.jpg; https://user-images.githubusercontent.com/1294454/55248342-a75dfe00-525a-11e9-8aa2-05e9dca943c6.jpg; https://user-images.githubusercontent.com/1294454/69354403-1d532180-0c91-11ea-88ed-44c06cefdf87.jpg; https://user-images.githubusercontent.com/1294454/29604020-d5483cdc-87ee-11e7-94c7-d1a8d9169293.jpg; https://user-images.githubusercontent.com/1294454/117738721-668c8d80-b205-11eb-8c49-3fad84c4a07f.jpg; https://user-images.githubusercontent.com/1294454/65177307-217b7c80-da5f-11e9-876e-0b748ba0a358.jpg; https://user-images.githubusercontent.com/1294454/117738721-668c8d80-b205-11eb-8c49-3fad84c4a07f.jpg; https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg; https://user-images.githubusercontent.com/1294454/27766119-3593220e-5ece-11e7-8b3a-5a041f6bcc3f.jpg; https://user-images.githubusercontent.com/1294454/37808081-b87f2d9c-2e59-11e8-894d-c1900b7584fe.jpg; https://user-images.githubusercontent.com/1294454/117201933-e7a6e780-adf5-11eb-9d80-98fc2a21c3d6.jpg; https://user-images.githubusercontent.com/1294454/27766244-e328a50c-5ed2-11e7-947b-041416579bb3.jpg; https://user-images.githubusercontent.com/1294454/27766244-e328a50c-5ed2-11e7-947b-041416579bb3.jpg; https://user-images.githubusercontent.com/1294454/28051642-56154182-660e-11e7-9b0d-6042d1e6edd8.jpg; https://user-images.githubusercontent.com/1294454/195989417-4253ddb0-afbe-4a1c-9dea-9dbcd121fa5d.jpg; https://user-images.githubusercontent.com/1294454/30597177-ea800172-9d5e-11e7-804c-b9d4fa9b56b0.jpg; https://user-images.githubusercontent.com/1294454/129991357-8f47464b-d0f4-41d6-8a82-34122f0d1398.jpg; https://user-images.githubusercontent.com/1294454/158227251-3a92a220-9222-453c-9277-977c6677fe71.jpg; https://user-images.githubusercontent.com/1294454/139516488-243a830d-05dd-446b-91c6-c1f18fe30c63.jpg; https://user-images.githubusercontent.com/51840849/87295554-11f98280-c50e-11ea-80d6-15b3bafa8cbf.jpg; https://user-images.githubusercontent.com/1294454/27786377-8c8ab57e-5fe9-11e7-8ea4-2b05b6bcceec.jpg; https://user-images.githubusercontent.com/1294454/169202626-bd130fc5-fcf9-41bb-8d97-6093225c73cd.jpg; https://user-images.githubusercontent.com/1294454/28501752-60c21b82-6feb-11e7-818b-055ee6d0e754.jpg; https://user-images.githubusercontent.com/1294454/147515585-1296e91b-7398-45e5-9d32-f6121538533f.jpeg; https://user-images.githubusercontent.com/1294454/42625213-dabaa5da-85cf-11e8-8f99-aa8f8f7699f0.jpg; https://user-images.githubusercontent.com/51840849/87327317-98c55400-c53c-11ea-9a11-81f7d951cc74.jpg; https://user-images.githubusercontent.com/51840849/89731817-b3fb8480-da52-11ea-817f-783b08aaf32b.jpg; https://user-images.githubusercontent.com/51840849/87153926-efbef500-c2c0-11ea-9842-05b63612c4b9.jpg; https://user-images.githubusercontent.com/51840849/76547799-daff5b80-649e-11ea-87fb-3be9bac08954.jpg; https://user-images.githubusercontent.com/1294454/27766442-8ddc33b0-5ed8-11e7-8b98-f786aef0f3c9.jpg; https://user-images.githubusercontent.com/1294454/40811661-b6eceae2-653a-11e8-829e-10bfadb078cf.jpg; https://user-images.githubusercontent.com/1294454/41764625-63b7ffde-760a-11e8-996d-a6328fa9347a.jpg; https://user-images.githubusercontent.com/51840849/87182088-1d6d6380-c2ec-11ea-9c64-8ab9f9b289f5.jpg; https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg; https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/281108917-eff2ae1d-ce8a-4b2a-950d-8678b12da965.jpg; https://user-images.githubusercontent.com/51840849/87460806-1c9f3f00-c616-11ea-8c46-a77018a8f3f4.jpg; https://user-images.githubusercontent.com/1294454/38003300-adc12fba-323f-11e8-8525-725f53c4a659.jpg; https://user-images.githubusercontent.com/1294454/225719995-48ab2026-4ddb-496c-9da7-0d7566617c9b.jpg; https://user-images.githubusercontent.com/1294454/28208429-3cacdf9a-6896-11e7-854e-4c79a772a30f.jpg; https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg; https://user-images.githubusercontent.com/1294454/83718672-36745c00-a63e-11ea-81a9-677b1f789a4d.jpg; https://user-images.githubusercontent.com/1294454/99450025-3be60a00-2931-11eb-9302-f4fd8d8589aa.jpg; https://user-images.githubusercontent.com/1294454/41933112-9e2dd65a-798b-11e8-8440-5bab2959fcb8.jpg; https://user-images.githubusercontent.com/51840849/87443315-01283a00-c5fe-11ea-8628-c2a0feaf07ac.jpg; https://user-images.githubusercontent.com/1294454/27766491-1b0ea956-5eda-11e7-9225-40d67b481b8d.jpg; https://user-images.githubusercontent.com/1294454/159177712-b685b40c-5269-4cea-ac83-f7894c49525d.jpg; https://user-images.githubusercontent.com/1294454/31784029-0313c702-b509-11e7-9ccc-bc0da6a0e435.jpg; https://user-images.githubusercontent.com/1294454/27816857-ce7be644-6096-11e7-82d6-3c257263229c.jpg; https://user-images.githubusercontent.com/1294454/27766555-8eaec20e-5edc-11e7-9c5b-6dc69fc42f5e.jpg; https://user-images.githubusercontent.com/1294454/75841031-ca375180-5ddd-11ea-8417-b975674c23cb.jpg; https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg; https://user-images.githubusercontent.com/1294454/85734211-85755480-b705-11ea-8b35-0b7f1db33a2f.jpg; https://user-images.githubusercontent.com/51840849/94481303-2f222100-01e0-11eb-97dd-bc14c5943a86.jpg; https://user-images.githubusercontent.com/51840849/87182090-1e9e9080-c2ec-11ea-8e49-563db9a38f37.jpg; https://user-images.githubusercontent.com/51840849/87070508-9358c880-c221-11ea-8dc5-5391afbbb422.jpg; https://user-images.githubusercontent.com/51840849/76173629-fc67fb00-61b1-11ea-84fe-f2de582f58a3.jpg; https://user-images.githubusercontent.com/24300605/81436764-b22fd580-9172-11ea-9703-742783e6376d.jpg; https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg; https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg; https://user-images.githubusercontent.com/51840849/87153927-f0578b80-c2c0-11ea-84b6-74612568e9e1.jpg; https://user-images.githubusercontent.com/1294454/61511972-24c39f00-aa01-11e9-9f7c-471f1d6e5214.jpg; https://user-images.githubusercontent.com/1294454/38063602-9605e28a-3302-11e8-81be-64b1e53c4cfb.jpg; https://user-images.githubusercontent.com/1294454/27766607-8c1a69d8-5ede-11e7-930c-540b5eb9be24.jpg; https://user-images.githubusercontent.com/1294454/155840500-1ea4fdf0-47c0-4daa-9597-c6c1cd51b9ec.jpg; https://user-images.githubusercontent.com/1294454/27837060-e7c58714-60ea-11e7-9192-f05e86adb83f.jpg; https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg; https://user-images.githubusercontent.com/1294454/108623144-67a3ef00-744e-11eb-8140-75c6b851e945.jpg; https://user-images.githubusercontent.com/1294454/92337550-2b085500-f0b3-11ea-98e7-5794fb07dd3b.jpg; https://user-images.githubusercontent.com/1294454/58385970-794e2d80-8001-11e9-889c-0567cd79b78e.jpg; https://user-images.githubusercontent.com/51840849/87295551-102fbf00-c50e-11ea-90a9-462eebba5829.jpg; https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg; https://user-images.githubusercontent.com/51840849/87153930-f0f02200-c2c0-11ea-9c0a-40337375ae89.jpg; https://user-images.githubusercontent.com/1294454/85225056-221eb600-b3d7-11ea-930d-564d2690e3f6.jpg; https://user-images.githubusercontent.com/1294454/27766817-e9456312-5ee6-11e7-9b3c-b628ca5626a5.jpg; https://user-images.githubusercontent.com/1294454/27766817-e9456312-5ee6-11e7-9b3c-b628ca5626a5.jpg; https://user-images.githubusercontent.com/51840849/79268032-c4379480-7ea2-11ea-80b3-dd96bb29fd0d.jpg; https://user-images.githubusercontent.com/1294454/70423869-6839ab00-1a7f-11ea-8f94-13ae72c31115.jpg; https://user-images.githubusercontent.com/1294454/183870484-d3398d0c-f6a1-4cce-91b8-d58792308716.jpg; https://user-images.githubusercontent.com/1294454/49245610-eeaabe00-f423-11e8-9cba-4b0aed794799.jpg; https://user-images.githubusercontent.com/1294454/84547058-5fb27d80-ad0b-11ea-8711-78ac8b3c7f31.jpg; https://user-images.githubusercontent.com/1294454/148647666-c109c20b-f8ac-472f-91c3-5f658cb90f49.jpeg; https://user-images.githubusercontent.com/1294454/66732963-8eb7dd00-ee66-11e9-849b-10d9282bb9e0.jpg; https://user-images.githubusercontent.com/1294454/150730761-1a00e5e0-d28c-480f-9e65-089ce3e6ef3b.jpg; https://user-images.githubusercontent.com/1294454/27766910-cdcbfdae-5eea-11e7-9859-03fea873272d.jpg; https://user-images.githubusercontent.com/1294454/27766927-39ca2ada-5eeb-11e7-972f-1b4199518ca6.jpg; https://user-images.githubusercontent.com/1294454/159202310-a0e38007-5e7c-4ba9-a32f-c8263a0291fe.jpg,,30661,2017-02-26T16:15:28Z
2024-03-04,https://github.com/ccxt/ccxt,https://raw.githubusercontent.com/Sanster/IOPaint/main/README.md,"IOPaint is a free, open-source tool designed for inpainting and outpainting tasks, utilizing state-of-the-art AI models. It's fully self-hosted and supports both CPU and GPU, including Apple Silicon, making it accessible for a wide range of users. The platform offers a versatile range of AI models for erasing unwanted objects or defects from images and for creative tasks like object replacement or outpainting. Models cater to various needs, from erasing to diffusion models capable of intricate modifications. Additionally, IOPaint provides plugins for functions like object segmentation, background removal, super-resolution, and face restoration, enhancing its utility. Users can easily manage their projects with features like a file manager and batch processing capabilities. The software facilitates easy installation and use via web UI or command line, supporting a broad spectrum of AI-driven image editing tasks with detailed instructions for setup and application. Development is actively supported, with resources available to contribute to both the front and back-end components.",Revolutionize Your Image Editing with IOPaint: The Ultimate AI-Powered Tool,"Discover IOPaint, the cutting-edge, free, and open-source image editing tool powered by state-of-the-art AI technology. With support for CPU, GPU, and Apple Silicon, this tool offers an array of features including erase, inpainting, outpainting, and various plugins. From removing unwanted objects to enhancing image quality, IOPaint caters to all your image editing needs. The tool is easily accessible through a one-click installer for Windows or directly from its GitHub repository. Transform your images effortlessly with IOPaint's advanced AI models and user-friendly web interface.","Explore IOPaint, a free AI-based image editing tool for inpainting & outpainting. Supports CPU/GPU, features 1-click Windows installer, various AI models & plugins.",Image Generation Platform.,"Python





        30,661





        7,147


        Built by

          









        14 stars today",,https://github.com/Sanster/IOPaint/assets/3998421/264bc27c-0abd-4d8b-bb1e-0078ab264c4a; https://github.com/Sanster/IOPaint/assets/3998421/1de5c288-e0e1-4f32-926d-796df0655846; https://github.com/Sanster/IOPaint/assets/3998421/ffd4eda4-f7d4-4693-93d8-d2cd5aa7c6d6; https://github.com/Sanster/IOPaint/assets/3998421/c4af8aef-8c29-49e0-96eb-0aae2f768da2,16163,2017-05-14T15:41:56Z
2024-03-04,https://github.com/Sanster/IOPaint,https://raw.githubusercontent.com/intel/intel-npu-acceleration-library/main/README.md,"The IntelÂ® NPU Acceleration Library is a Python library aimed at enhancing application performance through the Intel Neural Processing Unit (NPU), specifically designed for high-speed computations on compatible hardware. It is in active development, aiming to incorporate features to significantly boost efficiency. The Intel NPU, embedded within Intel Core Ultra processors, features a unique architecture that accelerates AI operations through Neural Compute Engines and supports general computing tasks. Its performance is optimized by efficient data transfer mechanisms and software that utilizes compiler technology for AI workload optimization. The libraryâ€™s upcoming enhancements include various quantization support and mixed precision inference among others. To use, the library requires an available NPU and can be installed via pip. It supports operations like matrix multiplication and model compilation for the NPU, with examples provided for practical implementation.",Maximize Your Application's Performance with Intel NPU Acceleration Library,"Discover how the IntelÂ® NPU Acceleration Library can enhance your computing tasks by utilizing the advanced capabilities of Intel Neural Processing Units (NPUs). This Python library is tailored for developers seeking to boost their applications' performance on Intel Core Ultra processors, featuring support for complex AI operations. Currently under active development, the library promises exciting features aimed at significantly improving performance across various computing domains. Learn how to install, set up, and start leveraging the library for high-speed computations, and stay tuned for upcoming enhancements that will take your computing efficiency to the next level.","Learn about the Intel NPU Acceleration Library, a groundbreaking tool designed to improve application efficiency through Intel's Neural Processing Units. Explore its features, setup guide, and future enhancements.",Python Libraries Collection,"Python





        16,163





        1,613


        Built by

          









        35 stars today",,https://www.youtube.com/watch?v=QSzNoX0qplE,167,2021-11-15T14:16:40Z
2024-03-04,https://github.com/intel/intel-npu-acceleration-library,https://raw.githubusercontent.com/xaitax/SploitScan/main/README.md,"SploitScan is a dynamic cybersecurity tool designed to efficiently identify exploits for known vulnerabilities and assess exploitation probabilities. It aims to aid cybersecurity professionals in enhancing security measures and developing robust detection strategies for emerging threats. Key features include CVE information retrieval from the National Vulnerability Database, integration with the Exploit Prediction Scoring System (EPSS) for exploit likelihood scoring, aggregation of public exploits, display of CVEs listed in CISA's Known Exploited Vulnerabilities, a patching priority system, support for multiple CVE queries, and easy export options to JSON and CSV. With a user-friendly interface, SploitScan serves as a comprehensive tool for quick security assessments. It also supports various exploit databases like GitHub, ExploitDB, and VulnCheck (requiring an API key). Recent updates include enhancements in CVSS support, Docker support, and the introduction of a prioritization system for security patches, guiding users on which vulnerabilities to address first. Contributions from the cybersecurity community are encouraged, reflecting collaborative efforts in improving SploitScan.",Maximizing Cybersecurity with SploitScan: A Comprehensive Tool for Exploit Detection,"Discover SploitScan, a user-centric tool designed to revolutionize exploit detection and vulnerability assessment in the cybersecurity arena. With features like CVE Information Retrieval, EPSS Integration, and a Patching Priority System, SploitScan empowers professionals to prioritize and patch vulnerabilities effectively. Its integration with major exploit databases and a user-friendly interface makes it indispensable for enhancing security postures. Ideal for quick assessments and advanced security strategy development, SploitScan is your go-to for staying ahead of potential threats.","Explore SploitScan, the ultimate tool for cybersecurity professionals. Streamline exploit detection, access comprehensive CVE details, and prioritize patches with ease. Elevate your security strategy now.",Cybersecurity Tool,"Python





        167





        7


        Built by

          






        71 stars today",,,456,2024-02-20T20:33:18Z
2024-03-05,https://github.com/layerdiffusion/sd-forge-layerdiffuse,https://raw.githubusercontent.com/layerdiffusion/sd-forge-layerdiffuse/main/README.md,"The text outlines the development and features of ""sd-forge-layerdiffuse,"" an extension for SD WebUI through Forge, designed to create transparent images and layers using latent transparency. Currently, the extension supports basic image generation and layer functionality, but the img2img feature for transparent images is still under development. The extension is evolving, and changes are expected, making it vital for professional users to back up their files regularly. It includes a demonstration of the process, indicating the capability to generate transparent effects unachievable with standard background removal methods. Various models are released for specific purposes, such as converting SDXL models into transparent image generators or layer generating models conditioned on foregrounds or backgrounds. Some models are slated for future release to improve performance and capabilities. Sanity checks are recommended for users to ensure their setup yields consistent results, demonstrated through various example prompts and outcomes. The text also includes FAQs and conditions for generating specific images, emphasizing the flexibility and potential of ""sd-forge-layerdiffuse"" in content creation with transparent images.",Revolutionizing Image Generation with SD-Forge LayerDiffuse: Transparent Layers Unveiled,"Discover the future of image creation with SD-Forge LayerDiffuse, a cutting-edge extension designed for the SD WebUI platform. This amazing tool allows users to generate transparent images and layers with unprecedented clarity and detail. While still a work in progress, basic functionalities such as image and layer generation are operational, promising a significant advancement in transparent img2img technology. The dynamic nature of this codebase promises exciting updates, catering especially to professional content creation studios seeking precision and consistency in their projects. Stay tuned for more updates as this technology evolves to redefine the standards of image generation.","Explore SD-Forge LayerDiffuse for SD WebUI, an innovative extension for creating detailed transparent images and layers. Ideal for professionals, it promises precise, high-quality image generation. Learn more about its features and future updates.",Image Generation Platform.,"Python





        2,158





        211


        Built by

          





        289 stars today",,,2158,2024-03-01T06:41:32Z
2024-03-05,https://github.com/huchenlei/ComfyUI-layerdiffuse,https://raw.githubusercontent.com/huchenlei/ComfyUI-layerdiffuse/main/README.md,"ComfyUI-layerdiffuse is an implementation of LayerDiffuse tailored for the ComfyUI environment. Users can install it by downloading the repository to the `custom_nodes` folder in ComfyUI or cloning it via GIT from the same location. Python dependencies are installed with `pip install -r requirements.txt`, though version conflicts with `diffusers` may occur, suggesting the use of separate Python venvs. The implementation supports a range of workflows for generating and blending foreground (FG) and background (BG), including separating RGB images with alpha channels, blending given FG/BG, extracting FG or BG from blended images, and generating FG from BG. A notable feature is the `stop at` parameter in some workflows, which enhances denoising process control. Currently, it primarily supports SDXL, with ongoing tasks including various conditioning and extraction capabilities. The workflows demonstrate color variations in FG extraction, pending confirmation from the LayerDiffuse authors if these are expected behaviors.",Integrate LayerDiffuse with ComfyUI for Enhanced Image Manipulation,"Explore the power of LayerDiffuse and ComfyUI integration for advanced image manipulation workflows. This guide covers installation steps, including a direct GitHub clone or download, and setting up necessary Python dependencies with potential conflict resolutions. Dive into comprehensive workflows for generating foregrounds, blending, and extracting elements with ease, all supported currently by SDXL. Enhance your projects by leveraging these innovative techniques for more control and creative freedom in image processing.","This post provides a succinct guide on integrating LayerDiffuse with ComfyUI for advanced image manipulation, including installation, setting up Python venvs to avoid dependency conflicts, and exploring various workflows for creative image processing.",Custom Node Pack,"Python





        344





        29


        Built by

          






        83 stars today",,,344,2024-03-02T22:56:05Z
2024-03-05,https://github.com/yangjianxin1/Firefly,https://raw.githubusercontent.com/yangjianxin1/Firefly/master/README.md,"Firefly is an open-source project offering a one-stop tool for training large models efficiently. It provides support for pretraining, instruction fine-tuning (SFT), and DPO tasks across a wide range of large models such as Gemma, MiniCPM, Llama, and Bloom among others. The project highlights its three training methodologies, full parameter training, and efficient training techniques LoRA and QLoRA. Specifically, QLoRA is recommended for those with limited training resources, proven effective in Open LLM Leaderboard standings. The project includes support for most mainstream open-source large models and aligns its training with official chat model templates while also offering an assortment of instruction fine-tuning datasets and open-sourced model weights. Additionally, Firefly has demonstrated the effectiveness of its QLoRA training process on the Open LLM Leaderboard and has facilitated multiple language model innovations including LongQLoRA for context length extension, LLMPruner for model size reduction, and a Chinese version of LLaMA2 for efficient training. Model weights for various adaptations such as Firefly-mixtral-8x7b and Firefly-LLaMA2-Chinese among others are available. The project also emphasizes various related projects aiming at model training optimizations, and a list of technical blogs providing insights into large model training and optimization techniques. Firefly stands out for its comprehensive approach towards efficiently training and fine-tuning large models for a variety of applications.",Firefly: The One-Stop Training Tool for Large Language Models,"Discover Firefly, the open-source project revolutionizing the training of large language models such as Gemma, MiniCPM, and Llama. Supporting efficient strategies like QLoRA for instruction fine-tuning, Firefly enables effective model training with limited resources. Join the tech community and access a comprehensive array of mainstream large models, contributing to advancements in AI language training.","Learn how Firefly, an open-source project, simplifies the training of large language models with efficient techniques like QLoRA, supporting a wide range of models including Gemma and MiniCPM. Join the vibrant tech community today.",Deep Learning Platform,"Python





        3,889





        335


        Built by

          





        15 stars today",https://img.shields.io/badge/å¾®ä¿¡äº¤æµç¾¤-Firefly-brightgreen?logo=wechat)](./pics/wechat-group.jpeg; https://raw.githubusercontent.com/yangjianxin1/Firefly/master/pics/firefly_logo.png; https://raw.githubusercontent.com/yangjianxin1/Firefly/master/pics/gongzhonghao.png; https://raw.githubusercontent.com/yangjianxin1/Firefly/master/pics/task_distribution.png; https://raw.githubusercontent.com/yangjianxin1/Firefly/master/pics/gongzhonghao.jpeg,,3889,2023-04-02T14:55:59Z
2024-03-05,https://github.com/hpcaitech/ColossalAI,https://raw.githubusercontent.com/hpcaitech/ColossalAI/main/README.md,"Colossal-AI is a system designed to make training large AI models more efficient, cost-effective, and accessible. It offers a suite of parallel components to enable easy and scalable model training, supporting various parallelism strategies including Data, Pipeline, Tensor, and more. Colossal-AI also provides features for heterogeneous memory management and user-friendly configuration-based parallelism setup. Recent updates include cost reduction and enhancement solutions in various applications like Open-Sora for replication and Colossal-LLaMA-2 for domain-specific language model training. It showcases significant advancements in inference performance, training efficiency for massive models, AI-generated content acceleration, and biomedicine. The system is designed for both large-scale distributed training and efficient single-GPU training, with comprehensive support for a wide range of models and tasks. Installation is straightforward via PyPI or from the source, with Docker support available for easy environment setup. The community-driven project encourages contributions and offers extensive documentation, tutorials, and examples to facilitate adoption and innovation in AI model training.","Revolutionizing AI with Colossal-AI: Cheaper, Faster, More Accessible","Discover how Colossal-AI is transforming the AI landscape by making large AI models more affordable, quicker, and easily accessible. With its latest updates, Colossal-AI offers enhanced parallelism, reduced costs, and comprehensive support for existing and upcoming AI models. Dive into the world where advanced AI technology is now within reach for researchers and organizations worldwide, thanks to the innovative solutions provided by Colossal-AI.","Explore how Colossal-AI is making significant strides in AI by facilitating cheaper, faster, and more accessible large model training. Learn about its latest features, including parallelism enhancements and cost reductions.",Deep Learning Platform.,"Python





        36,541





        4,102


        Built by

          









        20 stars today",https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png; https://img.shields.io/badge/å¾®ä¿¡-åŠ å…¥-green?logo=wechat&amp)](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/JamesDemmel_Colossal-AI.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-1.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-2.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20YouTube.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20Speed.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/llama2_pretraining.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA_pretraining.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/MOE_training.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT3-v5.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BERT.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/ViT.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-GPU1.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-NVME.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/PaLM-GPU1.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference_GPT-3.jpg; https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BLOOM%20serving.png,https://www.youtube.com/watch?v=KnXSfjqkKN0; https://www.youtube.com/watch?v=HcTiHzApHm0; https://www.youtube.com/watch?v=HcTiHzApHm0; https://www.youtube.com/watch?v=-qFBZFmOJfg,36541,2021-10-28T16:19:44Z
2024-03-05,https://github.com/StavC/ComPromptMized,https://raw.githubusercontent.com/StavC/ComPromptMized/master/README.md,"The paper presents Morris II, the first computer worm designed to target GenAI-powered applications within Generative AI (GenAI) ecosystems. The worm leverages adversarial self-replicating prompts to exploit GenAI models, compelling them to replicate malicious inputs, engage in harmful activities, and propagate these inputs to additional GenAI systems. Demonstrations were conducted against GenAI-powered email assistants, showing the worm's capability to spam and exfiltrate personal data under various conditions using text and images against three GenAI models. The performance of the worm, including its propagation rate and malicious activities, was evaluated to understand its impact on GenAI ecosystems. The research contributes to understanding and mitigating new forms of malware that could exploit interconnected GenAI applications.",Exploring ComPromptMized: The First Zero-click Worm in GenAI Ecosystems,"The breakthrough study of ComPromptMized unveils a pioneering zero-click worm, Morris II, targeting GenAI-powered applications, marking a digital leap in cyber-attacks within GenAI ecosystems. Demonstrating unparalleled capability, this worm manipulates GenAI services to propagate malicious activities, from data exfiltration to spam, testing against major GenAI models. Its unique method of self-replication and propagation through adversarial inputs highlights a significant vulnerability in interconnected GenAI applications, urging the need for advanced security measures. As the first of its kind, Morris II's exploration into GenAI-powered email assistants underpins the critical intersection of artificial intelligence and cybersecurity, warranting immediate attention from developers and cybersecurity experts.","Discover ComPromptMized, a groundbreaking study revealing Morris II, the first zero-click worm targeting GenAI ecosystems, capable of self-replicating malicious activities across GenAI-powered applications.",Cybersecurity Tool,"Python





        83





        8


        Built by

          





        29 stars today",https://raw.githubusercontent.com/StavC/ComPromptMized/master/Assets/Icon.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/Assets/InfoLeak.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/Assets/DJISpam.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/FlowSteering/assets/OriginalProcessedImages/America.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/FlowSteering/assets/PerturbatedImages/AmericaPerturbClassForward.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/FlowSteering/assets/OriginalProcessedImages/Cat.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/FlowSteering/assets/PerturbatedImages/CatPerturbClassForward.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/FlowSteering/assets/OriginalProcessedImages/Dji.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/FlowSteering/assets/PerturbatedImages/DjiPerturbClassForward.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/FlowSteering/assets/OriginalProcessedImages/Trump.png; https://raw.githubusercontent.com/StavC/ComPromptMized/master/FlowSteering/assets/PerturbatedImages/TrumpPerturbClassForward.png,https://www.youtube.com/watch?v=FL3qHH02Yd4,83,2024-02-26T23:21:05Z
2024-03-05,https://github.com/NanmiCoder/MediaCrawler,https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README.md,"This text provides a detailed overview of a repository that contains several social media crawler tools, including those for Xiaohongshu, Douyin, Kuaishou, Bilibili, and Weibo. These tools are capable of fetching a variety of data such as videos, images, comments, likes, and shares using Playwright to navigate webpages. The disclaimer emphasizes that the repository's content is for learning and reference only, forbidding commercial use and illegal activities. It absolves itself from legal liabilities arising from misuse. The document also outlines the functionalities provided by the crawlers, such as support for different login methods, keyword search, specified content fetching, data saving options, and the use of IP proxies. Installation and usage instructions are provided, including creating a Python virtual environment, installing dependencies, and running the crawler programs. The document highlights the importance of complying with legal restrictions, encourages contributions through pull requests, and includes a section for donations to support the project.",Maximizing Web Scraping Capabilities for Digital Content: A Guide to Advanced Techniques,"Discover the high efficiency of web scraping in capturing digital content across platforms like Xiaohongshu, Douyin, Kuaishou, Bilibili, and Weibo using Playwright. Learn the ethical use of scraping technology for research and study, while adhering to legal standards to avoid misuse. This post also introduces a community for crawler technology exchange, encouraging contributions and shared learning. Additionally, it outlines a step-by-step guide on setting up the environment, running the scraper, and storing data effectively. Remember, responsible use of scraping technology fosters innovation without infringing on others' rights.","Explore the art of web scraping with a focus on platforms like Xiaohongshu, Douyin, and others. Learn setup and execution tips while upholding ethical standards for data collection and usage.",Open Source Tool,"Python





        1,382





        320


        Built by

          








        23 stars today",https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/static/images/wechat_pay.jpeg; https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/static/images/zfb_pay.jpeg,,1382,2023-06-09T12:14:34Z
2024-03-05,https://github.com/apple/pfl-research,https://raw.githubusercontent.com/apple/pfl-research/main/README.md,"The `pfl` Python framework, designed by Apple, facilitates privacy-focused federated learning (FL) simulations for researchers, emphasizing efficient and confidential result dissemination. Not suitable for third-party FL deployments, it nonetheless offers insights beneficial for actual FL applications. It supports rapid simulations across distributed setups, adaptable API for innovative PFL concepts, scalable state-of-the-art algorithm simulations, compatibility with PyTorch and TensorFlow, and integration with diverse models (including beyond neural networks). Privacy features like differential privacy are also well-integrated. Installation is straightforward via PyPI, and resources like tutorial notebooks and benchmarks are available for immediate hands-on learning and research. Contributions to `pfl` are welcomed, with detailed guidelines provided on its website.",Introducing pfl: Revolutionizing Privacy in Federated Learning with Apple's Python Framework,"Discover 'pfl,' a cutting-edge Python framework by Apple designed for private federated learning (FL) simulations, aiming to enhance research dissemination and efficiency. This tool is exclusively crafted for researchers to conduct fast, scalable, and privacy-aware FL simulations, supporting both PyTorch and TensorFlow. Dive into flexible, powerful FL simulations with 'pfl' and contribute to open research by publishing impactful papers confidently. Whether you're experimenting with different models or integrating privacy features, 'pfl' offers the tools for groundbreaking FL advancements.","Explore 'pfl', an innovative Python framework by Apple for private federated learning simulations. Accelerate FL research with scalable, flexible simulations supporting PyTorch, TensorFlow, and robust privacy features.",Deep Learning Framework,"Python





        93





        11


        Built by

          









        21 stars today",,,93,2023-12-15T22:39:21Z
2024-03-05,https://github.com/langchain-ai/langchain,https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md,"LangChain is a versatile framework aimed at building context-aware applications that leverage language models for reasoning. It offers both Python and JavaScript libraries, complete with various tools and integrations for easy creation and customization of applications. Key components include support for managing prompts, optimizing them, data retrieval, and defining agents' actions. The framework facilitates building retrieval-augmented generation systems, analyzing structured data, and creating chatbots, among other applications. LangChain includes off-the-shelf chains for quick deployment and a collection of templates and libraries like LangServe and LangGraph for REST API deployment and building stateful, multi-actor applications, respectively. LangSmith serves as a developer platform for testing, monitoring, and integrating LLM-based chains. Open-source with an MIT license, LangChain encourages community contributions to enhance its capabilities further. Installation can be done via pip or conda, and detailed documentation, use cases, and best practice guides are available online for developers.",Revolutionizing Reasoning Applications with LangChain: The Ultimate Guide,"Discover how LangChain, the advanced framework for creating context-aware reasoning applications, is transforming the development landscape. Integrating seamlessly with language models, LangChain enables applications that are not only context-aware but also capable of sophisticated reasoning. From quick installation methods via pip or conda to a comprehensive ecosystem comprising libraries, templates, and developer platforms, LangChain offers an unparalleled toolkit for developers. Explore how LangChain facilitates the creation of diverse applications, such as chatbots and retrieval augmented generation, through its modular components and pre-built chains. Embrace the future of application development with LangChainâ€™s innovative approach to utilizing language models.","Explore the capabilities of LangChain, a powerful framework designed for building context-aware reasoning applications. Discover its easy installation, diverse ecosystem, and how it revolutionizes app development with language models.",Collaborative AI Framework.,"Python





        78,287





        11,803


        Built by

          









        127 stars today",,,78287,2022-10-17T02:58:36Z
2024-03-05,https://github.com/microsoft/promptflow,https://raw.githubusercontent.com/microsoft/promptflow/main/README.md,"Prompt flow is a comprehensive toolset aimed at simplifying the development lifecycle of LLM-based AI applications, covering everything from initial idea generation, through prototyping, testing, and evaluation, to final deployment and monitoring. It facilitates prompt engineering, making it easier to develop high-quality LLM applications. Users can effortlessly create, debug, and iteratively improve their projects, ensuring they are of high quality and performant. The suite supports streamlined development for production and offers optional team collaboration features through its cloud version on Azure AI. In addition to facilitating easy setup and interaction with APIs like OpenAI, it provides a VS Code extension for an improved UI-based development experience. The project encourages community involvement through discussions, issue reporting, and contributions, aiming to continuously improve its offerings. It's licensed under MIT, welcoming contributions under a Contributor License Agreement with a focus on communal growth and adherence to a code of conduct.",Mastering Prompt Flow: Streamline Your AI Application Development,"Discover how Prompt Flow revolutionizes the development of LLM-based AI applications by making prompt engineering simpler and more efficient. Learn to create, debug, evaluate, and deploy your AI flows with ease, ensuring high quality and performance. Dive into the benefits of integrations with popular platforms and tools, and how collaborative features in the cloud version further enhance productivity. Whether you're working on your local setup or leveraging cloud platforms, Prompt Flow offers a comprehensive suite of tools to bring your AI application from concept to production seamlessly. Unlock the full potential of your AI projects with Prompt Flowâ€™s advanced features and community support.","Explore how Prompt Flow simplifies the creation, evaluation, and deployment of LLM-based AI applications. Learn about its comprehensive tools and features that support developers from ideation to production.",AI Coding Assistant,"Python





        7,508





        574


        Built by

          









        11 stars today",https://raw.githubusercontent.com/microsoft/promptflow/main/examples/tutorials/quick-start/media/vsc.png,,7508,2023-06-30T06:03:06Z
2024-03-05,https://github.com/odoo/odoo,https://raw.githubusercontent.com/odoo/odoo/master/README.md,"Odoo is a comprehensive suite of web-based, open-source business applications. It offers a variety of apps including Open Source CRM, Website Builder, eCommerce, Warehouse Management, Project Management, Billing & Accounting, Point of Sale, Human Resources, Marketing, and Manufacturing. These apps can operate independently or integrate seamlessly to function as a complete Open Source ERP system. The platform provides a range of resources to get started, including detailed setup instructions, an eLearning platform, and a business game named Scale-up to help users learn the software. Additionally, developers have access to tutorials to begin building with Odoo. The provided links offer further assistance, documentation, and access to nightly builds.",Maximizing Business Efficiency with Odoo: A Comprehensive Guide to Open Source Apps,"Discover the potential of Odoo, a suite of web-based, open-source business applications designed to enhance efficiency. From CRM and eCommerce to project management and accounting, Odoo offers a versatile range of apps that can operate independently or together as a fully integrated ERP system. Ideal for businesses seeking scalable solutions, Odooâ€™s diverse apps are seamlessly integrated, ensuring a cohesive and comprehensive business management platform. Learn how to get started with Odoo and leverage its full capabilities to drive your business forward.","Explore the benefits of Odoo, a suite of open-source business apps for CRM, eCommerce, project management, and more. Learn how Odoo can streamline operations and boost efficiency across your business.",Open Source ERP,"Python





        33,653





        21,853


        Built by

          









        32 stars today",,,33653,2014-05-13T15:38:58Z
2024-03-05,https://github.com/tiangolo/fastapi,https://raw.githubusercontent.com/aappleby/hancho/main/README.md,"Hancho is a streamlined build system inspired by Ninja's speed and simplicity and Bazel's syntax and extensibility. It is coded in under 500 lines of Python, requiring no separate installationâ€”users simply copy it into their source tree. Unlike Ninja, Hancho doesn't mandate a separate build rule for each output file, and akin to Bazel, it allows invoking build rules like function calls with keyword arguments. However, it uniquely allows build rules to execute arbitrary Python code. Aimed at small to medium projects, Hancho has seen recent updates, including improvements in formatting and the handling of unrecognized command line flags. It supports parallel job execution, verbose and quiet modes, dry runs, debugging, and forced rebuilds. A simple example provided demonstrates compiling and linking a ""Hello World"" program in C++.",Simplify Your Build Process with Hancho: The Easy-to-Integrate Build System,"Discover Hancho, the minimalist build system inspired by Ninja's speed and Bazel's syntax, designed for simplicity and extensibility. Requiring no installation and fitting comfortably in under 500 lines of Python, Hancho streamlines the build process for small to medium projects without the complexity. Unlike traditional systems, Hancho allows invoking build rules as functions and integrating custom Python code, offering flexibility and speed. Perfect for developers seeking an efficient and straightforward building solution, Hancho simplifies while it accelerates.","Explore Hancho, a compact build system blending Ninja's speed with Bazel's extensibility in under 500 lines of Python. Ideal for small to medium projects, it simplifies the build process without sacrificing functionality.",Software Development,"Python





        68,924





        5,759


        Built by

          








        55 stars today",https://raw.githubusercontent.com/aappleby/hancho/main/hancho_small.png,,235,2018-12-08T08:21:47Z
2024-03-06,https://github.com/vikhyat/moondream,https://raw.githubusercontent.com/vikhyat/moondream/main/README.md,"Moondream is an impressive, compact vision language model capable of running universally. The model's ability to understand and generate language based on visual input marks a significant advancement in AI. Moondream2, the latest version with 1.86 billion parameters, improves on its predecessor in various visual question answering benchmarks. It incorporates weights from renowned models SigLIP and Phi 1.5, enhancing its performance. The model can be used easily with transformers, requiring installations of several libraries and following specific Python code snippets to function optimally. However, it's updated regularly, suggesting users pin a specific version to ensure stability. Despite its capabilities, users should be cautious of its limitations like generating inaccurate or biased statements, and the potential for creating offensive content.",Introducing moondream: The Compact Vision Language Model Revolutionizing AI,"Discover moondream, a versatile vision language model with 1.86B parameters empowering AI across platforms with benchmarks surpassing its predecessors. This cutting-edge model integrates seamlessly with tools like transformers, featuring easy installation and user-friendly APIs for diverse applications. Its regular updates ensure optimal performance, though users should note potential limitations like bias and accuracy. Explore moondream for innovative AI solutions that run anywhere.","Explore the power of moondream, a cutting-edge vision language model designed for flexibility and efficiency in AI applications. Learn about its benchmarks, easy integration, and how to harness its capabilities for your AI projects.",Artificial Intelligence,"Python





        2,433





        200


        Built by

          









        90 stars today",https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg; https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-2.jpg,,2433,2023-12-29T00:27:18Z
2024-03-06,https://github.com/Ryujinx/release-channel-master,https://raw.githubusercontent.com/Ryujinx/release-channel-master/master/README.md,"This text introduces a repository (repo) hosting the releases for the master build channel of Ryujinx, which is a software project. It includes a link to the releases page of the repo and a link to the main page of the Ryujinx project on GitHub. Essentially, it serves as a pointer for those interested in accessing the latest versions of the Ryujinx master build, directing users to where these releases can be found and providing a route to learn more about the Ryujinx project itself.",Exploring Ryujinx Master Build Releases: A Comprehensive Guide,"Discover what the Ryujinx master build channel has to offer by exploring its latest releases. The repository on GitHub is your ultimate source for downloading and staying updated with the most advanced versions of Ryujinx. Whether you're a developer or a gaming enthusiast, understanding these releases can enhance your emulation experience. Dive into the world of Ryujinx and unlock the full potential of your gaming adventures.","Get an in-depth look at the Ryujinx master build channel releases. Learn how to access, download, and utilize the latest versions for an improved gaming experience.",Software Development,"Python





        949





        76


        Built by

          





        13 stars today",,,949,2022-01-22T16:01:09Z
2024-03-06,https://github.com/gradio-app/gradio,https://raw.githubusercontent.com/gradio-app/gradio/main/README.md,"Gradio is an open-source Python library designed to easily create demos or web applications for machine learning models or arbitrary Python functions, without needing web development skills. Users can quickly share their applications via a built-in sharing feature, promoting user-friendly project dissemination. Gradio requires Python 3.8+ and can be installed with pip. It allows for the creation of interfaces with simple Python code enabling interactive demos. It supports various inputs and outputs, including text and sliders. Gradio offers key components for building machine learning applications such as `Interface`, `ChatInterface`, and `Blocks`, for diverse needs from simple function demonstrations to complex web apps. Additionally, Gradio integrates with other platforms and tools, forming a broad ecosystem for ML application development. It also emphasizes community engagement, bug reporting, and contributions while being under an Apache 2.0 license.",Easily Build and Share Machine Learning Web Apps with Gradio,"Discover how Gradio, an open-source Python package, empowers you to create and share ML model demos or web apps effortlessly, without web hosting or JavaScript knowledge. With Gradio, transform any Python function into a user-friendly interface, allowing for instantaneous sharing via built-in features. Perfect for machine learning enthusiasts and professionals looking to streamline their model demonstrations. Start building your first Gradio demo today with just a few lines of Python code!","Learn how to quickly build and share web applications for your machine learning models with Gradio - no web development experience required. Start creating beautiful, interactive demos in no time.",Python Libraries Collection,"Python





        26,938





        1,933


        Built by

          









        39 stars today",https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif; https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_world_4/screenshot.gif,,26939,2018-12-19T08:24:04Z
2024-03-06,https://github.com/QwenLM/Qwen-Agent,https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/README.md,"Qwen-Agent is a development framework designed for applications utilizing large language models (LLMs) that focus on instruction following, tool usage, planning, and memory. With examples like Browser Assistant, Code Interpreter, and Custom Assistant, it showcases its versatility. To get started, users clone the repository, install dependencies, and can choose between Alibaba Cloud's DashScope or their own model service for back-end support. The framework illustrates adding custom tools to agents, like an AI painting service, and offers a detailed guide for creating a powerful agent that leverages these tools. Additionally, BrowserQwen, a Chrome extension which integrates these capabilities to assist with web page discussions, document understanding, and automated tasks, is introduced. Users must deploy a local database service and install this extension to utilize its features. The project is still evolving, warning that backward compatibility issues may arise and advising caution with the code interpreter due to the lack of a sandbox environment.",Harness the Power of Qwen-Agent for Advanced LLM Applications,"Qwen-Agent is a robust framework designed to empower developers in creating sophisticated LLM applications featuring instruction following, tool usage, planning, and memory. It supports the integration of example applications such as Browser Assistant, Code Interpreter, and Custom Assistant. By utilizing components like LLMs, prompts, and Agents, developers can easily customize and extend functionalities. The framework facilitates seamless interaction with Alibaba Cloudâ€™s DashScope or your own model services, enhancing the development of potent, tool-equipped agents. Moreover, Qwen-Agent showcases its flexibility and utility through the BrowserQwen Chrome extension for enriched browser interactions.","Explore the capabilities of Qwen-Agent for developing state-of-the-art LLM applications with features like instruction following, planning, and tool usage. Learn how to easily integrate with Alibaba Cloudâ€™s DashScope for enriched functionalities.",Collaborative AI Framework,"Python





        818





        82


        Built by

          









        10 stars today",https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/logo-qwen-agent.png; https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-writing.png; https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-editor-movie.png; https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-multi-web-qa.png; https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-ci.png; https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-web-qa.png; https://raw.githubusercontent.com/QwenLM/Qwen-Agent/main/assets/screenshot-pdf-qa.png,,818,2023-09-22T02:24:56Z
2024-03-06,https://github.com/PKU-YuanGroup/Open-Sora-Plan,https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/README.md,"The Open-Sora Plan, initiated by the Peking University Yuangroup and the AIGC lab, aims to recreate and build upon OpenAI's Sora project, emphasizing Video-VQVAE (VideoGPT) + DiT technologies for scalable purposes. Acknowledging limited resources, the project calls on the open-source community for contributions to complete its ambitious goals. The project is structured into primary stages, focusing on setting up the codebase and training models for improved resolution and duration, and extensions for conducting text2video experiments, enhancing video quality to 1080p, and adding more control conditions to the models. Recent updates include reorganization of codes, discussion openings, and training code availability. The project outlines a comprehensive repo structure, offers detailed installation instructions, and provides examples of dataset usage, training, and video reconstruction results. Contributors are warmly invited to aid in the project's development, which is significantly reliant on community participation and aimed at fostering advanced video generation research and applications.",Recreating Sora: A Collaborative Effort Towards Open-Sourced AI Solutions,"Join the innovative Open-Sora Plan spearheaded by the Peking University and Rabbit Exhibition AIGC Joint Lab as we aim to reconstruct Sora (OpenAI) using limited resources. This project focuses on reproducing Video-VQVAE and DiT technologies at scale and calls for the open-source community to contribute towards this goal. With a foundational infrastructure currently in place, we eagerly invite contributions to evolve this project further. Dive into this journey of continuous improvement and rapid iteration by supporting through pull requests.","Explore the Open-Sora Plan, an ambitious effort to recreate Sora (OpenAI) using Video-VQVAE and DiT at scale. Contribute to this open-source project and help advance AI technologies.",Collaborative AI Framework,"Python





        4,748





        405


        Built by

          









        2,133 stars today",https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/we_want_you.jpg; https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/framework.jpg; https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/loss.jpg,,4748,2024-02-20T14:01:03Z
2024-03-06,https://github.com/microsoft/Swin-Transformer,https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/README.md,"The Swin Transformer repository is the official implementation of the ""Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"" paper and its follow-ups. It introduces a novel architecture designed as a general-purpose backbone for computer vision tasks. The Swin Transformer achieves efficiency and effectiveness by computing representations through shifted windows, enhancing performance on benchmarks like COCO and ADE20K. It supports various tasks, including image classification, object detection, semantic segmentation, video action recognition, and self-supervised learning among others. Recent updates include integration with Nvidia's FasterTransformer for enhanced inference speed on specific GPUs, the release of models trained using feature distillation improving fine-tuning performance, and the integration of self-supervised pre-training approach SimMIM, showing its adaptability and progression towards more refined visual representation learning. Additionally, it offers pretrained models on ImageNet-1K and ImageNet-22K, demonstrating impressive results in both efficiency and accuracy across different scales of Swin Transformers.",Exploring the Swin Transformer: A Revolutionary Approach in Computer Vision,"Discover how the Swin Transformer is redefining computer vision with hierarchical vision transformers using shifted windows. Initially introduced for image classification, its capabilities now extend to object detection, instance segmentation, and beyond. The updates include significant speed improvements on GPUs, feature distillation enhancements, and new benchmarks in semantic segmentation. Dive into the Swin Transformer's journey from ICCV 2021 best paper award to setting new records in computer vision tasks.","Learn about the Swin Transformer, a groundbreaking technology in computer vision, offering advanced capabilities in image classification, object detection, and semantic segmentation. This blog post covers its latest updates, achievements, and application areas.",Computer Vision Platform,"Python





        12,580





        1,963


        Built by

          









        6 stars today",https://raw.githubusercontent.com/microsoft/Swin-Transformer/main/figures/teaser.png,,12580,2021-03-25T12:42:36Z
2024-03-06,https://github.com/PrometheusStealer/Prometheus,https://raw.githubusercontent.com/XPandora/PhysGaussian/main/README.md,"PhysGaussian introduces a method to integrate physics into 3D Gaussians for generating dynamic motion in visuals. Developed by researchers from the University of California, Los Angeles, Zhejiang University, and the University of Utah, this approach combines Newtonian dynamics with 3D Gaussian kernels, eliminating the need for traditional geometry embedding techniques. By employing a customized Material Point Method (MPM), PhysGaussian adds kinematic deformation and mechanical stress to these kernels, based on principles of continuum mechanics. This method allows seamless transition between physical simulation and visual rendering, adhering to the principle of ""what you see is what you simulate."" It demonstrates versatility in simulating a wide range of materials, such as elastic materials, plastic metals, and non-Newtonian fluids. The project, accepted by CVPR 2024, has released its code and offers a comprehensive guide for setup, simulation, and custom dynamics generation, including data preprocessing and configuration.",Exploring PhysGaussian: Innovative 3D Gaussians in Generative Dynamics,"Discover PhysGaussian, the cutting-edge method blending 3D Gaussians with physics for dynamic motion synthesis. By employing Material Point Methods, it introduces kinematic deformations and mechanical stresses to Gaussian kernels, aligning with continuum mechanics. This integration allows for high-quality, diverse material simulations without traditional geometric constraints, showcasing a wide range of applications from elastic to granular materials. The project opens new avenues in visual content creation, presenting a 'what you see is what you simulate' approach.","Delve into PhysGaussian, a novel technique integrating 3D Gaussians with physics for authentic motion synthesis, offering a seamless blend of physical dynamics and visual rendering for various materials.",Computer Vision Platform,"Python





        160





        121


        Built by

          






        27 stars today",https://raw.githubusercontent.com/XPandora/PhysGaussian/main/_resources/teaser-1.jpg,https://www.youtube.com/watch?v=V96GfcMUH2Q,704,
2024-03-06,https://github.com/XPandora/PhysGaussian,https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/README.md,"Open-Sora is an open-source initiative aiming to replicate the Sora architecture, enhancing it with dynamic resolution abilities, support for various model structures, video compression methods, and parallel training optimizations. It leverages Colossal-AI for efficiencies in AI large model systems and supports multimodal model structures like adaLN-zero, cross attention, and token concat for video processing. The project uses datasets like MSR-VTT, guiding users through data preprocessing for video description tasks. Moreover, Open-Sora facilitates both training and inference phases, allowing customizations for specific needs and enabling video generation from text inputs. Acknowledgements are made to contributions from OpenAI, VideoGPT, Diffusion Transformers, Deepspeed Ulysses, and OpenDiT, from which Open-Sora has drawn significant insights and methodologies.",Revolutionizing Video AI with Open-Sora: A Comprehensive Guide,"Open-Sora, an open-source initiative powered by Colossal-AI, introduces a cutting-edge framework for creating high-performance video AI models, offering a replica of Sora's development pipeline. It uniquely supports dynamic resolution for training videos of any size, incorporates diverse model structures, and facilitates multiple video compression techniques. Moreover, Open-Sora enhances training through parallel optimizations and supports extensive datasets, including MSR-VTT and customized options. This project is a leap forward in efficient and versatile video AI model training, significantly reducing costs and expanding capabilities.","Discover Open-Sora, the open-source project revolutionizing video AI with a complete Sora reproduction solution, dynamic resolution support, and advanced parallel training optimizations.",Deep Learning Platform,"Python





        704





        18


        Built by

          






        19 stars today",https://img.shields.io/badge/å¾®ä¿¡-åŠ å…¥-green?logo=wechat&amp)](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-1.png; https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-2.png,,826,2023-11-13T06:49:28Z
2024-03-06,https://github.com/mini-sora/minisora,https://raw.githubusercontent.com/anthropics/anthropic-sdk-python/main/README.md,"The Anthropic Python API library provides easy access to the Anthropic REST API for Python 3.7+ applications, supporting both synchronous and asynchronous operations using httpx. It offers type definitions for parameters and response fields, enabling clear and efficient code development. The library can be installed via pip. The documentation and full API details are available online, including examples for standard and asynchronous usage, streaming responses, and utilizing the Anthropic Bedrock API. It includes functionalities like token counting, error handling, retries, and timeout settings. The library is designed with best practices in mind, including logging, managing HTTP resources, and versioning, ensuring compatibility and ease of use. Users are encouraged to manage sensitive information securely and to contribute feedback for improvements. Python 3.7 or higher is required to use this library.",Easily Interact with Anthropic API via Python Library: A Beginner's Guide,"Exploring the Anthropic Python library unlocks seamless access to the Anthropic REST API for Python 3.7+ applications. It provides convenient synchronous and asynchronous clients, thorough documentation, and supports streaming responses, all with comprehensive type definitions. Enhance your Python projects with advanced API interaction capabilities, simplified error handling, and efficient token counting. Whether you're managing async tasks or counting tokens, this library is equipped to streamline your development process.","Discover how the Anthropic Python library facilitates easy access to the Anthropic REST API, offering synchronous and asynchronous clients, streaming responses, and simplified error handling for Python 3.7+ applications.",AI Python Client,"Python





        683





        117


        Built by

          









        232 stars today",,,689,2024-02-21T13:50:34Z
2024-03-06,https://github.com/anthropics/anthropic-sdk-python,https://raw.githubusercontent.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/main/README.md,"The repository for ""Machine Learning From Scratch"" by AssemblyAI offers code from their YouTube course, featuring implementations of popular machine learning algorithms. This educational resource is designed for learners to understand and apply machine learning concepts through practical coding exercises. It's presented on AssemblyAI's YouTube channel, making it easily accessible for anyone interested in diving into the field of machine learning. The repository also acknowledges inspiration from a similar project by Python Engineer (Patrick Loeber), suggesting a community-based approach to learning and sharing knowledge in the domain of machine learning.",Learn Machine Learning with AssemblyAI's Comprehensive Course,"Dive into machine learning by exploring AssemblyAI's 'Machine Learning From Scratch' course available on YouTube. This valuable resource provides hands-on code implementations of popular ML algorithms, drawing inspiration from the notable work by Python Engineer, Patrick Loeber. Perfect for both beginners and seasoned programmers, it offers a deep dive into practical machine learning applications. Discover the ease of mastering ML algorithms through this detailed course. Start your journey into machine learning today by joining thousands of learners online.","Unlock the secrets of machine learning with AssemblyAI's 'Machine Learning From Scratch' course. Featuring hands-on code implementations and inspired by Python Engineer's work, this course is your gateway to mastering ML algorithms.",Machine Learning,"Python





        689





        79


        Built by

          









        25 stars today",,https://www.youtube.com/watch?v=p1hGz0w_OCo,519,2023-01-17T20:57:10Z
2024-03-06,https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch,https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/README.md,"The text introduces TikTokDownloader, a fully open-source, free tool designed for various data collection purposes from TikTok and Douyin including video downloads, live videos, comment data, account details, and more, using the Requests module. It supports batch downloads, detailed data collection, and has features like auto-skip for already downloaded files and persistence in data collection. The software is available for Windows 10 and above users through an executable file on the project's GitHub releases page. The creator emphasizes that it's solely released on GitHub without any partnership or payment plans, warning users to beware of scams. It lists comprehensive functionalities such as downloading watermark-free videos/photosets, collecting detailed data, multi-account support, and more, alongside various modes of interaction like terminal, Web UI, and Web API. It also provides a quick start guide, information on handling cookies for accessing different features, and advises on updates, alongside a disclaimer emphasizing the user's responsibility in legal matters and absolving the authors of any liabilities. Lastly, it encourages support for the project through stars on GitHub and financial donations, providing social and contact information for further assistance or contribution.",Ultimate Guide to TikTokDownloader: Free Tool for Downloading and Collecting TikTok Data,"Discover TikTokDownloader, the go-to open-source tool for effortlessly downloading and collecting a wide range of data from TikTok and Douyin. Whether it's video, live streams, or detailed analytics you're after, TikTokDownloader has got you covered. This comprehensive guide provides everything you need to get started, including setup instructions and key features. With full Windows compatibility and no cost, start maximizing your TikTok content today without any hassle. Remember, this tool is solely available on GitHub and aims to enhance your TikTok experience securely.","Explore the capabilities of TikTokDownloader, a fully open-source tool designed for downloading and collecting diverse TikTok data, including videos, live streams, and analytics. Get started with our comprehensive guide.",Open Source Tool,"Python





        519





        178


        Built by

          






        21 stars today",https://github.com/JoeanAmier/TikTokDownloader/blob/master/static/images/TikTokDownloader.png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/docs/ç»ˆç«¯äº¤äº’æ¨¡å¼æˆªå›¾1.png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/docs/ç»ˆç«¯äº¤äº’æ¨¡å¼æˆªå›¾2.png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/docs/WebUIæ¨¡å¼æˆªå›¾1.png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/docs/WebUIæ¨¡å¼æˆªå›¾2.png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/docs/WebUIæ¨¡å¼æˆªå›¾3.png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/docs/WebAPIæ¨¡å¼æˆªå›¾.png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/docs/ç¨‹åºè¿è¡Œæ¼”ç¤º.png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/./docs/å¾®ä¿¡èµžåŠ©äºŒç»´ç .png; https://raw.githubusercontent.com/JoeanAmier/TikTokDownloader/master/./docs/æ”¯ä»˜å®èµžåŠ©äºŒç»´ç .png,,4502,2022-09-09T14:37:46Z
2024-03-06,https://github.com/JoeanAmier/TikTokDownloader,https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/README.md,"The text introduces Qwen-VL-Plus and Qwen-VL-Max, upgraded models in the Qwen-VL family, supporting free access through various platforms and APIs. Qwen-VL-Plus enhances image recognition and text abilities, supporting high-definition images and extreme aspect ratios. Qwen-VL-Max improves visual reasoning and cognitive understanding, delivering optimal performance on complex tasks. Both models excel in image reasoning, detail recognition, and support for high-resolution images. They surpass other open-source models and compete with Gemini Ultra and GPT-4V in multimodal tasks, especially in Chinese question answering and text comprehension. The document includes a comparison table showcasing their superior performance across multiple benchmarks. It also mentions recent achievements, updates on Qwen-VL-Chat, and the introduction of efficient quantized models for reduced memory usage and faster inference. The evaluation section details their capabilities across standard benchmarks, TouchStone, and other multimodal benchmarks, proving their state-of-the-art performance. The document guides on requirements, quick start instructions, quantization, finetuning, demo setup, licenses, and citation for further reference.",Unveiling Qwen-VL: The Pinnacle of Multimodal AI Innovation,"Discover the groundbreaking advancements in multimodal AI with Qwen-VL and Qwen-VL-Max, the latest in the Qwen-VL series, setting new benchmarks in visual and linguistic tasks. These models excel in recognizing and understanding high-resolution images, delivering unparalleled performance across a wide array of complex challenges. With their state-of-the-art capabilities, Qwen-VL and Qwen-VL-Max not only surpass previous models but also rival the achievements of leading technologies like Gemini Ultra and GPT-4V in multimodal tasks.","Explore the latest breakthroughs in AI with Qwen-VL & Qwen-VL-Max, leading the charge in multimodal advancements. Discover their unmatched visual recognition & linguistic understanding abilities.",Collaborative AI Framework,"Python





        4,502





        697


        Built by

          





        45 stars today",https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/logo.jpg; https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo_vl.gif; https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/radar.png; https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/eval_mm/mme/perception.jpg; https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/eval_mm/mme/cognition.jpg; https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/eval_mm/seed_bench/leaderboard.jpg; https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo_highfive.jpg; https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo_spotting_caption.jpg; https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo_highfive.jpg,,3077,2022-10-10T14:27:36Z
2024-03-06,https://github.com/QwenLM/Qwen-VL,https://raw.githubusercontent.com/budtmo/docker-android/master/README.md,"Docker-Android is a docker image designed for Android application development and testing, including native, web, and hybrid apps. It offers various advantages such as emulators with different device profiles (e.g., Samsung Galaxy S6, Nexus 4), VNC support for visual access inside the container, web-UI for log access, external emulator control via adb, cloud solution integration like Genymotion Cloud, and the ability to build projects and run different tests (Appium, Espresso). It includes images for Android versions 9.0 through 14.0 and devices ranging from phones like the Samsung Galaxy S10 to tablets like the Nexus 7. Requirements include Docker installation, virtualization support, and specific instructions for Windows and Ubuntu users. Docker-Android supports various use-cases including Android project building, UI testing with Appium, and cloud deployment. It allows custom configurations and is integrated with Genymotion for those needing resources like simulator maintenance. The project also has a ""Pro"" version offering additional features like proxy setup, language setting, headless mode, and more for sponsors.",Maximizing Android Development Efficiency with Docker-Android,"Discover the power of Docker-Android for all your Android development needs. From application development to testing, Docker-Android offers numerous advantages including emulator support for various device profiles, VNC support, and integration with cloud solutions like Genymotion Cloud. Enhance your development efficiency by leveraging Docker-Android's capabilities to build and test projects across different frameworks.","Learn how to streamline your Android app development and testing with Docker-Android. Emulator support, VNC, log sharing, and cloud integration - all in one solution.",Software Development,"Python





        3,077





        225


        Built by

          









        22 stars today",https://raw.githubusercontent.com/budtmo/docker-android/master/./images/logo_docker-android.png; https://raw.githubusercontent.com/budtmo/docker-android/master/./images/logo_genymotion_and_dockerandroid.png; https://raw.githubusercontent.com/budtmo/docker-android/master/./images/docker-android_users.png,,6941,2023-08-21T07:57:15Z
2024-03-06,https://github.com/triwinds/ns-emu-tools,https://raw.githubusercontent.com/mlc-ai/mlc-llm/main/README.md,"The Machine Learning Compilation for Large Language Models (MLC LLM) project focuses on facilitating the deployment of AI models, particularly large language models, directly onto a variety of devices through machine learning compilation techniques. It achieves high performance by leveraging compiler acceleration and supports a wide array of hardware platforms, including AMD, NVIDIA, and Apple GPUs, across different operating systems like Linux, Windows, macOS, and on web browsers via WebGPU and WASM. MLC LLM is scalable, demonstrated by its performance on various GPUs and its compatibility with different model architectures. Additionally, it offers prebuilt models and the ability to compile others not listed. For developers, MLC LLM provides a range of APIs for deployment across platforms and programming languages. Recent updates include scalable multi-GPU support, prebuilt packages for ROCm and CUDA, and the addition of various model supports. The project embraces open collaboration and encourages citations for academic and research use, highlighting its commitment to advancing the field of machine learning and AI model deployment.",Unlocking AI's Potential with MLC LLM's Universal Deployment Solution,"Discover how the Machine Learning Compilation for Large Language Models (MLC LLM) project is revolutionizing universal AI deployment. With compiler acceleration, MLC LLM enables the native deployment of large language models across various devices and platforms. It offers scalable solutions on diverse GPUs and supports a wide array of models and APIs for seamless integration. Dive into the future of accessible, high-performance AI with MLC LLM.","Explore MLC LLM's innovative approach to deploying large language models natively across devices and platforms. Learn about its scalable solutions, extensive model support, and universal APIs for seamless AI integration.",Deep Learning Platform.,"Python





        3,739





        131


        Built by

          








        22 stars today",,,16074,2022-10-22T13:46:50Z
2024-03-06,https://github.com/budtmo/docker-android,https://raw.githubusercontent.com/evalplus/evalplus/master/README.md,"EvalPlus is a comprehensive evaluation framework designed to rigorously assess Large Language Models (LLMs) for code generation (LLM4Code). Featuring significantly expanded versions of the HumanEval and MBPP benchmarksâ€”HumanEval+ and MBPP+â€”it offers 80x and 35x more tests, respectively. EvalPlus aims to provide more reliable LLM rankings through its extensive datasets, helping to highlight the robustness of code generated by various models. It also offers pre-generated LLM code samples for different models, facilitating research in LLM4Code without the need for costly benchmark reruns. To ensure thorough evaluation, EvalPlus supports running evaluations in a sandbox environment, such as Docker, and offers tools for code sanity checks and post-sanitation, improving the usability of LLM-generated code. The framework is accessible for installation via pip or directly from its GitHub repository, and detailed instructions are provided for setting up environments, generating, and evaluating code samples. Acknowledging the limitations of evaluating code over a minimal number of test cases, EvalPlus emphasizes the importance of its expanded datasets for a comprehensive assessment. The initiative's findings and methodology are detailed in a paper presented at NeurIPS'23, with supplementary materials available online, including pre-evaluated LLM code samples and tools to facilitate research and evaluation in the LLM4Code domain.",Unleashing the Power of EvalPlus: Revolutionizing LLM Code Evaluation,"Discover how EvalPlus is setting new benchmarks in the rigorous evaluation of language models for code generation (LLM4Code). With innovative datasets like HumanEval+ and MBPP+, EvalPlus provides an enhanced evaluation framework, ensuring more reliable LLM rankings and coding rigor. Dive into the significant benefits of using EvalPlus, from rigorous testing to pre-generated LLM samples, and why it's vital for advancing LLM4Code research.","Explore EvalPlus, a groundbreaking evaluation framework for LLM4Code, featuring HumanEval+ and MBPP+ for comprehensive testing. Learn how it ensures reliable LLM rankings and enhances coding rigor in our latest blog post.",Deep Learning Platform,"Python





        6,940





        1,047


        Built by

          









        28 stars today",https://raw.githubusercontent.com/evalplus/evalplus/master/./gallary/render.gif,,712,2016-12-22T13:02:48Z
2024-03-06,https://github.com/mlc-ai/mlc-llm,https://raw.githubusercontent.com/rohankishore/Youtility/main/README.md,"Youtility is a user-friendly YouTube video and playlist downloader built with PyQt6 and PyTube, free from intrusive ads. Unlike typical online download services that bombard users with misleading advertisements, Youtility offers a clean, straightforward approach to downloading YouTube content. It allows users to download single videos along with captions, entire playlists (with an audio-only option), and convert videos to Mp3 format. To get started, users can either download the executable from the Youtility GitHub releases page or run the program manually using Python. The project is open-source, with special thanks attributed to PyTube and zhiyiYo's PyQt-Fluent-Widgets for their contributions. The creator encourages support through Ko-Fi donations or GitHub Sponsorships.",Youtility: The Ultimate Ad-Free YouTube Downloader,"Discover Youtility, the ultimate solution for downloading YouTube videos and playlists without the hassle of ads or bloatware, made possible with PyQt6 and PyTube. This open-source downloader allows for effortless saving of single videos with captions, entire playlists, and even converting videos to MP3 format. Say goodbye to intrusive ads and hello to a smooth, user-friendly downloading experience with Youtility. Whether you're looking to download for offline viewing or simply want to save your favorite content ad-free, Youtility has got you covered.","Experience hassle-free YouTube downloads with Youtility, an ad-free, open-source video and playlist downloader. Get started with easy downloads and say goodbye to ads.",Open Source Tool,"Python





        16,074





        1,202


        Built by

          









        94 stars today",,,111,2023-04-29T01:59:25Z
2024-03-06,https://github.com/evalplus/evalplus,https://raw.githubusercontent.com/psf/black/main/README.md,"Black is described as an uncompromising Python code formatter, focusing on speed, determinism, and eliminating manual formatting concerns. It standardizes formatting across different projects, making code review more efficient by minimizing diffs. It is PEP 8 compliant and offers limited style configuration, prioritizing consistency and simplicity. Black supports Python 3.8+ and can also format Jupyter Notebooks with a specific installation. Users can start using Black with simple commands and further customize formatting with a `pyproject.toml` file. It's widely adopted by major open-source projects and companies. Testimonials highlight its impact on productivity and code quality. Black encourages contributions and adheres to a code of conduct aligning with the Python Communityâ€™s standards, with a touch of humor inspired by Monty Python's Flying Circus. The documentation, changelog, and a list of contributors are available online, emphasizing community involvement and the evolution of the tool.",Mastering Python Code Formatting with Black: The Uncompromising Formatter,"Discover how Black, the uncompromising Python code formatter, can revolutionize your coding workflow. By prioritizing speed, determinism, and freedom from formatting errors, Black ensures your code is consistently styled, making code review a breeze. Adopting Black means focusing more on content and less on styling, thus saving valuable time. Experience the ease of code formatting with Black and join the community of developers who trust in its reliability. Ready to streamline your Python projects? Explore Black today.","Learn how Black, the Python code formatter, simplifies and standardizes code styling, saving developers time and enhancing code review processes. Make your Python projects more efficient with Black.",Open Source Tool,"Python





        712





        63


        Built by

          









        4 stars today",https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png,https://www.youtube.com/watch?v=esZLCuWs_2Y,36992,2023-04-15T04:20:10Z
2024-03-06,https://github.com/rohankishore/Youtility,https://raw.githubusercontent.com/dagster-io/dagster/master/README.md,"As you've requested a summary without providing the specific text from the ""python_modules/dagster/README.md"", I will craft a general overview based on what such a file would typically contain. The README file for the Dagster module in Python is likely an introductory guide that explains what Dagster is, its purpose, and how to use it. Dagster is an open-source data orchestration framework for machine learning, analytics, and ETL pipelines. This file probably includes instructions on how to install Dagster, basic examples of creating and running pipelines, and links to further documentation. It might also cover contributions guidelines, license information, and how to get support or connect with the community. Depending on the specific version and updates of the Dagster module, detailed updates, and features of the current release could also be highlighted.",Ultimate Guide to Dagster Python Modules for Efficient Data Workflows,"Dive into the world of Dagster, the open-source library designed for building scalable data pipelines in Python. This quick guide covers the basics of installing and utilizing Dagster modules to enhance your data processing tasks. Discover how Dagster simplifies complex workflows, ensures data integrity, and accelerates development with its intuitive framework. Perfect for data engineers and scientists, learn how to leverage Dagster for your next project. Upgrade your data orchestration today with our insights on Python's Dagster modules.",Explore the capabilities of Dagster Python modules in our comprehensive guide. Learn to streamline your data workflows for better efficiency and reliability with our expert insights.,Data Ingestion Tool,"Python





        111





        7


        Built by

          





        12 stars today",,,9791,2024-03-02T19:02:26Z
2024-03-07,https://github.com/VAST-AI-Research/TripoSR,https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/README.md,"TripoSR is a collaborative effort between Tripo AI and Stability AI, offering an advanced, open-source model for rapid 3D reconstruction from single images. Integrating principles from the Large Reconstruction Model, TripoSR highlights significant enhancements in speed and quality, delivering high-quality 3D models in under 0.5 seconds using NVIDIA A100 GPUs. It outperforms other open-source options in both qualitative and quantitative benchmarks across various datasets. The tool, which operates under the MIT license, comes with pre-trained models, source code, and an interactive demo, aiming to aid those in research, development, and creative fields. Installation involves Python 3.8 or greater, CUDA for GPU support, PyTorch, and other dependencies. Users can perform manual inferences or utilize a local Gradio app for interactive demonstrations. Troubleshooting tips address potential issues with CUDA compatibility and the torchmcubes library.",Exploring TripoSR: Revolutionizing 3D Reconstruction from Single Images,"TripoSR presents a groundbreaking collaboration between Tripo AI and Stability AI, setting a new standard in fast feedforward 3D reconstruction from single images. Utilizing the Large Reconstruction Model (LRM), it significantly enhances the speed and quality of 3D model generation, achieving high-quality outputs in under 0.5 seconds with an NVIDIA A100 GPU. TripoSR not only surpasses other open-source models in both qualitative and quantitative measures but also facilitates an engaging user experience with its interactive online demo and comprehensive technical report for detailed insights. This model, open-source under the MIT license, aims to empower the 3D generative AI community and content creators with cutting-edge tools and resources.","Discover TripoSR by Tripo AI and Stability AI, a cutting-edge open-source model for ultra-fast and high-quality 3D reconstruction from single images. Learn about its unique capabilities, performance benchmarks, and how it's pushing the envelope in 3D generative AI.",Image Generation Platform,"Python





        1,493





        170


        Built by

          









        175 stars today",https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/teaser800.gif; https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/comparison800.gif; https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/visual_comparisons.jpg; https://raw.githubusercontent.com/VAST-AI-Research/TripoSR/main/figures/scatter-comparison.png,,1493,2024-02-07T10:31:04Z
2024-03-07,https://github.com/iam-veeramalla/aws-devops-zero-to-hero,https://raw.githubusercontent.com/iam-veeramalla/aws-devops-zero-to-hero/main/README.md,"The ""AWS DevOps Zero to Hero"" playlist is designed to transform DevOps engineers into AWS experts in 30 days through a comprehensive program that includes practical projects, presentations, interview prep, and real-time examples. Starting with an introduction to AWS, the program covers a wide range of topics including IAM, EC2 instances, AWS networking with VPC, AWS security, and AWS Route 53. Participants will engage in hands-on projects, such as deploying web applications on EC2, configuring domains with Route 53, and setting up a secure VPC. Advanced topics include S3, AWS CLI, CloudFormation, CodeCommit, CodePipeline, CodeBuild, CodeDeploy, CloudWatch, Lambda, CloudFront, ECR, ECS, EKS, Systems Manager, Secrets Manager, and the use of Terraform for infrastructure creation. Key areas like AWS CloudTrail, Config, Elastic Load Balancer, cloud migration strategies, and AWS best practices are also covered. The program aims to equip individuals with the skills needed to manage AWS resources effectively, preparing them for AWS-related job roles with a project involving RDS on the final day.",Become an AWS DevOps Expert: Master AWS in 30 Days | Complete Guide,"Embark on a comprehensive journey to mastering AWS with our 30-day DevOps guide. From setting up your AWS account to deploying your first AWS project, this guide covers essential services like EC2, IAM, VPC, and more. With hands-on projects and real-time examples, you'll gain practical skills in AWS for DevOps applications. Discover advanced features, security best practices, and prepare for job interviews with our expert tips. Transform from zero to hero in AWS DevOps and propel your career forward.","Master AWS in 30 days with our DevOps guide. Learn EC2, IAM, VPC, and more through practical projects and real-time examples. Become an AWS DevOps expert and ace your job interviews with our comprehensive guide.",Software Development,"Python





        4,445





        5,401


        Built by

          






        16 stars today",,,4445,2023-06-19T06:13:08Z
2024-03-07,https://github.com/alibaba-damo-academy/FunASR,https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/README.md,"FunASR is a toolkit designed to bridge the gap between academic research and industrial applications in the field of speech recognition. It facilitates the training and fine-tuning of industrial-grade speech recognition models, allowing researchers and developers to more easily conduct research and production in this area. This toolkit supports a range of features including ASR, Voice Activity Detection, Punctuation Restoration, Language Models, Speaker Verification, Diarization, and multi-talker ASR. It provides convenient scripts and tutorials for model inference and fine-tuning. FunASR has released numerous pre-trained models for academic and industrial use, which are accessible through the ModelScope and Huggingface platforms. Recently, it has added large-scale audio-text multimodal models and the Whisper-large-v3 model for multilingual speech recognition and translation. Furthermore, FunASR offers services for offline file transcription in multiple languages and real-time transcription, with support for ARM64 platform docker images. It encourages community engagement and offers guidance through documentation and community groups for users encountering issues.",Explore the Future of Speech Recognition with FunASR Toolkit,"Discover FunASR, the cutting-edge end-to-end speech recognition toolkit designed to bridge the gap between academic research and industrial application. This comprehensive toolkit supports a variety of features like ASR, Voice Activity Detection, and more, offering easy scripts for both inference and model fine-tuning. With a vast collection of pre-trained models available through Model Zoo, and new additions like multilingual speech recognition models, FunASR is set to revolutionize how we interact with technology using natural language.","Unveil the potential of FunASR Toolkit for transformative speech recognition applications, offering features like ASR, VAD, and access to a broad range of pre-trained models. Ideal for researchers and developers looking to advance the field.",Natural Language Processing,"Python





        2,654





        322


        Built by

          









        22 stars today",https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/funasr_logo.jpg; https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/dingding.jpg; https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/wechat.png; https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/alibaba.png; https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/nwpu.png; https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/China_Telecom.png; https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/RapidAI.png; https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/aihealthx.png; https://raw.githubusercontent.com/alibaba-damo-academy/FunASR/main/docs/images/XVERSE.png,,2654,2022-11-24T02:28:11Z
2024-03-07,https://github.com/openai/whisper,https://raw.githubusercontent.com/openai/whisper/main/README.md,"Whisper is an advanced speech recognition model developed by OpenAI, adept at handling multilingual recognition, translation, and identifying languages. This model, operating with a sequence-to-sequence framework built on Transformer architecture, is capable of multitasking and replaces traditional multi-stage speech processing pipelines, favoring a unified approach where tasks like speech transcription, translation, and language identification are managed through token sequence predictions. It is compatible with recent Python and PyTorch versions, requiring specific Python packages and the ffmpeg command-line tool for full functionality. Five model sizes are available, varying in parameters, memory requirements, and speed to accommodate different application needs. Performance differs across languages, with specialized English-only models offering improved accuracy in certain situations. Whisper can be easily accessed via command-line or through Python code, providing flexibility for developers. The toolkit and its models are open-source, licensed under the MIT License, promoting wide accessibility and community-driven enhancements.",Unlocking the Power of Whisper: Revolutionizing Speech Recognition,"Discover Whisper, OpenAI's groundbreaking speech recognition model, in our latest blog post. Learn how it's trained on diverse datasets to perform tasks like multilingual speech recognition, speech translation, and language identification. Explore its setup, model sizes, command-line usage, and Python integration for a comprehensive understanding. Whether you're handling English-only projects or need multilingual capabilities, Whisper offers various models to meet your needs. Dive into our discussion on its features, setup guidelines, and real-world applications today.","Explore OpenAI's Whisper: a versatile speech recognition model designed for multilingual speech processing, translation, and identification. Learn about its features, how to set it up, and leverage it for your projects.",Natural Language Processing,"Python





        57,207





        6,483


        Built by

          









        81 stars today",https://raw.githubusercontent.com/openai/whisper/main/approach.png,,57207,2022-09-16T20:02:54Z
2024-03-07,https://github.com/KimMeen/Time-LLM,https://raw.githubusercontent.com/KimMeen/Time-LLM/main/README.md,"Time-LLM is introduced as a novel framework for time series forecasting, as presented at ICLR'24. This approach leverages the capabilities of large language models (LLMs) by reprogramming them to understand and process time series data as though it were natural language. The core methodology involves transforming the input time series into textual prototype representations aligned with the LLM's processing abilities and enhancing these inputs with declarative prompts that encapsulate domain expertise and specific task directives. This enables the LLM to apply its pre-existing knowledge and reasoning capabilities to time series forecasting tasks effectively. The associated research has garnered attention, encouraging contributions and further research, as indicated by the provided links to datasets, demonstration scripts, and further literature exploring the intersection of LLMs with time series and spatio-temporal data analysis.",Revolutionizing Time Series Forecasting with Time-LLM: Unveiling the Power of LLM Reprogramming,"Discover how Time-LLM leverages the prowess of large language models (LLMs) to transform the realm of time series forecasting. This groundbreaking framework reprograms LLMs to interpret time series as a language task, fusing text prototype representations with expert knowledge for unprecedented forecasting accuracy. Its innovative approach maintains the original LLM structure, promising a new horizon in predictive analytics. Ideal for researchers and practitioners alike, Time-LLM opens up new pathways for robust time series analysis and forecasting.","Explore Time-LLM, a novel framework reprogramming LLMs for enhancing time series forecasting. Learn how it merges text prototypes with expert knowledge, maintaining LLM integrity for superior predictions.",Deep Learning Platform,"Python





        322





        45


        Built by

          









        17 stars today",https://raw.githubusercontent.com/KimMeen/Time-LLM/main/./figures/logo.png; https://raw.githubusercontent.com/KimMeen/Time-LLM/main/./figures/framework.png; https://raw.githubusercontent.com/KimMeen/Time-LLM/main/./figures/method-detailed-illustration.png,https://www.youtube.com/watch?v=L-hRexVa32k,322,2024-01-20T01:26:30Z
2024-03-07,https://github.com/traceloop/openllmetry,https://raw.githubusercontent.com/traceloop/openllmetry/main/README.md,"OpenLLMetry is an open-source project that enhances the observability of LLM (Large Language Model) applications through a set of extensions based on OpenTelemetry. Developed and maintained by Traceloop under the Apache 2.0 license, it allows seamless integration with existing observability tools like Datadog, Honeycomb, and others. The project includes standard OpenTelemetry instrumentations for LLM providers and Vector DBs, alongside a Traceloop SDK that simplifies the initial setup while ensuring compatibility with standard OpenTelemetry data formats. The documentation offers a straightforward guide for getting started, including SDK installation and usage. OpenLLMetry supports multiple destinations and instruments various services, from LLM providers like OpenAI and Anthropic to Vector DBs such as Chroma and Pinecone, along with frameworks like LangChain and Haystack. The project encourages community contributions and offers support through Slack, GitHub Discussions, and other channels, acknowledging the vital input from the community, including the naming suggestion by Patrick Debois.",Maximize Your LLM Application's Performance with OpenLLMetry: An Observability Toolkit,"OpenLLMetry offers an advanced set of extensions for monitoring and optimizing your LLM applications by integrating seamlessly with OpenTelemetry. This open-source toolkit, maintained by Traceloop under the Apache 2.0 license, is designed to enhance your existing observability stack, including Datadog and Honeycomb, among others. With easy SDK installation and support for a wide range of LLM providers and Vector DBs, OpenLLMetry ensures comprehensive observability. Whether you're looking to instrument OpenAI, Anthropic, or any major Vector DB, OpenLLMetry has got you covered. Get started today and elevate your LLM application's observability with OpenLLMetry.","Discover how OpenLLMetry can revolutionize observability for your LLM application with advanced monitoring, seamless OpenTelemetry integration, and support for major LLM providers. Maximize performance today.",Collaborative AI Framework,"Python





        891





        60


        Built by

          









        23 stars today",https://raw.githubusercontent.com/traceloop/openllmetry/main/img/logo-light.png; https://raw.githubusercontent.com/traceloop/openllmetry/main/img/logo-dark.png,,891,2023-09-02T14:42:59Z
2024-03-07,https://github.com/bclavie/RAGatouille,https://raw.githubusercontent.com/bclavie/RAGatouille/main/README.md,"RAGatouille is a tool designed for integrating state-of-the-art retrieval methods easily into any RAG pipeline, enhancing ease of use and modularity, and is supported by research. Its main goal is to simplify the integration of advanced retrieval models, like ColBERT, into RAG pipelines, bridging the gap between cutting-edge research and practical application. Despite the effectiveness of dense embeddings for retrieval tasks, recent research suggests that models like ColBERT perform better across various domains and languages, especially with limited data. However, such models are less known and harder to use. RAGatouille aims to make these advanced methods accessible and straightforward to implement, providing a solution that simplifies their use without requiring in-depth knowledge of the underlying literature. The tool includes features for training and fine-tuning ColBERT models, embedding and indexing documents for retrieval, and document retrieval, all designed to be highly customizable yet easy to deploy. RAGatouille supports Python 3.9 to 3.11, is available for download, and provides detailed documentation for users. It emphasizes modularity, allowing components to be used independently or in combination according to user needs.",Revolutionizing RAG Pipelines with RAGatouille: The Ultimate Guide,"Discover RAGatouille, a groundbreaking tool designed to seamlessly blend advanced retrieval methods into any RAG pipeline, ensuring modularity and ease-of-use. Dive into a world where the latest research meets practical application, optimizing every aspect of your RAG pipeline with state-of-the-art models. RAGatouille not only simplifies the use of complex models like ColBERT but also paves the way for enhancing performance across diverse domains. Don't miss the opportunity to elevate your RAG pipeline with RAGatouille - start by running 'pip install ragatouille'. Explore the potential of RAGatouille on GitHub and become a part of the future of retrieval methods today.","Explore RAGatouille, a cutting-edge framework for integrating state-of-the-art retrieval methods into RAG pipelines effortlessly. Enhance your projects with ease-of-use, backed by the latest research. Get started with 'pip install ragatouille'.",Open Source Tool,"Python





        1,747





        119


        Built by

          









        11 stars today",https://raw.githubusercontent.com/bclavie/RAGatouille/main/RAGatouille.png,,1747,2023-12-29T16:26:42Z
2024-03-07,https://github.com/facebookresearch/jepa,https://raw.githubusercontent.com/flowtyone/ComfyUI-Flowty-TripoSR/master/README.md,"ComfyUI-Flowty-TripoSR is a custom node integration enabling the use of TripoSR, a collaborative project by Tripo AI and Stability AI, within ComfyUI. TripoSR is an advanced, open-source model capable of fast 3D reconstruction from single images. This node is designed for experimental purposes, and contributions via PRs for enhancements are welcomed. Installation involves cloning the repo into `custom_nodes` of ComfyUI, installing dependencies, and placing the downloaded TripoSR model in the specified directory. The project acknowledges MrForExample for his contributions through the ComfyUI-3D-Pack, which facilitated 3D model display functionalities in ComfyUI. This initiative is part of a community project by flowt.ai, encouraging users to explore and contribute to its development.",Integrate 3D Reconstruction in ComfyUI with Flowty-TripoSR Node,"Discover how to elevate your ComfyUI experience by integrating the Flowty-TripoSR node, enabling state-of-the-art 3D reconstruction from single images. Thanks to the collaboration between Tripo AI and Stability AI, this open-source model, TripoSR, can now be seamlessly utilized within ComfyUI. Installation is straightforward: simply install ComfyUI, clone the Flowty-TripoSR repository into the custom_nodes directory, and you're a few steps away from transforming images into 3D models. Don't forget to download the TripoSR model and place it in the appropriate directory. This innovation not only advances ComfyUI's capabilities but also highlights the powerful collaboration within the open-source community.",Learn how to integrate TripoSR with ComfyUI using the new Flowty-TripoSR node for quick 3D model reconstruction from imagesâ€”enhance your projects with cutting-edge tech.,Custom Node Pack,"Python





        1,906





        163


        Built by

          









        165 stars today",https://raw.githubusercontent.com/flowtyone/ComfyUI-Flowty-TripoSR/master/workflow-sample.png; https://raw.githubusercontent.com/flowtyone/ComfyUI-Flowty-TripoSR/master/flowt.png,,150,2024-02-12T15:34:31Z
2024-03-07,https://github.com/flowtyone/ComfyUI-Flowty-TripoSR,https://raw.githubusercontent.com/tobymao/sqlglot/main/README.md,"SQLGlot is a versatile SQL tool that serves as a parser, transpiler, optimizer, and engine without any dependencies. It supports 21 different SQL dialects, including popular ones like DuckDB, Presto/Trino, Spark/Databricks, Snowflake, and BigQuery. SQLGlot can format SQL, translate between dialects, and customize parsing. It highlights syntax errors and dialect incompatibilities, although validation is not its primary goal. Contributions to SQLGlot are encouraged. It offers API documentation and a primer on expression trees. Installation is straightforward via PyPI or a local setup, with optional development requirements. SQLGlot uses semantic versioning. The community can engage through a Slack channel. Examples in the documentation illustrate various functionalities, including formatting, transpiling, parsing errors, unsupported errors, building and modifying SQL, SQL optimization, AST introspection, and custom dialects. SQLGlot can even execute SQL on Python dictionaries, useful for unit testing. It is used by several projects and can be further explored through its documentation and testing procedures. Benchmarks indicate its performance relative to other parsers, and while SQLGlot's main engine isn't aimed at speed, it proves useful across different tasks. Optional dependencies like dateutil extend functionality, particularly in expression simplification.",Unlock SQL Dialect Translation with SQLGlot: A Comprehensive Guide,"Discover SQLGlot, the versatile SQL parser, transpiler, optimizer, and engine with no dependencies. Effortlessly format SQL and translate between 21 different dialects like DuckDB, Presto/Trino, Spark/Databricks, Snowflake, and BigQuery. Dive into a world where customizing the parser, analyzing queries, and building SQL programmatically is made simple. With its robust performance and comprehensive test suite, SQLGlot stands out in the realm of SQL parsers. Learn more and contribute to its thriving community.","Explore SQLGlot, a no-dependency SQL parser and transpiler that supports 21 dialects. Learn how to effortlessly parse, optimize, and translate SQL, customize parsing, and contribute to its development. Perfect for developers and data engineers.",SQL Tool,"Python





        150





        17


        Built by

          





        34 stars today",,,5064,2024-03-05T10:37:31Z
2024-03-07,https://github.com/tobymao/sqlglot,https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README.md,"The GPT Researcher is an autonomous agent designed for conducting extensive online research on various topics, promising detailed, factual, and unbiased reports. This tool overcomes the limitations of current large language models (LLMs) such as outdated information and bias, by aggregating content from over 20 web sources to form objective conclusions. It runs both ""planner"" and ""execution"" agentsâ€”the planner formulates research questions, while the execution agents find related information, which is then aggregated into a research report. The system is notably efficient, completing tasks in around 3 minutes at a cost of approximately $0.1. It's built on GPT-3.5-turbo and GPT-4-turbo, optimizing costs by judicious use of resources. The GPT Researcher also includes a user-friendly web interface, support for scraping web sources with JavaScript, and the ability to export reports to PDF among other features. Contributions to the project are welcomed, and it is noted that while the tool aims to reduce bias and factual inaccuracies, users must manage and bear the costs of API usage.",Unleashing the Power of AI in Research: Discover GPT Researcher,"GPT Researcher revolutionizes online research by generating detailed, unbiased, and factual resources, utilizing the latest AI technologies for speed and efficiency. Embracing parallelized agent work, it outpaces traditional synchronous operations, offering a blend of accuracy and rapid insights for diverse tasks. Tailored for both individuals and organizations, it ensures access to reliable information, fostering informed decision-making. Its cost-effective and swift approach, completing tasks in about 3 minutes at minimal costs, positions it as a game-changer in research methodologies. Dive into the future of research with GPT Researcher and harness the full potential of AI-driven insights.","Discover how GPT Researcher employs AI to provide rapid, accurate, and unbiased online research. Learn about its features, including customization and efficient agent work, transforming data gathering for better decision-making.",AI Coding Assistant,"Python





        5,064





        500


        Built by

          









        15 stars today",https://cowriter-images.s3.amazonaws.com/architecture.png,,7377,2021-03-13T05:01:56Z
2024-03-08,https://github.com/google/maxtext,https://raw.githubusercontent.com/google/maxtext/main/README.md,"MaxText is an open-source, high performance, scalable, and simple Large Language Model (LLM) framework designed for use with Google Cloud TPUs. It achieves up to 60% model-flop utilization and easily scales from a single host to large clusters without requiring complex optimizations, leveraging the Jax and XLA compiler. Aimed at facilitating ambitious LLM projects in both research and production, MaxText is built for ease of use and modification, encouraging users to fork and adapt it to their specific requirements. It supports seamless installation and offers robust testing frameworks, detailed runtime performance results, and comparisons with alternative LLM frameworks. MaxText stands out by its pure Python implementation, emphasizing simplicity and performance, and it is comparably efficient to other leading implementations like Nvidia's Megatron-LM but focuses on encouraging direct codebase extension. Additionally, MaxText includes features for debugging and optimization, such as stack trace collection and ahead of time compilation (AOT), enhancing usability and efficiency. It also provides support for various open models, facilitating training and inference with ease.","MaxText: The High-Performance, Scalable Open-Source LLM for Ambitious Projects","MaxText, a high-performance, scalable, open-source LLM designed for Google Cloud TPUs, combines Jax and XLA compiler's power for simple, optimization-free scaling from single hosts to vast clusters. Aimed at both research and production, MaxText invites users to rapidly deploy it as-is or fork it for customized needs. Its superior model-flop utilization and inbuilt features make it ideal for ambitious LLM projects, providing a versatile, easily adaptable foundation.","Explore MaxText, an open-source LLM offering exceptional performance and scalability on Google Cloud TPUs. Perfect for ambitious LLM projects, it's designed for easy use and customization. Learn how MaxText paves the way for groundbreaking research and production applications.",Deep Learning Framework,"Python





        664





        96


        Built by

          








        11 stars today",,,664,2023-02-28T19:47:29Z
2024-03-08,https://github.com/abi/screenshot-to-code,https://raw.githubusercontent.com/abi/screenshot-to-code/main/README.md,"The ""screenshot-to-code"" application translates screenshots into code, supporting formats like HTML/Tailwind CSS, React, Bootstrap, and Vue by leveraging GPT-4 Vision or Claude 3 for code generation and DALL-E 3 for generating similar images. It recently added support for Claude 3 and the ability to clone live websites using a URL. Recent updates include video-to-app functionality, converting video or screen recordings into functioning apps, and enhanced support for Claude Sonnet 3, promising better speed and performance than GPT-4 Vision. The app requires an OpenAI API key with GPT-4 Vision access and offers detailed instructions for setup involving a React/Vite frontend and a FastAPI backend. Docker support is available for easier setup, and the app facilitates feedback and bug reporting through its GitHub issues page or Twitter. Examples on the GitHub page showcase its capabilities with various websites.",Revolutionize Web Development with the Screenshot-to-Code App,"Discover the groundbreaking app that turns screenshots into code, leveraging GPT-4 Vision or Claude 3 technologies. Whether it's HTML/Tailwind CSS, React, Bootstrap, or Vue, transform any screenshot or live website into functional code effortlessly. Newly supporting Claude 3, and enhanced with DALL-E 3 for image generation, this tool is a game-changer for developers and designers alike. Explore examples, follow updates on Twitter, and try the app with your OpenAI key today.","Transform screenshots into HTML, React, Bootstrap, or Vue code with an innovative app using GPT-4 Vision or Claude 3. Experience effortless coding and image generation with DALL-E 3. Try it now!",AI Coding Assistant,"TypeScript





        43,766





        5,117


        Built by

          









        350 stars today",https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png,,43766,2023-11-14T17:53:32Z
2024-03-08,https://github.com/Azure/azure-sdk-for-python,https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/README.md,"The Azure SDK for Python repository is dedicated to the active development of the Azure SDK, tailored for Python developers. It serves as a hub for both existing Azure service consumers and those looking to integrate Azure services into their Python applications. Notably, support for Python 2.7 ceased on January 1, 2022. The SDK simplifies working with Azure services by offering separate libraries for each service, avoiding the need for a monolithic package. It requires Python 3.8 or later and organizes libraries into new releases and previous versions for both client and management purposes. New releases comply with the latest guidelines and core functionalities like retries and authentication, provided by the `azure-core` library. The documentation, including guidelines, version support policy, and a migration guide for transitioning from older versions, is available online. The project encourages community contributions and adheres to the Microsoft Open Source Code of Conduct. Security concerns should be reported to the Microsoft Security Response Center.",Ultimate Guide to Azure SDK for Python: Features & Getting Started,"Dive into the Azure SDK for Python for seamless cloud management and development. Keep up with the robust set of libraries available for various services, ensuring efficient cloud operations. Whether you're new or transitioning, there's comprehensive support and documentation to get you started. Note the shift from Python 2.7 support and explore the client and management libraries aimed at enhancing your Azure experience. Engage with the community and contribute to the ongoing development of this essential toolkit.","Explore the Azure SDK for Python: Discover new and previous versions of client and management libraries, get started with detailed documentation, and join the community. Ideal for developers and IT professionals.",AI Python Client,"Python





        4,153





        2,603


        Built by

          









        4 stars today",https://azure-sdk-impressions.azurewebsites.net/api/impressions/azure-sdk-for-python%2FREADME.png,,4153,2012-04-24T16:46:12Z
2024-03-08,https://github.com/google-deepmind/alphafold,https://raw.githubusercontent.com/google-deepmind/alphafold/main/README.md,"The text details the implementation and usage of AlphaFold v2, a deep learning algorithm by DeepMind for predicting protein structures with high accuracy. It highlights the main components: AlphaFold for individual proteins, and the work-in-progress AlphaFold-Multimer for protein complexes, alongside their documentation, technical notes, and a GitHub repository for installation and usage instructions. Requirements include a Linux machine, SSD storage for up to 3 TB of genetic database space, and a modern NVIDIA GPU. The AlphaFold system relies on several genetic databases and employs Docker for setup. It specifies detailed steps for installation, running predictions, and the expected outputs including the predicted protein structures. Additionally, it provides information on updating installations, citing the work, community contributions, and acknowledges third-party software and libraries used within AlphaFold. Lastly, it mentions the licensing for AlphaFold's code and model parameters, and how to contact the AlphaFold team.",Unlocking the Power of Protein Predictions with AlphaFold v2,"AlphaFold v2 has revolutionized the field of bioinformatics by providing a highly accurate inference pipeline for protein structure prediction. From its inception, AlphaFold has evolved to include the AlphaFold-Multimer, and the latest updates in v2.3.0 offer enhanced models and inference procedures. Its ability to predict protein structures with unprecedented accuracy has vast implications for drug discovery, understanding diseases, and synthetic biology. Reliable setup and execution require a Linux machine with significant storage and a modern NVIDIA GPU. For detailed steps on installation and execution, including using reduced databases for lower hardware requirements, see our complete guide.","Discover the revolutionary capabilities of AlphaFold v2 for protein structure prediction. Learn how to set up and run AlphaFold, including the AlphaFold-Multimer, with our concise guide on system requirements and execution steps.",Deep Learning Platform,"Python





        11,465





        2,041


        Built by

          








        12 stars today",https://raw.githubusercontent.com/google-deepmind/alphafold/main/imgs/header.jpg; https://raw.githubusercontent.com/google-deepmind/alphafold/main/imgs/casp14_predictions.gif,,11465,2021-06-17T14:06:06Z
2024-03-08,https://github.com/FlagOpen/FlagEmbedding,https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/master/README.md,"FlagEmbedding is a project that enhances retrieval-augmented LLMs, featuring initiatives like Long-Context LLM, LM-Cocktail for fine-tuning, and various Dense Retrieval models. A key release, the BGE-M3, encompasses multi-linguality, multi-granularity, and multi-functionality in retrieval. It stands out for supporting all three retrieval methods and hitting new benchmarks in multi-lingual and cross-lingual evaluations. Another notable project is Activation Beacon, which extends LLM context length efficiently. LM-Cocktail merges multiple models to maintain general capabilities and facilitate new task modeling without fine-tuning. LLM Embedder caters to diverse retrieval needs of LLMs, while BGE Reranker and BGE Embedding address re-ranking and general embedding challenges, respectively. Collaborations are encouraged, with MIT License governing the project's open-source code.",Revolutionizing Language Models: FlagEmbedding's Multifaceted Approach,"FlagEmbedding stands at the forefront of developing sophisticated language models with its flagship projects including Long-Context LLMs, LM-Cocktail for fine-tuning, and groundbreaking retrieval models like BGE-M3. With projects spanning dense retrieval, rerankers, and benchmarks such as C-MTEB, FlagEmbedding is pushing the boundaries of multilingual and multi-functional embedding models. The recent release of BGE-M3 has set new standards in the field by excelling in multi-lingual, multi-granular, and multi-functional retrieval, making it an essential tool for researchers and developers alike.",Explore FlagEmbedding's innovative projects including BGE-M3 for advanced retrieval and LM-Cocktail for fine-tuning language models. Discover how it paves the way for future LLMs.,Natural Language Processing,"Python





        3,845





        248


        Built by

          









        22 stars today",,,3845,2023-08-02T02:08:11Z
2024-03-08,https://github.com/facebookresearch/llama,https://raw.githubusercontent.com/facebookresearch/llama/main/README.md,"The latest version of Llama, a significant development in large language models, has been made available to a wide audience including individuals, creators, researchers, and businesses to facilitate experimentation, innovation, and growth in a responsible manner. This release introduces pre-trained and fine-tuned Llama language models with parameters ranging from 7B to 70B, alongside the model weights and starting code. Users can access and run these models through a straightforward set-up process, with detailed examples available in the llama-recipes repository on GitHub. This initiative also includes steps for quick start and running inference, with support for different model-parallel values depending on the model. Furthermore, the release is backed by comprehensive documentation on updates, download issues resolution, and a guide on responsible use to mitigate potential risks associated with deploying the technology. Llama 2 aims to empower technology development while ensuring ethical AI advancements through its open licensing for both research and commercial use, guided by an Acceptable Use Policy.",Unleashing Llama 2: A Giant Leap in Language Model Technology,"Explore the latest advancements in language model technology with Llama 2, designed for innovators across all fields. This version offers pre-trained and finely-tuned language models ranging from 7B to 70B parameters, making it easier than ever to harness the power of AI. Access to the models is simplified, with detailed guidance for both downloading and running inferences. Embrace the future of AI research and application by diving into Llama 2, where possibilities are limitless. Ensure responsible usage with our detailed guidelines, and join a community pushing the boundaries of what's possible with AI.","Discover Llama 2, the groundbreaking language model accessible to all. With models from 7B to 70B parameters, Llama 2 is revolutionizing AI innovation and research. Learn how to start experimenting today.",Language Models,"Python





        50,782





        8,715


        Built by

          









        72 stars today",,,50782,2023-02-14T09:29:12Z
2024-03-08,https://github.com/taojy123/KeymouseGo,https://raw.githubusercontent.com/taojy123/KeymouseGo/master/README.md,"KeymouseGo is a software designed for recording and automating mouse and keyboard operations on Windows, Linux, and macOS platforms. This lightweight alternative to macro software enables users to easily record actions such as mouse clicks and keyboard inputs, and then automate them by playing back these actions as many times as needed. This can be particularly useful for repetitive tasks, allowing users to save time and effort by automating processes with previously recorded scripts. The software is developed using Python and supports customization through scripting, allowing for extended functionality beyond basic recording. Users can run scripts via a graphical interface or command line, adjust execution speed, set scripts to run multiple times, and even integrate custom extensions for advanced automation tasks. KeymouseGo is open-source, and thanks to its community and contributors, it continues to evolve with new features and improvements.",Automating Repetitive Tasks with KeymouseGo: A Comprehensive Guide,"Discover KeymouseGo, the revolutionary tool designed to record and playback mouse and keyboard actions, making repetitive tasks effortless. Whether for work or personal projects, KeymouseGo offers a simplified, green alternative to traditional automation software. It supports Windows, Linux, and macOS, making it incredibly versatile. With easy installation and use, anyone can automate their tasks, saving time and increasing productivity. Dive into the world of efficient computing with KeymouseGo!","Learn how to automate repetitive tasks easily with KeymouseGo. This guide covers installation, features, and usage on Windows, Linux, and macOS, making your work more efficient and productive.",Open Source Tool,"Python





        5,720





        876


        Built by

          









        25 stars today",https://raw.githubusercontent.com/taojy123/KeymouseGo/master/Preview.png; https://raw.githubusercontent.com/taojy123/KeymouseGo/master/jetbrains-variant-2.png,,5720,2015-01-31T04:16:37Z
2024-03-08,https://github.com/KurtBestor/Hitomi-Downloader,https://raw.githubusercontent.com/KurtBestor/Hitomi-Downloader/master/README.md,"The text introduces Hitomi-Downloader, a software designed for downloading multimedia from various online sources. It boasts a simple interface, supports download acceleration, can handle up to 24 threads per task, and offers a speed limit feature. Additionally, it supports user scripts, BitTorrent and Magnet links, as well as M3U8 and MPD format videos. Other features include a dark mode, portability, clipboard monitoring, and easy task organization. The downloader is compatible with a wide range of sites, including major ones like YouTube, Facebook, Instagram, and more specialized sites catering to different interests and media types, indicating a broad utility for users interested in content downloading.",Maximize Downloads with Hitomi Downloader: The Ultimate Guide,"Discover the power of Hitomi Downloader, your ultimate solution for fast and efficient downloading. With a simple interface, download acceleration, and support for numerous sites, it's the downloader you never knew you needed. From mainstream platforms like YouTube to niche sites, Hitomi Downloader simplifies your downloads with features like dark mode, BitTorrent support, and a clipboard monitor. Perfect for organizing tasks and multitasking, it's a must-have tool in your digital toolkit.","Explore Hitomi Downloader: A versatile tool offering download acceleration, multi-thread support, and compatibility with many sites, including YouTube, Pixiv, and more. Simplify your downloads today!",Open Source Tool,"Python





        19,003





        1,819


        Built by

          









        121 stars today",https://raw.githubusercontent.com/KurtBestor/Hitomi-Downloader/master/imgs/card_crop.png; https://raw.githubusercontent.com/KurtBestor/Hitomi-Downloader/master/imgs/how_to_download.gif,,19003,2017-11-26T09:23:06Z
2024-03-08,https://github.com/unslothai/unsloth,https://raw.githubusercontent.com/unslothai/unsloth/main/README.md,"Unsloth.ai offers tools for fine-tuning AI models like Mistral, Gemma, and Llama up to 5x faster using 70% less memory, making the process beginner-friendly and cost-efficient. Their free Colab notebooks enable users to finetune models effortlessly with significant performance enhancements, such as up to 3.9x faster speeds and up to 74% less memory usage across various models, including Gemma, Mistral, Llama-2, TinyLlama, CodeLlama, and others. The platform updates include the addition of conversational notebooks and support for Direct Preference Optimization (DPO). Unsloth.ai's collaboration with Hugging Face, evident in their blog and official documentation, highlights their integration into the broader AI community. They also offer detailed installation instructions for different environments, comprehensive documentation for model training, and extensive benchmarking data showcasing the platform's efficiency improvements in training times and memory usage across various settings, including single and multi-GPU setups.",Accelerate Your AI Project: How to Fine-tune Models Faster with Unsloth.ai,"Discover the power of Unsloth.ai, the beginner-friendly solution for accelerating Mistral, Gemma, Llama, and more with up to 70% less memory usage. By leveraging Unsloth.ai's optimized fine-tuning process on Colab, users enjoy a significant speed boost and efficiency, making AI model training more accessible than ever. Dive into using Unsloth.ai's free notebooks to supercharge your AI projects today!","Boost your AI model training with Unsloth.ai. Learn how to fine-tune AI models like Mistral, Gemma, and Llama 2-5x faster with 70% less memory on Colab. Start for free with beginner-friendly notebooks.",Deep Learning Platform,"Python





        3,905





        182


        Built by

          







        29 stars today",https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png; https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png; https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png; https://raw.githubusercontent.com/unslothai/unsloth/main/images/buy me a coffee button.png; https://i.ibb.co/sJ7RhGG/image-41.png; https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png; https://i.ibb.co/sJ7RhGG/image-41.png; https://i.ibb.co/sJ7RhGG/image-41.png,,3905,2023-11-29T16:50:09Z
2024-03-08,https://github.com/hiyouga/LLaMA-Factory,https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README.md,"LLaMA Factory is a comprehensive toolkit for fine-tuning and deploying various large language models, designed to facilitate easy customization and optimization of models for specific tasks. The toolkit supports a wide range of models, including LLaMA, Mistral, and BLOOM, among others, and provides integrated methods for pre-training, supervised fine-tuning, and more advanced techniques like reward modeling and PPO training. It also offers scalable resources, advanced algorithms, and practical tricks to enhance model performance and efficiency. The benchmarking section demonstrates the efficiency of LLaMA Factory's LoRA tuning compared to traditional methods, showing significant improvements in training speed and Rouge scores for text generation tasks. Recent updates have introduced new algorithms, support for more models, and various other enhancements. Users can train models on a single GPU or distribute the training process across multiple GPUs, customize training scripts, and evaluate model performance using supplied datasets or their own. Additionally, LLaMA Factory provides tools for integrating fine-tuned models into applications with ease, offering inference APIs, command-line interfaces, and web demonstrations.",LLaMA Factory: The Cutting-Edge Model Tuning Tools Simplified,"Discover the groundbreaking LLaMA Factory, a versatile tool for fine-tuning large language models with ease and efficiency. It supports a wide range of models and training approaches, enabling faster training and better performance with lesser resource consumption. From advanced algorithms to practical tricks, LLaMA Factory is your gateway to unlocking the full potential of AI language models.","Explore how LLaMA Factory simplifies the fine-tuning of large language models, offering faster training, advanced algorithms, and support for various models. Start optimizing your AI projects with LLaMA Factory today.",Collaborative AI Framework,"Python





        12,758





        1,576


        Built by

          









        72 stars today",https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png,,12758,2023-05-28T10:09:12Z
2024-03-08,https://github.com/JoeanAmier/XHS-Downloader,https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/README.md,"XHS-Downloader is a free, open-source tool for extracting links and downloading content without watermarks from å°çº¢ä¹¦ (Xiaohongshu). It allows users to gather content information, download links, and store data efficiently. The tool can automate downloads, manage file integrity, customize download formats, and organize content into folders. It supports both programmatic and command-line usage, with extensive features for working with å°çº¢ä¹¦ content. Additionally, a user script is available for those using the Tampermonkey browser extension, offering a convenient way to experience the project's functionality without installation. XHS-Downloader emphasizes legal and ethical use, providing a GNU General Public License v3.0, and disclaims any responsibility for misuse or legal implications. Users are encouraged to contribute through stars or donations and can contact the author for support or further inquiry.",Ultimate Guide to XHS-Downloader: Download Xiaohongshu Content with Ease,"Discover the power of XHS-Downloader, the ultimate tool for downloading content from Xiaohongshu without watermarks. This comprehensive guide covers everything from setting up and using the downloader to advanced features like custom download formats and API support. Whether you're downloading for personal use or exploring content curation, XHS-Downloader offers a robust solution free of charge, ensuring you stay ahead in content gathering without compromising on quality.","Learn how to use XHS-Downloader to effortlessly download and curate content from Xiaohongshu. This guide offers insights into its features, setup, and advanced functionalities for watermark-free downloads.",Document Conversion Tool,"Python





        2,866





        381


        Built by

          








        68 stars today",https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/XHS-Downloader.png; https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/screenshot/ç¨‹åºè¿è¡Œæˆªå›¾CN1.png; https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/screenshot/ç¨‹åºè¿è¡Œæˆªå›¾CN2.png; https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/screenshot/ç¨‹åºè¿è¡Œæˆªå›¾CN3.png; https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/screenshot/ç”¨æˆ·è„šæœ¬æˆªå›¾1.png; https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/screenshot/ç”¨æˆ·è„šæœ¬æˆªå›¾2.png; https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/static/screenshot/èŽ·å–Cookieç¤ºæ„å›¾.png; https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/./static/å¾®ä¿¡èµžåŠ©äºŒç»´ç .png; https://raw.githubusercontent.com/JoeanAmier/XHS-Downloader/master/./static/æ”¯ä»˜å®èµžåŠ©äºŒç»´ç .png,,2866,2023-08-16T11:03:36Z
2024-03-08,https://github.com/WZMIAOMIAO/deep-learning-for-image-processing,https://raw.githubusercontent.com/WZMIAOMIAO/deep-learning-for-image-processing/master/README.md,"This tutorial summarizes the author's research during their postgraduate studies, aiming to assist others in learning about deep learning applications in image processing. It will be shared through videos, covering the structure and innovation of networks, and their construction and training using PyTorch and TensorFlow (with Keras). Various models are addressed, including LeNet, AlexNet, VGGNet, GoogLeNet, ResNet, ResNeXt, MobileNet, ShuffleNet, EfficientNet, Vision Transformer, Swin Transformer, ConvNeXt, and MobileViT for image classification. There's also content on object detection with Faster R-CNN, SSD, YOLO series, FCOS; semantic segmentation with FCN, DeepLabV3, LR-ASPP, U-Net, U2Net; instance segmentation with Mask R-CNN; and keypoint detection with HRNet. The tutorial provides links to videos for detailed explanations and coding tutorials. The recommended environment includes Anaconda3, Python 3.6/3.7/3.8, PyCharm, PyTorch 1.10, torchvision 0.11.1, and TensorFlow 2.4.1. The creator encourages following their WeChat public account for learning summaries and discusses further on their CSDN blog and Bilibili channel.",Unlocking the Power of Deep Learning in Image Processing: A Comprehensive Tutorial,"Dive into the transformative world of image processing with our detailed tutorial on deep learning applications. From structure and innovation in neural networks to hands-on training using Pytorch and TensorFlow, we break down complex concepts into understandable bits. Enhance your skills with downloadable course materials and stay ahead with updates on the latest in deep learning technology. Whether you're a beginner or looking to refine your knowledge, this guide provides invaluable insights into the future of image analysis.","Explore the applications of deep learning in image processing with our expert guide. Learn about neural network structures, Pytorch and TensorFlow training, and access downloadable course materials for a comprehensive understanding.",Deep Learning Platform,"Python





        19,942





        7,623


        Built by

          






        49 stars today",,,19942,2019-11-14T15:02:27Z
2024-03-08,https://github.com/lm-sys/FastChat,https://raw.githubusercontent.com/lm-sys/FastChat/main/README.md,"FastChat is an open platform designed for training, serving, and evaluating large language model (LLM) based chatbots, featuring its application Chatbot Arena which has processed over 6 million chat requests for more than 50 LLMs. This platform provides training and evaluation code for cutting-edge models like Vicuna and MT-Bench, and supports a distributed multi-model serving system with a web UI and OpenAI-compatible RESTful APIs. Recent updates include the release of LMSYS-Chat-1M, a conversational dataset, Vicuna v1.5, and Chatbot Arena Conversations dataset. It offers detailed installation instructions, model weights guidance, inference options for various computing environments, and notes for fine-tuning. The platform also accommodates scalability, providing OpenAI-compatible RESTful APIs, evaluation benchmarks, and fine-tuning documentation. FastChat represents a significant resource for developers interested in deploying and evaluating AI chatbots with state-of-the-art LLM technologies.",FastChat: Revolutionizing LLM-Based Chatbots with Chatbot Arena,"Discover how FastChat is transforming the landscape of language model based chatbots through its innovative platform. Harnessing the power of over 50 LLMs, FastChat facilitates over 6 million chat interactions in the Chatbot Arena, offering an unparalleled resource with over 200K human votes influencing the LLM Elo leaderboard. Dive into the cutting-edge features including multi-model training, evaluation, and a distributed serving system designed for state-of-the-art models like Vicuna and MT-Bench. Stay informed with the latest releases, including LMSYS-Chat-1M, a comprehensive real-world LLM conversation dataset. FastChat sets a new standard for chatbot development and evaluation, making it a cornerstone for researchers and developers alike.","Explore FastChat, the ultimate platform for training, serving, and evaluating LLM-based chatbots. With over 6 million chat requests and 200K human votes, FastChat's Chatbot Arena is leading the way in advanced chatbot technology and LLM evaluation. Learn about the latest updates and how you can leverage its features for your projects.",Collaborative AI Framework,"Python





        32,265





        3,962


        Built by

          









        51 stars today",https://raw.githubusercontent.com/lm-sys/FastChat/main/assets/demo_narrow.gif; https://raw.githubusercontent.com/lm-sys/FastChat/main/assets/screenshot_cli.png; https://raw.githubusercontent.com/lm-sys/FastChat/main/assets/screenshot_gui.png,,32265,2023-03-19T00:18:02Z
2024-03-08,https://github.com/parthsarthi03/raptor,https://raw.githubusercontent.com/parthsarthi03/raptor/master/README.md,"RAPTOR introduces a novel approach for retrieval-augmented language models by creating a recursive tree structure from documents, improving efficiency and context-aware information retrieval beyond the capabilities of traditional models. This method is detailed in their paper, which is available for further reading. To use RAPTOR, Python 3.8 or higher is required. Users can clone the RAPTOR repository and install its dependencies. The setup involves setting up an OpenAI API key, initializing RAPTOR, adding documents to its tree, and querying it to answer specific questions. It also allows for the customization and extension with other models for summarization, question-answering, and embedding generation, by implementing specific classes for each requirement. Additionally, RAPTOR encourages open-source contribution and is licensed under the MIT License. Citations for using RAPTOR in research are also provided.",Exploring RAPTOR: Next-Level Information Retrieval,"RAPTOR revolutionizes retrieval-augmented language models by introducing a tree-structured approach, allowing for more efficient and context-aware searching through extensive texts, overcoming traditional models' limitations. This novel method shows promise in enhancing information retrieval's accuracy and speed. For those interested in digging deeper, the original paper provides comprehensive insights into RAPTOR's methodologies and implementations, paving the way for advanced research and applications in language models.","Discover how RAPTOR leverages a recursive tree structure to enhance retrieval-augmented language models, making information retrieval more efficient and context-aware across vast texts.",Natural Language Processing,"Python





        186





        22


        Built by

          





        14 stars today",https://raw.githubusercontent.com/parthsarthi03/raptor/master/raptor.jpg; https://raw.githubusercontent.com/parthsarthi03/raptor/master/raptor.jpg,,186,2024-02-27T01:33:26Z
2024-03-09,https://github.com/jiaweizzhao/GaLore,https://raw.githubusercontent.com/jiaweizzhao/GaLore/master/README.md,"GaLore introduces a memory-efficient algorithm for training large language models (LLMs) by employing Gradient Low-Rank Projection. This strategy maintains full-parameter learning while being more memory-efficient compared to traditional low-rank adaptation methods like LoRA. GaLore's design allows it to be easily integrated with existing optimizers with minimal code changes. The repository provides installation instructions, usage examples including how to configure GaLore with different optimizers, and benchmark scenarios for pre-training and fine-tuning models on datasets like C4 and GLUE tasks. The benchmarks highlight GaLore's capability to train large models on limited hardware, for instance, training a 7B parameter model on a single NVIDIA RTX 4090 GPU. Additionally, it offers per-layer weight updates for single GPU setups, aiming for future support for multi-GPU configurations. The citation for the GaLore method is also provided for referencing in academic work.",Unlocking Memory-Efficient LLM Training with GaLore Algorithm,"Discover the revolutionary GaLore algorithm for memory-efficient Large Language Models (LLM) training, enabling full-parameter learning without the high memory cost. GaLore stands out by allowing integration with existing optimizers in just two lines of code, offering a practical solution for optimizing memory usage during LLM training. Find out how to install, configure, and utilize this groundbreaking tool for both pre-training and fine-tuning tasks, making it a game changer for researchers and developers alike.","Explore the GaLore algorithm for efficient LLM training, a memory-saving strategy that facilitates full-parameter learning with ease of integration into any optimizer. A perfect tool for enhancing LLM training and optimization.",Deep Learning Platform,"Python





        446





        47


        Built by

          





        67 stars today",https://raw.githubusercontent.com/jiaweizzhao/GaLore/master/imgs/galore_code_box.png,,446,2024-03-07T01:34:59Z
2024-03-09,https://github.com/dortania/OpenCore-Legacy-Patcher,https://raw.githubusercontent.com/dortania/OpenCore-Legacy-Patcher/main/README.md,"The OpenCore Legacy Patcher is a Python-based initiative leveraging Acidanthera's OpenCorePkg and Lilu to enable macOS functionality on both supported and unsupported Macs. Aimed primarily at revitalizing older Macs (dating back to 2007) that Apple no longer supports, it facilitates the running of macOS Big Sur and later versions. The project features support for macOS Big Sur through Sonoma, enabling native system updates, graphics acceleration for Metal and non-Metal GPUs, and advanced security measures like System Integrity Protection and Secure Boot, among others. It also unlocks additional features like Sidecar and AirPlay even on supported Macs and offers enhanced power management for non-Apple storage devices. Clean installs and upgrades are possible, excluding systems already patched with other tools due to integrity concerns. While it's aimed at Big Sur and newer versions, for Mojave and Catalina, dosdude1's patchers are recommended. Offered as-is, the project is backed by a community on Discord for support and troubleshooting.",Revitalize Your Old Mac: How OpenCore Legacy Patcher Unlocks New macOS Features,"OpenCore Legacy Patcher offers a new lease of life for unsupported Macs, enabling them to run macOS Big Sur and newer versions with ease. This Python-based project leverages Acidanthera's OpenCorePkg and Lilu to not only support systems as old as 2007 but also unlock modern features like Sidecar and AirPlay. It supports clean installs and upgrades, ensuring no firmware patching is needed for a seamless experience. Whether you're looking to extend the life of your Mac or unlock new functionalities, OpenCore Legacy Patcher is your go-to solution.",Discover how OpenCore Legacy Patcher can rejuvenate unsupported Macs by enabling macOS Big Sur and newer installations while unlocking advanced features like Sidecar and AirPlay without any firmware patching required.,Open Source Tool,"Python





        10,404





        1,025


        Built by

          









        21 stars today",https://raw.githubusercontent.com/dortania/OpenCore-Legacy-Patcher/main/images/OC-Patcher.png,,10404,2020-11-24T01:54:56Z
2024-03-09,https://github.com/langgenius/dify,https://raw.githubusercontent.com/langgenius/dify/main/README.md,"Dify is an LLM (Large Language Model) application development platform that has successfully built over 100,000 applications. It provides an integrated solution combining Backend as a Service (BaaS) and LLM Operations (LLMOps), designed for creating generative AI-native applications. Dify features a built-in Retriever-Augmented Generation (RAG) engine, and offers the ability to deploy customized Assistant APIs and GPTs based on various LLMs. Users can experiment by leveraging Dify.AI Cloud, which comes with 200 free requests to OpenAI GPT-3.5. The platform stands out due to its API-oriented programming approach, open-source ecosystem strategy, and support for a wide array of LLMs. Dify includes features such as a Prompt IDE for visual application development, a RAG engine for enhancing queries, and an AI Agent framework for customization. It encourages continuous operations through monitoring and analysis for prompt, dataset, or model improvements. The community edition of Dify can be easily installed using Docker, with further options for Kubernetes deployment through a Helm chart. Dify encourages contributions, aims to extend its language support, and offers various channels for community and support, including direct meetings for feedback and technical discussions.",Revolutionize App Development: Leveraging Dify.AI for Next-Gen AI Applications,"Dify.AI is transforming the landscape of LLM application development, having successfully aided in the creation of over 100,000 applications. By integrating BaaS with LLMOps and offering a comprehensive tech stack essential for building AI-native applications, Dify distinctively supports deployment of personalized Assistants API and GPTs tailored to various LLMs. With features like a built-in RAG engine and support for a wide array of LLMs, Dify.AI Cloud now offers a seamless experience with 200 free requests to OpenAI GPT-3.5. Dive into the future of app development by exploring Dify's unique advantages over counterparts like LangChain and Assistants API, and leverage the power of visual orchestration, continuous operations, and diverse AI Agent tools.","Discover how Dify.AI is redefining application development with its LLM platform, offering over 100,000 apps with BaaS, LLMOps, a RAG engine, and support for various LLMs including OpenAI GPT. Start your journey with 200 free OpenAI GPT-3.5 requests.",Collaborative AI Framework,"Python





        16,922





        2,159


        Built by

          









        51 stars today",https://raw.githubusercontent.com/langgenius/dify/main/./images/describe.png; https://raw.githubusercontent.com/langgenius/dify/main/./images/demo.png; https://raw.githubusercontent.com/langgenius/dify/main/./images/models.png,,16922,2023-04-12T07:40:24Z
2024-03-09,https://github.com/d2l-ai/d2l-zh,https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/README.md,"""Dive into Deep Learning"" (D2L.ai), an open-source project, aims to teach deep learning concepts, background knowledge, and code in an integrated manner. It strives to be a unified resource, freely accessible online, providing technical depth to enable readers to become applied deep learning scientists. The text includes executable code to show problem-solving in practice, allowing for experimentation and learning through modification and observation. It is designed for continuous iteration by the community to stay updated with the rapidly advancing deep learning field. The book is supplemented by forums for questions and experience exchange. It is recommended by academics and industry professionals alike and is used as a textbook or reference book in universities worldwide. The project encourages community contributions and provides links for further engagement.",Mastering Deep Learning: An In-Depth Guide with D2L.ai,"Dive into the world of Deep Learning with D2L.ai, an open-source project aimed at teaching both the theoretical concepts and practical application skills needed to become an adept Deep Learning Scientist. This unified resource is freely available online, offering deep technical knowledge, runnable code examples, and a dynamic community forum for peer support. It serves as a comprehensive guide to not just understanding the mathematical foundations of deep learning, but also how to implement and improve upon these methods in real-world scenarios. Ideal for both students and professionals, D2L.ai is continuously updated to keep pace with the rapidly evolving field.","Discover how to become a deep learning expert with D2L.ai's comprehensive guide, featuring in-depth technical knowledge, practical code examples, and a supportive community forum.",Deep Learning Platform,"Python





        54,824





        10,206


        Built by

          









        82 stars today",https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/eq.jpg; https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/figure.jpg; https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/code.jpg; https://raw.githubusercontent.com/d2l-ai/d2l-zh/master/static/frontpage/_images/notebook.gif; https://d2l.ai/_images/map.png,,54824,2017-08-23T04:40:24Z
2024-03-09,https://github.com/weaviate/Verba,https://raw.githubusercontent.com/weaviate/Verba/main/README.md,"Verba: The Golden RAGtriever is an open-source application designed for streamlined Retrieval-Augmented Generation (RAG), offering a user-friendly interface for efficient data exploration and insight extraction. It supports local and cloud deployment, integrating with LLM providers such as OpenAI, Cohere, and HuggingFace. Installation is straightforward via pip, and the application boasts features like advanced query resolution, effortless data import, and accelerated queries through a semantic cache. Verba combines Weaviate's Generative Search technology and Large Language Models to understand and respond to queries contextually. The tool simplifies importing documents and features a robust project architecture, welcoming community contributions and open-source development. Deployment options include pip installation, Docker, or building from source, with comprehensive support for API key configuration for various services.",Exploring Verba: The Ultimate Tool for Retrieval-Augmented Generation,"Discover Verba: The Golden RAGtriever, a groundbreaking open-source application revolutionizing Retrieval-Augmented Generation (RAG) with an intuitive, user-friendly interface. Seamlessly integrate with leading LLM providers like OpenAI, Cohere, and HuggingFace. Simplify your data analysis and insight extraction process locally or through cloud services. Get started effortlessly with Verba by installing it via pip. Join the community of forward-thinking developers and harness the power of generative search with Verba today.","Unveil the power of Verba, the open-source application designed for streamlined Retrieval-Augmented Generation (RAG). Install easily via pip and transform data analysis with leading LLM integrations.",Collaborative AI Framework,"Python





        1,861





        204


        Built by

          









        29 stars today",https://github.com/weaviate/Verba/blob/dev/img/verba.gif; https://github.com/weaviate/Verba/blob/dev/img/verba_screen.png; https://github.com/weaviate/Verba/blob/dev/img/verba_import.png; https://github.com/weaviate/Verba/blob/dev/img/verba_status.png; https://github.com/weaviate/Verba/blob/dev/img/verba_data.gif,,1861,2023-07-28T16:53:42Z
2024-03-09,https://github.com/W01fh4cker/CVE-2024-27198-RCE,https://raw.githubusercontent.com/W01fh4cker/CVE-2024-27198-RCE/main/README.md,"The text discusses the exploitation of vulnerabilities in JetBrains TeamCity, using different cyberspace search engines like Fofa, ZoomEye, Hunter.how, and Shodan to locate instances using specific query syntaxes unique to each platform. It instructs users on setting up a Python 3.9 environment with necessary libraries and highlights problems encountered during exploitation, such as issues with yakit proxy leading to failed uploads of malicious plugins, errors in making post requests after webshell upload, and the workaround of retrieving X-TC-CSRF-Token through a 403 error. Additionally, it details creating a vulnerability recurrence environment using Docker to pull and run a vulnerable TeamCity server image. References are provided for further reading on relevant CVEs addressed in the JetBrains TeamCity updates.

",A Comprehensive Guide to Mapping Cyberspace: Exploiting JetBrains TeamCity Vulnerabilities,"Discover crucial techniques for exploiting JetBrains TeamCity vulnerabilities across different platforms, including Fofa, ZoomEye, Hunter.how, and Shodan. Learn to navigate the challenges of cyber defense mechanisms, from uploading malicious plugins to mitigating post-request errors. This guide also covers setting up a vulnerable environment using Docker for practical vulnerability replication. Unlock the potential of Python scripting to automate your cybersecurity testing. Enhance your ethical hacking skills by understanding and solving common problems in cyberspace mapping.","Unveil expert strategies for exploiting JetBrains TeamCity with our in-depth guide. Learn to use Fofa, ZoomEye, Hunter.how, and Shodan for effective cyberspace mapping and overcome cybersecurity challenges.",Cybersecurity Tool,"Python





        47





        15


        Built by

          





        6 stars today",https://raw.githubusercontent.com/W01fh4cker/blog_image/main/image-20240308230546869.png; https://raw.githubusercontent.com/W01fh4cker/blog_image/main/image-20240308230808193.png; https://raw.githubusercontent.com/W01fh4cker/blog_image/main/image-20240307232348600.png; https://raw.githubusercontent.com/W01fh4cker/blog_image/main/image-20240307232404056.png,,47,2024-03-06T03:15:03Z
2024-03-09,https://github.com/prometheusdevel/Prometheus,https://raw.githubusercontent.com/prometheusdevel/Prometheus/main/README.md,"Prometheus is a tool designed for educational and research purposes, emphasizing the creator's disclaimer against its misuse for illegal acts. It features a range of functionalities aimed at system infiltration and data extraction, including GUI building, startup execution, fake errors, binding executables, and various forms of session and data theft from popular platforms and services (e.g., Discord, Steam, Epic, browsers, cryptocurrency wallets). It offers additional VIP features, such as UAC Bypass, disabling Windows Defender, audio recording, and more malicious capabilities like keylogging and file encryption. To build the stub, users need Windows 7, Python 3.10, and an internet connection. The process involves downloading Prometheus, extracting the content, and using a builder script. The VIP version, accessible via purchase, includes more advanced features, catering to users seeking further capabilities beyond the free version.",Exploring Prometheus: A Comprehensive Guide to Its Features and Capabilities,"Discover the potential of Prometheus, a versatile tool equipped with a GUI Builder, capable of executing tasks on startup, and providing robust security features along with a detailed VIP version for enhanced functionalities. It offers a wide range of features from password and session stealing across various platforms to data encryption and more, catering to both educational and research purposes. Understand the prerequisites and the step-by-step guide to build and utilize Prometheus effectively, ensuring full awareness of its capabilities and disclaimer responsibilities. Perfect for those seeking an in-depth look at its features, VIP benefits, and user requirements.","A detailed guide exploring Prometheus' functionalities, including GUI building, security features, and VIP benefits. Learn the setup requirements and how to utilize this tool responsibly with our step-by-step guide.",Cybersecurity Tool,"Python





        134





        97


        Built by

          





        30 stars today",https://raw.githubusercontent.com/prometheusdevel/Prometheus/main/logo.png; https://github.com/prometheusdevel/Prometheus/blob/main/window.png; https://github.com/prometheusdevel/Prometheus/blob/main/msg.png; https://github.com/prometheusdevel/Prometheus/blob/main/virustotal.png,,134,2023-12-19T23:16:42Z
2024-03-09,https://github.com/TobikoData/sqlmesh,https://raw.githubusercontent.com/TobikoData/sqlmesh/main/README.md,"SQLMesh is a modern framework for data transformation and modeling, offering compatibility with dbt but aiming for ease of use, accuracy, and efficiency. It allows data professionals to run and deploy transformations using SQL or Python, enhancing the efficiency, reliability, and maintenance of dbt projects without being just an alternative. Key features include semantic understanding of SQL for error checking, simple SQL definitions without Jinja + YAML, self-documenting queries, efficient never-rebuild table logic, partition-based incremental models, a Terraform-like workflow, CI/CD integrations, automatic column lineage, and data contracts with unit tests. SQLMesh encourages community interaction through its Slack, GitHub, and email, welcoming contributions. Installation is straightforward via pypi.",Revolutionize Your Data Workflows with SQLMesh: The Ultimate dbt Alternative,"Discover SQLMesh, a cutting-edge data transformation framework offering backwards compatibility with dbt and superior efficiency. With its unique features like semantic understanding of SQL, compile-time error checking, and a CI/CD bot for seamless integrations, SQLMesh not only enhances your dbt projects but sets a new standard in data modeling and transformation. Dive deeper into its capabilities, including partition-based incremental models and automatic column level lineage, to streamline your data operations. Ideal for data practitioners seeking an intuitive yet powerful tool, SQLMesh promises to make your data projects more reliable and maintainable. Get started today and join the vibrant community to excel in your data journey.","Unlock the full potential of your data with SQLMesh, a next-gen framework offering seamless dbt compatibility, enhanced efficiency, and robust features for data modeling and transformation. Explore now.",Data Transformation Tool,"Python





        1,115





        74


        Built by

          









        16 stars today",,,1115,2022-09-23T16:28:39Z
2024-03-09,https://github.com/vanna-ai/vanna,https://raw.githubusercontent.com/vanna-ai/vanna/main/README.md,"Vanna is an open-source Python framework for SQL generation using the Retrieval-Augmented Generation (RAG) method, designed to be both powerful and user-friendly. With an MIT license, it allows users to train a RAG model with their data and then generate SQL queries by simply asking questions. This tool is particularly useful for users with complex datasets, offering high accuracy, security, privacy, and the capability to self-learn from successful queries. Vanna supports any SQL database and allows for various front-end interfaces, including Jupyter Notebook, Slack, web apps, Streamlit apps, or custom interfaces. It's easy to get started with examples and detailed documentation provided for installing and using the tool. Vanna is extendable, supporting connections to any database, language models (LLMs), and vector databases, making it adaptable to future advancements in LLM technology.",Harnessing Vanna: Revolutionize Your Data with Python's RAG Framework for SQL Generation,"Explore the cutting-edge Vanna, an open-source Python framework leveraging Retrieval-Augmented Generation (RAG) for advanced SQL generation and more. Designed for simplicity, Vanna transforms data queries into actionable SQL with easy steps, empowering users with minimal technical expertise. The framework supports diverse SQL databases and integrates seamlessly with various front-end interfaces, offering versatility in deploying data-driven applications. Get started with Vanna today to unlock sophisticated data querying capabilities in your projects.","Discover how Vanna, the MIT-licensed RAG framework for SQL generation, revolutionizes data analysis by transforming simple questions into actionable SQL queries. Learn about its easy training process, interface support, and secure, private data handling.",SQL Tool,"Python





        5,668





        328


        Built by

          









        36 stars today",https://raw.githubusercontent.com/vanna-ai/vanna/main/img/vanna-readme-diagram.png; https://raw.githubusercontent.com/vanna-ai/vanna/main/img/top-10-customers.png,,5668,2023-05-13T17:26:28Z
2024-03-09,https://github.com/UpstageAI/dataverse,https://raw.githubusercontent.com/UpstageAI/dataverse/main/README.md,"Dataverse is an open-source project aimed at enhancing the ETL (Extract, Transform, Load) pipeline process using Python, particularly for data scientists, analysts, and developers. It simplifies the workflow by integrating multiple preprocessing libraries and offering easy-to-use configurations for Spark, even for those with limited familiarity. Dataverse supports collaboration through uniform preprocessing codes and is designed to be block-based, configure-based, and extensible to accommodate custom features. To use this library, prerequisites include Python (versions 3.10 to 3.11), JDK (version 11), and PySpark, with installation available via PyPi. It includes a quick start guide with examples for custom functions, testing ETL processes, and scaling out with EMR. The project encourages contributions and is licensed under the Apache-2.0 license. Acknowledged by the Data-Centric LLM Team at Upstage, Dataverse focuses on advancing data handling for Large Language Models (LLM) and is cited for its innovative approach towards a streamlined data ecosystem.",Exploring Dataverse: A Revolution in Data Engineering with Python,"Discover how Dataverse revolutionizes the ETL pipeline in data engineering, making data processing seamless for developers and data scientists alike. With its simple, block-based approach, Dataverse integrates various libraries including HuggingFace datasets, simplifies Spark, and enhances collaboration across different expertise levels, all within the Python ecosystem. Its configuration-based setup invites users at any expertise level, supporting a broad range of custom features for diverse project demands. Dive into the future of data handling with Dataverse - the open-source project leading the way in data-centric innovation for Large Language Models (LLMs).","Learn how Dataverse transforms ETL pipelines with Python, offering a user-friendly, integrated approach to data engineering, Spark simplification, and collaborative features. Unlock the power of data with Dataverse.",Data Ingestion Tool,"Python





        156





        11


        Built by

          









        20 stars today",https://raw.githubusercontent.com/UpstageAI/dataverse/main/docs/images/dataverse_logo-color.png,,156,2023-08-21T15:50:11Z
2024-03-09,https://github.com/alibaba/data-juicer,https://raw.githubusercontent.com/alibaba/data-juicer/main/README.md,"Data-Juicer is an advanced multimodal data processing system designed for enhancing the quality and digestibility of data for large language models (LLMs). It integrates a comprehensive set of features including a systematic and reusable library of operators, data recipes, and toolkits, supported by detailed data analysis and visualization capabilities. The system is optimized for efficiency, using less memory and CPU resources, and offers flexibility in handling various data formats. Data-Juicer supports community involvement through contributing to data development, research, and an array of dedicated communication platforms. The tool has been utilized across various products and research initiatives, underlining its practical application and contribution to the field of artificial intelligence and machine learning. Data-Juicer is openly developed and encourages the community to contribute to its expansion and refinement.",Revolutionizing LLM Data Processing with Data-Juicer,"Discover how Data-Juicer, a multimodal data processing system, transforms raw data into high-quality, comprehensive data for Large Language Models (LLMs). With its active updates, including DJ-SORA, and a community-driven approach, Data-Juicer invites users to contribute to the future of LLM data development and research. Join our collaborative platform to enhance LLM data processing techniques and contribute to the advancement of machine learning technologies.","Learn about Data-Juicer, an all-encompassing, multimodal data processing system designed to upgrade data for LLMs. Explore its features, community contributions, and how you can join the movement towards superior data processing.",Data Transformation Tool,"Python





        1,260





        68


        Built by

          









        7 stars today",https://img.alicdn.com/imgextra/i3/O1CN017Eq5kf27AlA2NUKef_!!6000000007757-0-tps-1280-720.jpg; https://img.alicdn.com/imgextra/i3/O1CN01QbwHJa1EV5uZwmU9c_!!6000000000356-2-tps-400-400.png; https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png; https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png; https://img.alicdn.com/imgextra/i4/O1CN01kUiDtl1HVxN6G56vN_!!6000000000764-2-tps-43-19.png; https://img.alicdn.com/imgextra/i2/O1CN01IMPeD11xYRUYLmXKO_!!6000000006455-2-tps-3620-1604.png; https://img.alicdn.com/imgextra/i1/O1CN011E99C01ndLZ55iCUS_!!6000000005112-0-tps-2701-1050.jpg; https://img.alicdn.com/imgextra/i2/O1CN019WtUPP1uhebnDlPR8_!!6000000006069-2-tps-2530-1005.png; https://img.alicdn.com/imgextra/i4/O1CN01Sk0q2U1hdRxbnQXFg_!!6000000004300-0-tps-2438-709.jpg,,1260,2023-08-01T09:16:41Z
2024-03-10,https://github.com/OpenAccess-AI-Collective/axolotl,https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/README.md,"Axolotl is an advanced tool designed for efficiently fine-tuning multiple AI models, supporting various configurations and architectures. It facilitates training of diverse Huggingface models, including llama, pythia, falcon, and mpt, and allows for extensive customization through simple YAML or CLI commands. The tool can handle different dataset formats and is equipped to work with advanced features like xformer, flash attention, and rope scaling. It supports single and multiple GPU setups through FSDP or Deepspeed and can be run locally or in the cloud with Docker. Additionally, Axolotl integrates with wandb or mlflow for logging results and checkpoints. It offers a wide range of configurations for fine-tuning, including support for fp16/fp32, various fine-tuning techniques like lora, qlora, and gptq, amongst others, ensuring compatibility with a broad spectrum of AI models. Quickstart guides, detailed documentation, and advanced setup options ensure users can effectively harness the tool for their AI development projects.",Streamlining AI Model Training: Introducing Axolotl,"Axolotl revolutionizes AI model fine-tuning with multi-architecture support, allowing customization via yaml or CLI. With compatibility for various Huggingface models and integration for multiple GPUs, it simplifies processes with Docker and logs results efficiently. Its versatility extends to diverse dataset formats and optimization techniques, enhancing productivity and performance in AI development environments.","Discover how Axolotl streamlines AI model fine-tuning across multiple architectures, offering advanced features like multiple GPU support, diverse dataset compatibility, and efficient logging. Get started with Axolotl today.",Deep Learning Platform,"Python





        4,745





        521


        Built by

          









        32 stars today",https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl.png; https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png; https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png,,4745,2023-04-14T04:25:47Z
2024-03-10,https://github.com/Vahe1994/AQLM,https://raw.githubusercontent.com/Vahe1994/AQLM/main/README.md,"The AQLM project offers a PyTorch implementation for compressing large language models through Additive Quantization, significantly reducing model size without compromising performance. It supports models from the LLaMA, Mistral, and Mixtral families and provides Google Colab examples for basic operations, streaming, and fine-tuning. Prequantized models are offered with varying arrangements that balance size, performance, and compatibility across different hardware setups for optimal inference speed and efficiency. Installation requires an inference library, and the project comes with detailed instructions for quantization, including dependencies, data processing, and model fine-tuning guidelines. Quantization can take considerable time but can be sped up with multiple GPUs. The project also includes zero-shot benchmarking via LM Evaluation Harness and details for converting quantized models into Hugging Face compatible formats. Contributions are welcome, following specific guidelines on code formatting and submission.",Revolutionize Large Language Model Compression with AQLM,"Discover the groundbreaking method to compress large language models using the Additive Quantization approach. AQLM offers a practical solution for reducing model sizes significantly while maintaining performance, as demonstrated in the official PyTorch implementation. Learn how to implement AQLM across various platforms through concise Google Colab examples, covering basic generation to fine-tuning. Dive into the capabilities of prequantized models across the LLaMA, Mistral, and Mixtral families, and explore the optimized inference kernels for both GPU and CPU that promise accuracy and speedup. Join the forefront of AI technology compression by accessing the full implementation details and supportive resources.","Learn about AQLM's revolutionary approach to compressing large language models with additive quantization. Explore the official PyTorch implementation, example demos, model capabilities, and optimized inference kernels for efficient AI compression.",Deep Learning,"Python





        442





        55


        Built by

          









        18 stars today",,,442,2024-01-12T07:51:15Z
2024-03-10,https://github.com/hatchet-dev/hatchet,https://raw.githubusercontent.com/hatchet-dev/hatchet/main/README.md,"Hatchet is presented as a state-of-the-art, distributed, and fault-tolerant task queue designed to replace cumbersome legacy systems and simplify the management of durable workloads with emphasis on concurrency, fairness, and rate limiting. It enables function distributions across a set of workers with minimal configuration, boasting features like ultra-low latency, high throughput scheduling, and built-in strategies for queue management. Hatchet is resilient by design, offering customizable retry policies and integrated error handling to swiftly recover from failures. It also provides enhanced control and visibility through features like observability, durable execution, cron, one-time scheduling, spike protection, and incremental streaming. Hatchet targets various use cases, including fairness for Generative AI, batch processing for document indexing, workflow orchestration for multi-modal systems, and correctness for event-based processing. It supports SDKs for Python, Typescript, and Go, offering quickstart guides for easy integration. Hatchet distinguishes itself from alternatives like Celery and BullMQ by leveraging Postgres for full transactional enqueueing and offering built-in task observability, aiming for ease of use and reliability. Contributions are encouraged, with a community-supported model facilitated through Discord and GitHub.",Revolutionize Your Workflow with Hatchet: The Ultimate Task Queue Solution,"Discover how Hatchet revolutionizes distributed, fault-tolerant task queues, offering unmatched latency, throughput, and flexibility for managing workloads. With features like concurrency control, rate limiting, and enhanced observability, Hatchet simplifies complex tasks across various use cases from AI fairness to workflow orchestration. Its resilience and customizable retry policies ensure your operations swiftly recover from failures, while support for multiple SDKs ensures seamless integration into your technology stack. Dive into Hatchet's capabilities and learn how it stands out from traditional queueing solutions.","Explore Hatchet, the distributed task queue designed for ultra-low latency, high throughput, and fault tolerance. Learn how it simplifies workflows with features like concurrency control and enhanced observability.",Open Source Tool,"Python





        1,895





        53


        Built by

          







        314 stars today",,,1895,2023-12-15T17:50:29Z
2024-03-10,https://github.com/ml-explore/mlx-examples,https://raw.githubusercontent.com/ml-explore/mlx-examples/main/README.md,"The MLX Examples repo provides a range of examples utilizing the MLX framework, targeting learners and developers in machine learning. It offers a starting point with the MNIST example for newcomers. Advanced use cases involve text models like Transformer training, large-scale text generation through LLaMA, Mistral, Phi-2, and a mixture-of-experts model Mixtral 8x7B, among others. In the realm of image processing, it features ResNets for image classification, Stable Diffusion for image generation, and CVAE on MNIST. Audio models include speech recognition via OpenAI's Whisper. Multimodal and other models cover joint embeddings, semi-supervised learning, and real NVP for density estimation. The repo also integrates with Hugging Face, allowing for downloading of specific models. Contributors are acknowledged and encouraged, supported by a BibTex citation for academic use, highlighting the collaborative effort behind MLX Examples by Awni Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert.",Exploring the Versatile World of MLX Framework with Practical Examples,"Discover the power of the MLX framework through a collection of standalone examples for various machine learning models. Starting with the MNIST example, learners can quickly grasp how to leverage MLX for their projects. Dive into advanced text, image, audio, and multimodal models, including Transformer language models, Image classification with ResNets, and speech recognition with Whisper. Join the MLX Community on Hugging Face to access additional resources and contribute new models. Explore, contribute, and cite the MLX Examples to bolster your machine learning research and projects.","Unlock the full potential of the MLX framework with our comprehensive guide on standalone examples, covering text, image, audio, and multimodal models. Perfect for beginners and advanced users.",Machine Learning,"Python





        4,319





        607


        Built by

          









        13 stars today",,,4319,2023-11-28T23:37:49Z
2024-03-10,https://github.com/commaai/openpilot,https://raw.githubusercontent.com/commaai/openpilot/master/README.md,"Openpilot is an open-source driver assistance system developed by comma.ai, offering features like Adaptive Cruise Control (ACC), Automated Lane Centering (ALC), Forward Collision Warning (FCW), Lane Departure Warning (LDW), and a Driver Monitoring (DM) system that alerts distracted or sleeping drivers. It supports over 250 car models and requires a comma device like the comma 3/3X for operation, along with proper car harnessing and software installation. Openpilot's development is community-driven, supported by contributions on GitHub, where enthusiasts can join discussions, contribute to the codebase, or explore development tools. The system adheres to ISO26262 guidelines, undergoes rigorous safety testing, and regularly updates its safety models. User data is uploaded by default to improve the system but can be opted out of. Openpilot is released under the MIT license, emphasizing that users must comply with local laws and acknowledge the absence of warranty and the software's alpha status for research purposes only.",Exploring openpilot: The Future of Driver Assistance Technology,"Discover openpilot, an innovative open source driver assistance system revolutionizing the driving experience. Offering features like Adaptive Cruise Control, Automated Lane Centering, and a Driver Monitoring feature, openpilot brings advanced vehicle safety and convenience to a wide range of compatible cars. Its development by comma and the community highlights a commitment to cutting-edge automotive technologies. Learn how openpilot is setting the standard for open-source driving solutions and how you can start using it today.","Explore openpilot, an open source driver assistance system transforming road safety and convenience with features like Adaptive Cruise Control, Automated Lane Centering, and more. Learn how to enhance your driving experience with openpilot.",Open Source Tool,"Python





        46,894





        8,376


        Built by

          









        18 stars today",https://cdn-images-1.medium.com/max/1600/1*C87EjxGeMPrkTuVRVWVg4w.png,https://www.youtube.com/watch?v=NmBfgOanCyk; https://www.youtube.com/watch?v=VHKyqZ7t8Gw; https://www.youtube.com/watch?v=SUIZYzxtMQs,46894,2016-11-24T01:33:30Z
2024-03-10,https://github.com/llmware-ai/llmware,https://raw.githubusercontent.com/llmware-ai/llmware/main/README.md,"LLMWare is a versatile toolkit designed for building applications using large language models (LLMs), offering quick POC development to scalable enterprise solutions. It integrates over 50 models from Hugging Face, supporting applications like Retrieval Augmented Generation (RAG) and Multi-Step Orchestration of Agent Workflows. The framework caters to all skill levels, facilitating the integration of open-source specialized models for creating sophisticated, knowledge-based applications. Key features include a model catalog for easy access to a diverse model range, a library system for organizing large knowledge collections, and query functionalities for mixed-data interrogation. Additionally, it offers simple database options for scalability and supports various models, including the SLIM series for multi-step agent workflows and the DRAGON series for robust RAG applications. LLMWare is constantly updated, aiming to simplify the deployment of fine-tuned models and maintain privacy and security in cloud environments.",Harness the Power of LLMWare: Your One-Stop Toolkit for LLM Application Development,"Discover LLMWare, the integrated framework revolutionizing LLM application development with over 50+ models from Hugging Face. Ideal for both beginners and advanced AI developers, LLMWare facilitates the rapid creation of industrial-grade, knowledge-based enterprise applications. Its focus on easy integration of open source, specialized models ensures secure, seamless connection of enterprise knowledge. Explore the endless possibilities with LLMWare and join the community to transform your LLM application projects.","Explore LLMWare, the comprehensive toolkit for developing LLM applications, featuring over 50 models from Hugging Face, easy integration, and community support.",Collaborative AI Framework,"Python





        2,791





        232


        Built by

          









        34 stars today",,https://www.youtube.com/watch?v=cQfdaTcmBpY; https://www.youtube.com/watch?v=cQfdaTcmBpY; https://www.youtube.com/watch?v=ZJyQIZNJ45E; https://www.youtube.com/watch?v=JjgqOZ2v5oU; https://www.youtube.com/watch?v=Bncvggy6m5Q; https://www.youtube.com/watch?v=BI1RlaIJcsc; https://www.youtube.com/watch?v=8aV5p3tErP0; https://www.youtube.com/watch?v=Cf-07GBZT68; https://www.youtube.com/watch?v=h2FDjUyvsKE; https://www.youtube.com/watch?v=cQfdaTcmBpY; https://www.youtube.com/watch?v=JjgqOZ2v5oU; https://www.youtube.com/watch?v=d_u7VaKu6Qk; https://www.youtube.com/watch?v=Bncvggy6m5Q; https://www.youtube.com/watch?v=ZJyQIZNJ45E; https://www.youtube.com/watch?v=h2FDjUyvsKE; https://www.youtube.com/watch?v=qITahpVDuV0; https://www.youtube.com/watch?v=9wXJgld7Yow; https://www.youtube.com/watch?v=8aV5p3tErP0; https://www.youtube.com/watch?v=VHZSaBBG-Bo; https://www.youtube.com/watch?v=O0adUfrrxi8; https://www.youtube.com/watch?v=s0KWqYg5Buk; https://www.youtube.com/watch?v=0naqpH93eEU; https://www.youtube.com/watch?v=tAGz6yR14lw; https://www.youtube.com/watch?v=qiEmLnSRDUA,2791,2023-09-29T15:19:06Z
2024-03-11,https://github.com/Bing-su/adetailer,https://raw.githubusercontent.com/Bing-su/adetailer/main/README.md,"ADetailer is an extension designed for the stable diffusion webui, utilizing ultralytics for enhanced object detection, unlike Detection Detailer which uses mmdet. It simplifies the process of installing directly from the Extensions tab without the need to download a base model from huggingface. The installation process involves adding the extension's URL, installing it, and then restarting the webui. ADetailer offers various options for customization, including selecting models to determine detection specifics, adjusting detection confidence thresholds, and configuring mask preprocessing and inpainting settings. It also supports ControlNet Inpainting with different models for various inpainting controls. Advanced options for API requests and configuration adjustments are available, alongside instructional media and a description of the supported models, which include several YOLO models for detecting faces, hands, and people with high accuracy. Users can also add custom ultralytics YOLO models. The extension works by creating an image, detecting objects to create a mask, and then inpainting based on the mask and original image.",Enhance Your AI Art: A Comprehensive Guide to Installing and Using ADetailer for Stable Diffusion,"Discover the power of ADetailer, an essential extension for stable diffusion webui, aimed at refining your AI-generated art by emphasizing precision in object detection. Learn how to effortlessly install ADetailer using a straightforward URL from the Extensions tab and navigate through its extensive options and advanced features. This extension allows for detailed adjustments and enhancements, ensuring your AI artwork stands out. Whether you're looking to improve object detection or explore inpainting options, ADetailer offers comprehensive solutions catered to creators and developers alike. Elevate your stable diffusion projects today with ADetailer's innovative capabilities.","Learn how to install and utilize ADetailer, a powerful extension for stable diffusion webui that elevates your AI-generated art through precise object detection and inpainting options. Enhance your creations with advanced features and detailed adjustments.",Computer Vision Platform,"Python





        3,469





        268


        Built by

          









        6 stars today",https://i.imgur.com/g6GdRBT.png,https://www.youtube.com/watch?v=sF3POwPUWCE; https://www.youtube.com/watch?v=urNISRdbIEg,3469,2023-04-26T07:54:51Z
2024-03-11,https://github.com/TheAlgorithms/Python,https://raw.githubusercontent.com/TheAlgorithms/Python/master/README.md,"The Algorithms - Python is a GitHub repository showcasing various algorithms implemented in Python, aiming for educational purposes. The project encourages contributions, welcoming those interested to first read through their Contribution Guidelines. Despite being primarily for learning, the implementations may not be as efficient as those in the Python standard library, urging users to exercise discretion. Additionally, it offers community channels on Discord and Gitter for discussions, questions, and help. The project also provides a directory for easier navigation and an overview of the available algorithms. Support for Gitpod and code style enforcement using pre-commit and black is highlighted through badges.",Master Python Algorithms Easily: A Beginner's Guide,"Dive into the world of Python algorithms with 'The Algorithms - Python', a comprehensive resource for learning and mastering algorithms in Python. Ideal for education, these implementations are designed for understanding rather than efficiency, making it perfect for beginners and students. Before contributing, make sure to check out the Contribution Guidelines. Join our vibrant community on Discord and Gitter to ask questions and get help. Explore our directory for an organized overview of all available algorithms.","Discover 'The Algorithms - Python', your ultimate guide to learning and mastering Python algorithms for educational purposes. Join our community and start enhancing your coding skills today!",Python Learning Journey,"Python





        176,430





        43,044


        Built by

          









        29 stars today",,,176430,2016-07-16T09:44:01Z
2024-03-11,https://github.com/Yuukiy/JavSP,https://raw.githubusercontent.com/Yuukiy/JavSP/master/README.md,"Jav Scraper Package (JavSP) is a tool designed for automatically scraping and assembling adult video (AV) metadata from multiple websites. It identifies the unique codes from video filenames, generates metadata files compatible with media server software like Emby, Jellyfin, and Kodi, and organizes video files based on specified rules. However, the project currently lacks Docker support and a graphical user interface (UI), directing users interested in these features to [@tetato/JavSP-Docker](https://github.com/tetato/JavSP-Docker). Presently, the project only offers support in Chinese, with a possibility for adding more languages based on user votes. Key functionalities include AV code recognition, handling split video files, daily automated checks of site crawlers, multithreaded scraping, downloading high-resolution covers, AI-based unconventional cover cropping, auto-updates, and title/plot translation. Installation is straightforward with options for direct software download or building from source with Python 3.8. The project is open for contributions in various formsâ€”from code and documentation to testing and genre translationâ€”welcoming community engagement. JavSP is governed by GPL-3.0 and Anti 996 Licenses, emphasizing non-commercial use, compliance with local laws, and discouragement of promotion on restricted social platforms.",Maximize Your AV Collection: The Ultimate Jav Scraper Package Guide,"Discover the power of Jav Scraper Package (JavSP), the most comprehensive AV metadata scraper collecting data from multiple sites. Tailor-made for Emby, Jellyfin, Kodi users, this tool automates the extraction of movie filenames codes, categorizes files according to preset rules, and generates metadata files without needing Docker or a WebUI. While currently supporting only Chinese, the project is open for language expansion through community votes. Dive into the world of efficient AV collection management with JavSP, strictly for educational and technical exchange purposes.","Explore JavSP, the go-to AV metadata scraper for Emby, Jellyfin, Kodi users. Automate your AV collection organization with file categorization, metadata generation, and more. Strictly for non-commercial use.",Document Conversion Tool,"Python





        1,587





        146


        Built by

          









        25 stars today",,,1587,2021-01-03T07:42:23Z
2024-03-11,https://github.com/serengil/deepface,https://raw.githubusercontent.com/serengil/deepface/master/README.md,"Deepface is a comprehensive Python framework for face recognition and analysis, covering attributes such as age, gender, emotion, and race. It encapsulates several state-of-the-art models like VGG-Face, Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, and Dlib. These models have surpassed human-level accuracy in facial recognition tasks. Deepface simplifies the face recognition process into a single line of code for various functionalities, including verification, search, and analysis, without needing deep technical knowledge of the underlying processes. It supports multiple face recognition models, offering flexibility in choosing the most suitable one for specific applications. Additionally, Deepface provides a robust facial attribute analysis module for predicting age, gender, emotions, and race. It also wraps around various face detectors, allowing for easy swapping among them based on preference for accuracy or speed. Installation is straightforward, available via PyPI, Conda, or directly from the source. Deepface also features real-time analysis capabilities, a command-line interface, an API, and Docker support for deployment. Furthermore, it is open-source under the MIT license, with an emphasis on community contributions and support.",Unlocking the Power of Deepface: A Comprehensive Guide to Advanced Face Recognition,"Deepface is transforming the world of face recognition and facial attribute analysis with its lightweight Python framework. This comprehensive tool harnesses the capabilities of state-of-the-art models like VGG-Face, Google FaceNet, and more, to exceed human-level accuracy in facial recognition tasks. From simple installation steps to a wide range of functionalities including verification, find, analysis, and real-time video analysis, Deepface proves to be invaluable for developers and researchers. Moreover, it offers extensive documentation, command line interface, and Docker support, ensuring an accessible and versatile platform for all facial recognition needs.","Explore the capabilities of Deepface, a leading Python framework for face recognition and facial attribute analysis. Learn about its easy installation, versatile functionalities including verification and analysis, and how it leverages state-of-the-art models to offer superior accuracy.",Computer Vision Platform,"Python





        9,490





        1,743


        Built by

          









        17 stars today",https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-icon-labeled.png; https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-1.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/embedding.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/model-portfolio-v8.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-portfolio-v5.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-outputs-20240302.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/retinaface-results.jpeg; https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-dockerized-v2.jpg; https://raw.githubusercontent.com/serengil/deepface/master/icon/patreon.png,https://www.youtube.com/watch?v=WnUVYQP4h44; https://www.youtube.com/watch?v=KRCvkNCOphE; https://www.youtube.com/watch?v=Hrjp-EStM_s; https://www.youtube.com/watch?v=i_MOwvhbLdI; https://www.youtube.com/watch?v=i_MOwvhbLdI; https://www.youtube.com/watch?v=i_MOwvhbLdI; https://www.youtube.com/watch?v=GT2UeN85BdA; https://www.youtube.com/watch?v=GZ2p2hj2H5k; https://www.youtube.com/watch?v=-c9sSJcx6wI; https://www.youtube.com/watch?v=HeKCQ6U9XmI; https://www.youtube.com/watch?v=PKKTAr3ts2s,9490,2020-02-08T20:42:28Z
2024-03-11,https://github.com/vwxyzjn/cleanrl,https://raw.githubusercontent.com/vwxyzjn/cleanrl/master/README.md,"CleanRL is a Deep Reinforcement Learning (DRL) library known for its high-quality, single-file implementations of various DRL algorithms. It's distinguished by its research-friendly features, such as detailed single-file algorithms, benchmarks for over 7 algorithms and 34 games, Tensorboard logging, local reproducibility via seeding, video capture of gameplay, and experiment management with integrations like Weights and Biases, Docker, and AWS. CleanRL is designed with accessibility and simplicity in mind, ensuring that even complex DRL algorithms are understandable for those who prefer not reading through extensive library modules. Additionally, it supports online DRL algorithms exclusively and is migrating to Gymnasium from `openai/gym`. Despite its non-modular nature limiting reuse without modification, CleanRL excels in offering an easy-to-debug framework that's conducive to both learning and rapid prototyping of DRL algorithms. The project also participates in the Open RL Benchmark, promoting transparency in experimental DRL data. CleanRL encourages community engagement through Discord and GitHub, and appreciates contributions and support from various individual and organizational contributors.",Mastering Deep Reinforcement Learning with CleanRL: A Comprehensive Guide,"Discover CleanRL, the go-to library for implementing deep reinforcement learning algorithms seamlessly. With its single-file approach for algorithm variants, CleanRL offers an easy-to-understand, research-friendly platform for deep RL enthusiasts. It is designed to scale experiments on AWS and integrates smoothly with tools like Tensorboard and Weights and Biases for enhanced experiment management. Whether you're a beginner aiming to grasp algorithmic intricacies or a researcher prototyping new features, CleanRL's minimalistic codebase is tailored for efficient learning and development.",Explore CleanRL for a simplified yet powerful approach to deep reinforcement learning. Learn how its single-file implementations and cloud integration can accelerate your RL projects and research.,Deep Learning Platform,"Python





        4,185





        501


        Built by

          









        7 stars today",https://raw.githubusercontent.com/vwxyzjn/cleanrl/master/docs/static/o1.png; https://raw.githubusercontent.com/vwxyzjn/cleanrl/master/docs/static/o2.png; https://raw.githubusercontent.com/vwxyzjn/cleanrl/master/docs/static/o3.png,https://www.youtube.com/watch?v=dm4HdGujpPs,4185,2019-06-07T16:31:50Z
2024-03-11,https://github.com/paperless-ngx/paperless-ngx,https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/README.md,"Paperless-ngx is a document management system aimed at reducing physical paper use by transforming documents into a searchable online archive. It's the successor to Paperless and Paperless-ng, encouraging community involvement in its development. A demo, supported by DigitalOcean, showcases its capabilities, emphasizing the need for caution with sensitive information due to the lack of encryption. It features a straightforward setup using docker compose, and offers detailed documentation for installation and migration from Paperless-ng. Community contributions are welcomed, including bug fixes, enhancements, and translations facilitated through Crowdin. It supports multiple languages and allows for feature suggestions and bug reporting via GitHub. The documentation advises running Paperless-ngx on a trusted local server to maintain security, highlighting its integration with affiliated projects for enhanced compatibility.",Transform Your Documents with Paperless-ngx: The Future of Document Management,"Explore Paperless-ngx, the innovative successor to Paperless and Paperless-ng projects, designed for efficient document management. With its easy-to-use features, this system turns your physical documents into a searchable online archive, minimizing paper usage while maximizing accessibility. Supported by DigitalOcean, it offers a demo for users to experience its capabilities firsthand. Join the community movement to enhance and support Paperless-ngx, contributing to a sustainable, paperless future.","Discover Paperless-ngx, the ultimate document management system turning physical papers into a secure, searchable online archive. Experience the future of minimalistic documentation.",Open Source Tool,"Python





        15,926





        803


        Built by

          









        19 stars today",https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png; https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png,,15926,2022-02-12T21:56:52Z
2024-03-11,https://github.com/blakeblackshear/frigate,https://raw.githubusercontent.com/blakeblackshear/frigate/master/README.md,"Frigate is a comprehensive, local NVR system integrated with AI for real-time object detection, tailored for use with Home Assistant. It employs OpenCV and Tensorflow for local, real-time IP camera monitoring. Utilizing a Google Coral Accelerator, which significantly enhances performance allowing for processing of 100+ FPS with minimal overhead, is optional but recommended. The system is designed for efficiency, operating on a need-to-detect basis while minimizing resource usage. It features tight Home Assistant integration, multiprocessing for real-time performance, low-overhead motion detection, MQTT communication for broader system integration, video recording with object-based retention, continuous recording, re-streaming, and low-latency live view via WebRTC & MSE. Visit https://docs.frigate.video for more information and support development via Github Sponsors.",Maximizing Home Security: The Power of Frigate NVR with Realtime Object Detection,"Discover the future of home security with Frigate, an advanced NVR system integrating real-time object detection for IP cameras, designed for seamless use with Home Assistant. Leverage the power of OpenCV and TensorFlow for local, efficient object detection, and enhance performance with an optional Google Coral Accelerator. Experience smart home integration, minimal resource usage, and high-performance security tailored to your needs. With support for 24/7 recording, low-latency live view, and easy integration into other systems, Frigate redefines home security.","Explore Frigate NVR: A cutting-edge home security system with real-time AI object detection for IP cameras. Optimized for Home Assistant, it offers high performance, smart integration, and optional Google Coral Accelerator support.",Cybersecurity Tool,"Python





        13,991





        1,289


        Built by

          









        13 stars today",https://raw.githubusercontent.com/blakeblackshear/frigate/master/docs/static/img/frigate.png; https://raw.githubusercontent.com/blakeblackshear/frigate/master/docs/static/img/media_browser.png; https://raw.githubusercontent.com/blakeblackshear/frigate/master/docs/static/img/notification.png; https://raw.githubusercontent.com/blakeblackshear/frigate/master/docs/static/img/home-ui.png; https://raw.githubusercontent.com/blakeblackshear/frigate/master/docs/static/img/camera-ui.png; https://raw.githubusercontent.com/blakeblackshear/frigate/master/docs/static/img/events-ui.png,,13992,2019-01-26T13:52:38Z
2024-03-11,https://github.com/Mikubill/sd-webui-controlnet,https://raw.githubusercontent.com/Mikubill/sd-webui-controlnet/main/README.md,"The ControlNet extension is designed for the Stable Diffusion web UI by AUTOMATIC1111, allowing users to integrate ControlNet's image generation capabilities directly within the web interface, enhancing the original model without the need for merging. To install, users follow steps including using the ""Extensions"" tab to install from a provided URL, restarting the WebUI, downloading and correctly placing model files from ControlNet 1.1 on HuggingFace, and ensuring the model files' compatibility. The extension supports various features such as perfect support for available models, high-resolution fixes, comprehensive mask support, and a ""Pixel-Perfect"" mode. It also introduces a user-friendly GUI, supports nearly all upscaling scripts, and introduces more control modes to balance between prompts and ControlNet's influence. Additionally, a new 'reference-only' preprocessor is introduced for direct image reference guidance. ControlNet 1.1 also supports multi-ControlNet inputs, customizable control weight/start/end for detailed control mechanism, and batch mode for bulk image generation. Additionally, the extension is accessible via API or scripts for automated tasks and adds specific command line arguments for advanced configurations. MacOS support is available with specific conditions, and an archived version of the extension is maintained for those requiring exact reproduction of past results.",Enhance Your Stable Diffusion Experience with ControlNet WebUI Extension,"Discover how the ControlNet WebUI extension revolutionizes image generation in Stable Diffusion by introducing on-the-fly integration of ControlNet and other SD controls for enhanced, precise image creation. From installation to leveraging advanced features like 'Pixel-Perfect' Mode and multi-ControlNet inputs, this guide covers everything you need to boost your creative workflow. Learn about downloading models, setting up, and optimizing your experience for unparalleled image generation capabilities.","Learn how to seamlessly integrate ControlNet with Stable Diffusion web UI for advanced image generation. Get step-by-step guidance on installation, model downloads, and leveraging new features for enhanced creativity.",Image Generation Platform,"Python





        15,405





        1,807


        Built by

          









        17 stars today",https://user-images.githubusercontent.com/19834515/236641759-6c44ddf6-c7ad-4bda-92be-e90a52911d75.png; https://raw.githubusercontent.com/Mikubill/sd-webui-controlnet/main/samples/cm1.png; https://raw.githubusercontent.com/Mikubill/sd-webui-controlnet/main/samples/cm2.png; https://raw.githubusercontent.com/Mikubill/sd-webui-controlnet/main/samples/cm3.png; https://raw.githubusercontent.com/Mikubill/sd-webui-controlnet/main/samples/cm4.png; https://raw.githubusercontent.com/Mikubill/sd-webui-controlnet/main/samples/ref.png; https://user-images.githubusercontent.com/19834515/235620638-17937171-8ac1-45bc-a3cb-3aebf605b4ef.png; https://user-images.githubusercontent.com/31246794/222947416-ec9e52a4-a1d0-48d8-bb81-736bf636145e.jpeg; https://user-images.githubusercontent.com/31246794/222947435-1164e7d8-d857-42f9-ab10-2d4a4b25f33a.png; https://user-images.githubusercontent.com/31246794/222947557-5520d5f8-88b4-474d-a576-5c9cd3acac3a.png; https://user-images.githubusercontent.com/31246794/222947416-ec9e52a4-a1d0-48d8-bb81-736bf636145e.jpeg; https://user-images.githubusercontent.com/31246794/222965711-7b884c9e-7095-45cb-a91c-e50d296ba3a2.png; https://user-images.githubusercontent.com/31246794/220448620-cd3ede92-8d3f-43d5-b771-32dd8417618f.png; https://user-images.githubusercontent.com/31246794/220448619-beed9bdb-f6bb-41c2-a7df-aa3ef1f653c5.png; https://user-images.githubusercontent.com/31246794/220448613-c99a9e04-0450-40fd-bc73-a9122cefaa2c.png,,15405,2023-02-12T16:26:27Z
2024-03-11,https://github.com/datacontract/cli,https://raw.githubusercontent.com/datacontract/cli/main/README.md,"The Data Contract CLI is a versatile open-source command-line tool designed for managing Data Contracts through YAML files. It facilitates various operations such as linting data contracts, connecting to different data sources, executing schema and quality tests, detecting changes, and exporting data models to multiple formats. Developed in Python, it can be utilized directly, integrated into CI/CD pipelines, or used as a Python library. Installation options include pip, pipx, and Docker, supporting various environments and preferences. The CLI efficiently tests data contracts against actual datasets located in storage options like S3 buckets, ensuring compliance with predefined schemas and quality specifications. Furthermore, it supports programmatic use within Python, integration with Data Mesh Manager for publishing test results, and offers comprehensive documentation for installation, usage, and development setup. Import and export functionalities allow conversion of data contracts to and from different formats, facilitating interoperability and flexible data management practices.",Master Data Contract CLI: Your Ultimate Guide to Seamless Data Management,"Discover how Data Contract CLI revolutionizes data management, offering seamless integration with data sources and rigorous data quality checks. This Python-based command-line tool ensures your data contracts are lint-free, connects to various data sources, and performs comprehensive schema and quality tests. Ideal for both standalone use and integration within CI/CD pipelines, Data Contract CLI simplifies executing, exporting, and validating data contracts, making quality control more accessible than ever.","Learn how Data Contract CLI streamlines data management through linting, integration with data sources, and executing schema and quality tests. Perfect for developers and data professionals seeking efficient data contract validation and execution.",Data Transformation Tool,"Python





        182





        21


        Built by

          









        3 stars today",https://raw.githubusercontent.com/datacontract/cli/main/datacontractcli.png,,182,2023-07-24T14:54:16Z
2024-03-11,https://github.com/RsaCtfTool/RsaCtfTool,https://raw.githubusercontent.com/RsaCtfTool/RsaCtfTool/master/README.md,"The RsaCtfTool is a comprehensive tool designed for decrypting data encrypted with weak RSA public keys and recovering the corresponding private keys. It integrates multiple integer factorization algorithms to enhance decryption capabilities. Primarily aimed at educational use, it underscores the importance of understanding the mathematical foundations of RSA, including number theory and integer factorization. The tool supports various attacks, including both factorization-dependent and independent methods, offering a broad spectrum of strategies for cracking RSA encryption. Users are encouraged to interact with the source code, understanding its workings and contributing improvements. It facilitates various operations such as public key creation, decryption of files, and private key recovery among others, emphasizing responsible use within legal and ethical boundaries. Installation instructions are provided for different operating systems including Ubuntu, Fedora, and MacOS. The community is invited to contribute to its development, adhering to the provided guidelines and code of conduct.",Unlocking RSA Encryption: Exploring the RsaCtfTool for Educational Use,"Discover the capabilities of the RsaCtfTool, a comprehensive utility for deciphering weak RSA public keys. This tool combines various algorithms to tackle integer factorization, enhancing decryption efforts. It's designed primarily for educational purposes, helping users understand RSA encryption's complexities through practical application. Despite its limitations, such as supporting only semiprime composite moduli, the RsaCtfTool remains a valuable resource for those keen on cryptography.","Explore the RsaCtfTool, a multifaceted utility aimed at decrypting weak RSA public keys through a suite of integer factorization algorithms. Ideal for educational purposes, it emphasizes the practical understanding of RSA encryption.",Cybersecurity Tool,"Python





        5,120





        872


        Built by

          









        5 stars today",,,5120,2015-03-07T17:29:33Z
2024-03-11,https://github.com/01-ai/Yi,https://raw.githubusercontent.com/01-ai/Yi/main/README.md,"This text introduces the Yi series, a next-generation open-source and bilingual Large Language Models (LLMs) developed by 01.AI, trained on a 3T multilingual corpus. The Yi models, including the Yi-34B-Chat and Yi-34B models, have shown exceptional performance, ranking highly on various AI evaluation leaderboards in both English and Chinese. These models are designed for diverse applications, suitable for personal, academic, and commercial use. Contributions from the Transformer and Llama communities have significantly facilitated Yi's development, emphasizing that these models are not derivatives of Llama but rather benefit from the shared architecture. The text outlines the availability of different Yi models for chat and base applications, instructions for quick start deployment through several methods like pip, docker, and conda-lock, along with options for fine-tuning, quantization, and deployment. Furthermore, it highlights the role of the Yi Learning Hub for educational resources and community engagement through GitHub discussions, Discord, and WeChat.",Revolutionizing Language Understanding: The Emergence of Yi's Bilingual LLMs,"Discover how the Yi series models are pioneering the next frontier in AI with unmatched bilingual language understanding and versatility in applications. These next-generation, open-source large language models, trained on a diverse multilingual corpus, not only clinch top positions on global leaderboards but also usher in a new era of AI accessibility and collaborative innovation. With a robust architecture inspired by Transformer and Llama, Yi models have set new standards in the realms of commonsense reasoning, reading comprehension, and more.","Explore how Yi series models are setting new benchmarks in bilingual language understanding by leveraging a 3T multilingual corpus and robust architecture, outperforming notable LLMs across diverse applications.",Language Models,"Python





        6,429





        384


        Built by

          









        38 stars today",,https://www.youtube.com/watch?v=NJ89T5mO25Y; https://www.youtube.com/watch?v=CVQvj4Wrh4w; https://www.youtube.com/watch?v=On3Zuv27V3k,6429,2023-11-03T16:08:37Z
2024-03-11,https://github.com/magic-research/piecewise-rectified-flow,https://raw.githubusercontent.com/magic-research/piecewise-rectified-flow/main/README.md,"PeRFlow, or Piecewise Rectified Flow, is introduced as a method to enhance the speed and efficiency of generating high-quality images using pre-trained diffusion models, specifically improving upon the limitations of Stable Diffusion models. By learning a piecewise linear probability flow, PeRFlow can generate images in significantly fewer steps - as little as 4, compared to traditional methods. This not only speeds up the generation process but also ensures the images are of high fidelity and diversity. Training PeRFlow is efficient, requiring only 4,000 iterations for fine-tuning, and it seamlessly integrates with existing SD-based workflows, supporting various enhancements such as classifier-free guidance for higher quality generation. PeRFlow's application extends to fast image and text-to-image generation, image enhancement, and efficient multiview generation, demonstrating its versatility as a plug-and-play accelerator module for a wide range of image generation tasks. The project has garnered contributions from several institutions and offers demos, training scripts, and model downloads to encourage further exploration and application in the field.",Exploring PeRFlow: A Revolution in Accelerating Diffusion Models for Spectacular Image Generation,"Discover the groundbreaking PeRFlow model, a piecewise rectified flow technique, enhancing the efficiency of pre-trained diffusion models for faster, high-quality image generation. PeRFlow, developed by a collaborative research team, demonstrates remarkable advancements in image synthesis, including fast generation, efficient training, and compatibility with existing SD workflows. Its application ranges from enriching text-to-image conversion to refining images and facilitating multiview generation, showcasing its versatility and potential to reshape the future of AI-driven image creation.","Unveil the potential of PeRFlow, a cutting-edge approach to accelerate diffusion models, enabling rapid, high-quality image generation. Explore how it revolutionizes AI-powered image synthesis across various applications.",Image Generation Platform,"Python





        197





        11


        Built by

          






        14 stars today",https://raw.githubusercontent.com/magic-research/piecewise-rectified-flow/main/assets/rocket-icon-png-21.png,,197,2024-02-16T16:13:34Z
2024-03-11,https://github.com/myshell-ai/OpenVoice,https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/README.md,"The text introduces OpenVoice, a technology detailed in a paper and website by researchers from MIT, Tsinghua University, and MyShell. OpenVoice excels in three main areas: it can accurately clone the tone color of voices across multiple languages and accents, offers flexible control over voice styles such as emotion and accent, and supports zero-shot cross-lingual voice cloning without requiring the languages to be included in its multi-lingual training dataset. Since May 2023, OpenVoice has been integral to myshell.ai's instant voice cloning feature, achieving tens of millions of uses and significant user growth by November 2023. The project, prohibited for commercial use under a Creative Commons license (soon to change for free commercial use), emphasizes social responsibility and anti-misuse by detecting if an audio is generated by OpenVoice. The technology builds on the work of several pioneering projects in text-to-speech and voice synthesis.",Exploring OpenVoice: The Frontier of Instant Voice Cloning Technology,"Discover the revolutionary OpenVoice technology that's redefining the future of voice cloning. From accurate tone color replication in multiple languages to flexible voice style control and zero-shot cross-lingual cloning, OpenVoice marks a significant milestone in voice synthesis. Highlighting its impactful use since May 2023, the technology has already seen millions of applications, showcasing rapid user growth on the platform. Developed by leading researchers and backed by a vibrant community, OpenVoice is at the forefront of voice cloning advancements. Dive into how OpenVoice is powering innovative voice cloning at an unprecedented scale.","Delve into OpenVoice â€“ a groundbreaking voice cloning technology with accurate tone cloning, flexible style control, and zero-shot cross-lingual capabilities, seeing explosive growth since its 2023 launch.",Natural Language Processing,"Python





        14,872





        1,328


        Built by

          









        90 stars today",https://raw.githubusercontent.com/myshell-ai/OpenVoice/main/resources/openvoicelogo.jpg,,14872,2023-11-29T12:17:01Z
2024-03-11,https://github.com/XingangPan/DragGAN,https://raw.githubusercontent.com/XingangPan/DragGAN/main/README.md,"The paper ""Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold"" by Xingang Pan, Ayush Tewari, Thomas LeimkÃ¼hler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt, presented at SIGGRAPH 2023, introduces a novel method for manipulating GAN-generated images through an interactive point-based interface. Users can intuitively edit images by dragging points across the image's surface, significantly enhancing usability in image manipulation tasks. This project incorporates the use of PyTorch, provides demos on platforms like OpenXLab, Hugging Face, and Gradio, and offers detailed instructions for setup on different systems, including those without Nvidia GPUs. The Docker-based Gradio visualizer setup is explained for users seeking to explore the tool quickly. Additionally, the documentation guides on downloading pre-trained StyleGAN2 weights for immediate application and provides a GUI for straightforward operation. Acknowledging the foundation in StyleGAN3 and other resources, the project highlights its license terms and appreciates the broader community's contributions. The BibTeX entry is provided for citation purposes.",Unveiling DragGAN: Revolutionize Image Editing with Interactive GAN Manipulation,"Discover the groundbreaking DragGAN, introduced by Xingang Pan and team, revolutionizing the realm of GAN-generated imagery. Unveiled at SIGGRAPH 2023, this tool offers unparalleled interactive point-based manipulation, enabling users to alter generative image manifolds with ease. Bridging creativity and technology, DragGAN leverages advanced algorithms for intuitive image editing, promising a new era of digital artistry. Dive into the detailed process of setting up DragGAN, from environment setup to launching the Gradio visualizer, and explore the endless possibilities of image manipulation.","Explore the capabilities of DragGAN, a cutting-edge tool by Xingang Pan and team for interactive GAN-based image manipulation, as presented at SIGGRAPH 2023. Learn how to set up and utilize this innovative technology for creative image editing.",Image Generation Platform,"Python





        34,666





        3,305


        Built by

          









        105 stars today",https://raw.githubusercontent.com/XingangPan/DragGAN/main/DragGAN.gif,,34666,2023-05-18T10:08:02Z
2024-03-11,https://github.com/RVC-Boss/GPT-SoVITS,https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/README.md,"GPT-SoVITS-WebUI is a potent few-shot voice conversion and text-to-speech interface. It features zero-shot text-to-speech conversion using a 5-second vocal sample for instant transformation, and few-shot TTS that refines voice similarity with only 1 minute of training. The system supports cross-lingual inference in English, Japanese, and Chinese. Integrated tools assist in creating training datasets and models, including voice accompaniment separation and automatic training set segmentation. Installation guides are provided for various operating systems, and a section on installing dependencies manually is also included. Pretrained models can be downloaded, and the format for TTS annotation files is detailed. Future updates aim to include zero-shot voice conversion, speaking speed control, and enhanced emotional expression in TTS among others. Special thanks are given to projects and contributors who have influenced the development of GPT-SoVITS-WebUI.",Revolutionize Your TTS Experience with GPT-SoVITS WebUI,"Introducing GPT-SoVITS-WebUI, the groundbreaking few-shot voice conversion and text-to-speech interface designed to revolutionize your TTS experience. With features like zero-shot TTS, cross-lingual support, and integrated WebUI tools, GPT-SoVITS offers unparalleled ease and realism in voice conversion. Whether youâ€™re looking to fine-tune the model with minimal training data or explore voice separation and automatic training set segmentation, GPT-SoVITS stands ready to exceed expectations. Dive into the future of voice conversion; try GPT-SoVITS today!","Discover GPT-SoVITS WebUI: An advanced few-shot voice conversion and TTS platform offering zero-shot capabilities, cross-lingual support, and integrated tools for an unmatched voice conversion experience.",Deep Learning Platform,"Python





        18,228





        1,928


        Built by

          









        168 stars today",,,18228,2024-01-14T18:05:21Z
2024-03-11,https://github.com/SethBling/cbscript,https://raw.githubusercontent.com/SethBling/cbscript/master/README.md,"CBScript, designed by SethBling, is a transpiled language that compiles CBScript files into Minecraft datapack zip files, offering higher-level programming features not available in Minecraft's command system. It's implemented to reduce performance overhead and bugs, with datapack files organized by source file line numbers for easy navigation. The compiler needs Python 3 and the Python ""py launcher"" for running, along with certain Python dependencies installed via pip. Instructions for setting up file execution and syntax highlighting in Notepad++ are provided. Running the compiler auto-compiles on file changes, placing the datapack in the specified world's /datapacks folder. CBScript enhances datapack creation with features like include files, loops, conditional blocks, simplified syntax for game commands, macro support, and more, aiding in cleaner, maintainable code structures.",Mastering CBScript: Enhance Your Minecraft Datapacks with Ease,"Discover the power of CBScript, a transpiled language crafted by SethBling for creating sophisticated Minecraft datapacks. This innovative compiler transforms CBScript files into organized Minecraft datapack zip files, introducing high-level language features absent in Minecraft commands. By understanding its implementation details, you can sidestep common bugs and performance issues. Learn how to install the compiler, which requires Python 3, and explore its advanced features, such as include files, loops, and custom entity selectors.","Unlock the full potential of Minecraft datapacks with CBScript: a high-level language designed by SethBling for seamless Minecraft modding. Learn installation, features, and best practices.",Game Development Tool,"Python





        797





        12


        Built by

          





        163 stars today",,,797,2019-04-22T19:33:45Z
2024-03-11,https://github.com/3b1b/manim,https://raw.githubusercontent.com/3b1b/manim/master/README.md,"Manim is an animation engine, particularly designed for math videos, originally created for the [3Blue1Brown](https://www.3blue1brown.com) series. It's programmed to produce precise animations programmatically. There are two versions: the original by 3b1b and a more stable, community fork that's easier for beginners. The software, ManimGL, requires Python 3.7 or higher, along with FFmpeg, OpenGL, and optionally LaTeX (for LaTeX use). For Linux users, Pango is also needed. Installation differs slightly across platforms but generally involves cloning the GitHub repository and using pip for installation. Users can test it with provided example scenes or create custom animations. The tool offers various flags for rendering and customizing output. There's ongoing documentation, including a version in Chinese by **@manim-kindergarten**. Contributions are welcomed, especially to the community edition, which focuses on active development. The project is licensed under the MIT License, encouraging openness and collaboration.",Exploring ManimGL: The Animation Engine Powering Math Videos,"ManimGL is a specialized animation engine developed by the creator of 3Blue1Brown, aimed at crafting precise, programmatic animations for mathematical storytelling. Distinguishable from its community edition, ManimGL focuses on stability and ease of use for individual projects. Installation instructions are specific, requiring Python 3.7+, FFmpeg, OpenGL, and optionally LaTeX for full functionality. Through simple commands, users can install ManimGL and begin creating educational content with high-quality animations. Whether for educational purposes or personal projects, ManimGL offers a robust platform for animating complex mathematical concepts.","Discover ManimGL by 3Blue1Brown for creating mathematical animations. Learn how to install and use ManimGL, the preferred engine for high-quality math videos. Installation guide and usage tips included.",Animation Engine,"Python





        56,804





        5,537


        Built by

          









        55 stars today",https://raw.githubusercontent.com/3b1b/manim/master/logo/cropped.png,,56804,2015-03-22T18:50:58Z
2024-03-11,https://github.com/mathialo/bython,https://raw.githubusercontent.com/mathialo/bython/master/README.md,"Bython is essentially Python enhanced with braces to address the challenges associated with whitespace sensitivity. It serves as a preprocessor that converts code with curly brackets into standard Python indentation, aimed at those who find Python's indentation rules cumbersome. Key features include resilience against common indentation errors, compatibility with Python's modules like NumPy and Matplotlib, and a focus on maintaining code readability despite differing indentation styles. Installation is straightforward via pip from PyPI or directly from the Git repository. Bython operates by translating `.by` files into `.py` files for execution, requiring an operational Python environment. It supports running Bython programs with command line arguments and offers a `py2by` tool for converting Python files into Bython format. The repository includes the Bython package, manual pages, scripts, and test cases.",Embrace Bython: Unleashing Python with Braces for Enhanced Readability,"Discover Bython, the revolutionary Python preprocessor that brings the power of braces to Python syntax, aiming to simplify coding by eliminating the frustration of whitespace sensitivity. Bython seamlessly translates curly brackets into Python's indentation, ensuring compatibility with existing Python modules like NumPy and Matplotlib. Ideal for developers seeking a blend of Python's versatility and a more forgiving syntax, Bython offers an accessible solution for code formatting challenges. With easy installation via PyPI or directly from GitHub, Bython enhances Python coding efficiency without compromising on performance. Dive into the world of Bython for a smoother coding experience, leveraging braces for clearer, more maintainable code.","Explore Bython, a Python preprocessor that introduces braces for better syntax handling, ensuring compatibility with Python modules. Learn how Bython simplifies coding by addressing whitespace issues, offering an easy installation process for improved coding efficiency.",Python Libraries Collection,"Python





        868





        32


        Built by

          






        83 stars today",,,868,2016-04-06T09:09:11Z
2024-03-11,https://github.com/TheR1D/shell_gpt,https://raw.githubusercontent.com/TheR1D/shell_gpt/main/README.md,"ShellGPT is a command-line tool that enhances productivity by automating the generation of shell commands, code snippets, and documentation, eliminating the dependency on external searches. It's compatible with Linux, macOS, Windows, and major shells like PowerShell, CMD, Bash, and Zsh. Installation is straightforward via pip and requires an OpenAI API key for GPT-4 model usage, though free local models like Ollama are supported with additional setup. ShellGPT simplifies technical tasks by accepting both stdin and command-line inputs, supporting piping, redirection, and offering features like quick shell command generation, code generation, chat and REPL modes for iterative interaction, function calling for executing system commands, role creation for custom tasks, request caching, and extensive configuration options. It also includes Docker support for containerized environments. ShellGPT is a versatile tool for developers seeking to streamline their workflow through AI-powered automation.",Revolutionizing Command-Line Productivity with ShellGPT: An AI-Powered Tool,"Meet ShellGPT, the revolutionary command-line tool designed to supercharge your productivity by generating shell commands, code snippets, and documentation using AI large language models (LLM). Compatible with major operating systems and shells, ShellGPT simplifies complex tasks, offering a seamless integration into your workflow. Installation is a breeze with a simple pip command, and you can choose between using OpenAI's API or locally hosted models for flexibility. Dive into the future of command-line operations with ShellGPT and elevate your development experience.","Discover ShellGPT, an AI-driven command-line tool that generates shell commands, code, and documentation effortlessly. Easy installation, OS/shell compatibility, and the choice of API models make it a must-have for developers.",AI Coding Assistant,"Python





        7,812





        610


        Built by

          









        81 stars today",https://s10.gifyu.com/images/repl-demo.gif,,7812,2023-01-18T19:40:11Z
2024-03-11,https://github.com/langchain-ai/chat-langchain,https://raw.githubusercontent.com/langchain-ai/chat-langchain/master/README.md,"The Chat LangChain repository hosts a locally hosted chatbot designed for querying the LangChain documentation. It utilizes LangChain, FastAPI, and Next.js for its construction, with a currently deployed version accessible. For JavaScript enthusiasts, a JS variant is available. This chatbot employs LangChain's streaming and async API for live updates across users. Setting it up locally involves installing dependencies, configuring environmental variables, ingesting documentation data, and running both backend and frontend components. Its operation divides into two main parts: ingestion and question-answering, with each comprising multiple steps involving document processing and generating answers from inputs. Additional documentation provides a deeper insight into customization and deployment for users' specific needs.",Unleashing the Power of Chat LangChain: A Local Hosted Chatbot for AI Driven Insights,"Discover the innovative locally hosted Chat LangChain, an application designed for dynamic question-answering over the LangChain documentation. This chatbot, powered by LangChain, FastAPI, and Next.js, offers real-time updates for multiple users through its advanced streaming and async APIs. With a focus on seamless deployment and interactive engagement, Chat LangChain transforms how developers and users interact with AI documentation. Learn how to run it locally, ingest LangChain docs, and navigate through its technical components for a comprehensive understanding.","Explore Chat LangChain, a locally hosted chatbot application leveraging LangChain documentation for question-answering. Powered by LangChain, FastAPI, and Next.js, understand its setup, deployment, and real-time interaction functionalities.",AI Coding Assistant,"Python





        4,520





        1,016


        Built by

          









        20 stars today",,,4520,2023-01-16T20:43:13Z
2024-03-12,https://github.com/danielmiessler/fabric,https://raw.githubusercontent.com/danielmiessler/fabric/main/README.md,"Fabric is an open-source project developed to integrate Artificial Intelligence (AI) into everyday tasks, addressing the challenge of making powerful AI functionalities more accessible and usable in real life. Initiated to combat the integration problem faced by AI, rather than its capabilities, Fabric provides a framework for enhancing human creativity and problem-solving through AI augmentation. Its philosophy emphasizes breaking down problems into components and employing AI to tackle them individually. The project includes detailed documentation and tools for setting up and using Fabric, encouraging users to update regularly due to frequent additions and improvements. Fabric offers ""Patterns"" which are AI prompts designed for various activities, ranging from extracting information from videos to generating social media posts. Users can interact with these Patterns through a CLI-native client, setting up their own infrastructure for personal AI assistance. Additionally, the project supports collaborative contributions, acknowledging several individuals who have played significant roles in its development.",Maximizing Human Creativity: Unleash Potential with AI Augmentation Using Fabric,"Discover how Fabric, an open-source framework introduced in 2024, is revolutionizing the integration of AI into our daily lives to solve real human problems. By breaking down challenges into components and offering customizable AI patterns called 'Patterns', Fabric enables granular application of AI, making technology more accessible and enhancing human creativity. Stay updated with Fabric to leverage AI innovations, from local models like Claude to an array of new Patterns for everyday tasks.","Explore Fabric, the AI augmentation framework designed to enhance human creativity by solving integration problems with customizable AI patterns. Learn how to seamlessly incorporate AI into your life with Fabric.",Collaborative AI Framework,"Python





        6,516





        596


        Built by

          









        396 stars today",https://raw.githubusercontent.com/danielmiessler/fabric/main/./images/fabric-logo-gif.gif,https://www.youtube.com/watch?v=wPEyyigh10g,6516,2024-01-03T23:18:31Z
2024-03-12,https://github.com/microsoft/qlib,https://raw.githubusercontent.com/microsoft/qlib/main/README.md,"Qlib, developed by Microsoft, is an AI-oriented open-source quantitative investment platform aimed at harnessing the power of AI technologies in quantitative investment. It supports various machine learning modeling paradigms including supervised learning, market dynamics modeling, and reinforcement learning. Qlib facilitates the entire machine learning pipeline including data processing, model training, back-testing, and covers the quantitative investment process from alpha seeking, risk modeling, portfolio optimization, to order execution. It also provides a framework for increasing SOTA Quant research works/papers to address challenges in quantitative investment. Recent features include KRNN and Sandwich models, RL Learning Framework, and various models for market prediction and analysis. The platform is designed with a high-level framework that allows for modular use of its components. Installation is supported on Linux, Windows, and macOS with specific Python version requirements. Detailed documentation and tutorials are available for users to explore Qlib's capabilities.",Maximizing Your Quantitative Investment Strategies with Microsoft's Qlib,"Explore the latest features of Qlib, Microsoft's AI-oriented quantitative investment platform, and learn how it's shaping the future of finance with its wide support for Python versions and operating systems. Discover the potential of Qlib in empowering research and creating value in quantitative investment through advanced models like KRNN, Sandwich, and others. Dive into how Qlib facilitates comprehensive quantitative research workflows and the development of sophisticated trading strategies with its recent updates.","Discover Microsoft's Qlib: an AI-oriented quantitative investment platform revolutionizing quantitative investment research with its latest features, including KRNN, Sandwich models, and comprehensive support for Python. Elevate your trading strategies with Qlib.",AI Python Client,"Python





        13,680





        2,381


        Built by

          









        31 stars today",http://fintech.msra.cn/images_v070/logo/1.png; https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/framework-abstract.jpg; https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/score_ic.png; https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/cumulative_return.png; https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/risk_analysis.png; https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/rank_label.png; http://fintech.msra.cn/images_v070/qrcode/gitter_qr.png; https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif,,13680,2020-08-14T06:46:00Z
2024-03-12,https://github.com/THUDM/ChatGLM3,https://raw.githubusercontent.com/THUDM/ChatGLM3/main/README.md,"ChatGLM3, developed by ZhipuAI and Tsinghua University's KEG lab, is an open-source conversational pre-trained model, part of a series of models designed for fluent conversations with low deployment thresholds. The ChatGLM3-6B model introduces several improvements over its predecessors: a stronger base model (ChatGLM3-6B-Base) trained on a wider variety of data, more extensive training steps, and enhanced training strategies, making it one of the most powerful models under 10B parameters across multiple datasets in terms of semantics, math, reasoning, code, and knowledge. It also supports a more complete range of functions including multi-turn dialogues, tool invocation, code execution, and agent tasks with a new Prompt format, along with a comprehensive open-source lineup including basic, long-text, and enhanced long-text understanding models, all available freely for academic research and eligible for free commercial use post registration. 

The model aims to propel technological advancements in large models with the open-source community, urging developers to adhere to open-source protocols and not use the model for harmful purposes. Despite every effort for data compliance and accuracy during training, due to its smaller scale and probabilistic nature, the model's outputs may not always be accurate and can be misled by user inputs. The project does not assume any risks or liabilities that might arise from data security issues or model misuse.

Furthermore, the model has been evaluated across several benchmarks, showing significant improvement over its second-gen counterpart, especially in long-text applications like paper reading, executive summaries, and financial analysis. The document also details installation, a comprehensive demo covering chat, tool use, and code execution modes, code invocation methods, model tuning, web and CLI demos, deployment strategies including model quantization, CPU and Mac deployment, OpenVINO and TensorRT-LLM accelerated inference, and references for citing their work.",Unveiling ChatGLM3: The Latest Breakthrough in Conversational AI,"Discover the groundbreaking ChatGLM3, a conversational model co-developed by Zhipu AI and Tsinghua University's KEG lab, offering unparalleled dialogue fluency and deployment accessibility. ChatGLM3-6B introduces an enhanced foundation model, full-featured support, and a comprehensive suite of open-source derivatives for both academic and commercial use. This release marks a significant step forward in conversational AI, pushing the boundaries of what's possible with deep learning technologies.","Explore ChatGLM3-6B, the latest conversational AI model by Zhipu AI and Tsinghua's KEG lab, featuring improved performance, full-featured support, and open-source access for advanced dialogue systems.",Language Models,"Python





        10,582





        1,186


        Built by

          









        51 stars today",https://raw.githubusercontent.com/THUDM/ChatGLM3/main/resources/tool.png; https://raw.githubusercontent.com/THUDM/ChatGLM3/main/resources/heart.png; https://raw.githubusercontent.com/THUDM/ChatGLM3/main/resources/web-demo.gif; https://raw.githubusercontent.com/THUDM/ChatGLM3/main/resources/web-demo2.png; https://raw.githubusercontent.com/THUDM/ChatGLM3/main/resources/cli-demo.png,https://www.youtube.com/watch?v=Pw9PB6R7ORA,10582,2023-10-26T06:22:47Z
2024-03-12,https://github.com/python/mypy,https://raw.githubusercontent.com/python/mypy/master/README.md,"Mypy is a static type checker for Python that helps developers ensure that variables and functions are used correctly in their code by adding type hints, as per PEP 484. It operates without running the program, unlike Python's dynamic error discovery. Mypy supports gradual typing, allowing incremental addition of type hints to a codebase. It features a powerful type system, including type inference, generics, callable types, and more, aiming to make programs easier to understand and maintain. Mypy can be easily integrated into several IDEs and supports installation via pip. It also offers a daemon mode for faster incremental updates on large codebases. The project welcomes contributions in various forms including testing, development, and documentation. Mypy is compiled using Mypyc to enhance its performance, making it significantly faster than an interpreted version. Additional information, documentation, and ways to contribute can be found on its website and GitHub page.",Mypy: The Ultimate Static Type Checker for Your Python Code,"Mypy is revolutionizing Python programming by introducing static type checking, making code error detection easier without running the program. It ensures better code quality and debugging by using type hints as per PEP 484. Mypy supports dynamic language properties with static checking, enhancing Python's flexibility. Gradual typing allows incremental codebase improvement, making Python code maintenance more efficient. Explore Mypy's powerful features to improve your code's reliability and understandability.","Discover how Mypy, the static type checker for Python, enhances code quality through type hints, supporting gradual typing and a robust type system for error-free coding.",Python Libraries Collection,"Python





        17,330





        2,661


        Built by

          









        7 stars today",,,17330,2012-12-07T13:30:23Z
2024-03-12,https://github.com/elebumm/RedditVideoMakerBot,https://raw.githubusercontent.com/elebumm/RedditVideoMakerBot/master/README.md,"The Reddit Video Maker Bot, created by Lewis Menelaws and TMRRW, is a tool designed to automate the creation of videos from Reddit content, a process that traditionally requires significant editing effort. This bot simplifies video production to cater to popular platforms like TikTok, YouTube, and Instagram, which thrive on quick, engaging content that often garners millions of views. Currently, the bot requires manual video uploads to avoid community guideline conflicts. It operates with Python 3.10 and relies on Playwright for functionality, requiring users to follow a set installation process. The project is open-source, inviting contributions of any skill level to improve features such as documentation, UI, content selection, and more. Notably, it supports customizations like background music, subreddit selection, and voice changes. Further details, including installation guides and support, are available through its documentation and a dedicated Discord server. The development team, including founder Elebumm and various maintainers, encourages community engagement and contributions to enhance the bot's capabilities.",Revolutionizing Content Creation: Introducing the Reddit Video Maker Bot,"Discover the Reddit Video Maker Bot, a powerful tool created by Lewis Menelaws and TMRRW that simplifies video content creation for platforms like TikTok, YouTube, and Instagram, by automating video editing and compilation processes. This groundbreaking bot requires only your programming skills to transform Reddit threads into engaging videos, promising a hassle-free experience for content creators. Explore the motivation, installation, and potential improvements to make content creation effortless and more efficient.","Explore how the Reddit Video Maker Bot by Lewis Menelaws & TMRRW is transforming content creation by automating video edit and compilation for TikTok, YouTube, & Instagram. Effortless setup & customization.",Video Translation Tool,"Python





        6,043





        1,701


        Built by

          









        9 stars today",https://user-images.githubusercontent.com/6053155/170528582-cb6671e7-5a2f-4bd4-a048-0e6cfa54f0f7.png; https://user-images.githubusercontent.com/6053155/173631669-1d1b14ad-c478-4010-b57d-d79592a789f2.png,https://www.youtube.com/watch?v=3gjcY_00U1w,6043,2022-05-26T15:48:10Z
2024-03-12,https://github.com/netease-youdao/QAnything,https://raw.githubusercontent.com/netease-youdao/QAnything/master/README.md,"QAnything is a local question-answering system that facilitates querying a wide range of formats and databases offline, enhancing data security and cross-language QA ability. It's designed for easy installation and use, aiming at enterprises needing high-performance, secure data handling. QAnything supports PDF, Word, PPT, XLS, Markdown, Email, TXT, images, CSV, and HTML files, planning more format compatibility. Its architecture employs two-stage retrieval to prevent degradation with data scaling, leveraging BCEmbedding for bilingual proficiency, offering better performance as data increases. Recent updates include custom large models, enhanced deployment on Windows, and improved file parsing. For Linux and Windows via WSL, it requires NVIDIA GPU with a minimum GTX 1050Ti and other software prerequisites. Installation involves cloning the GitHub repository and running a startup script. Online experiences and API usage are outlined, ensuring easy interaction with the system. Offline installation instructions are provided as well. The project welcomes contributions and involvement via Discord, WeChat, and GitHub issues.",Exploring QAnything: The Ultimate Question and Answer System for Any Format,"QAnything revolutionizes the way we handle data by offering a question and answer system that supports a wide array of file formats and databases, ensuring data security, and providing cross-language support. Its architecture is designed to handle massive data sets with ease, promising high performance for enterprise applications. With its user-friendly interface, QAnything facilitates one-click installation and seamless deployment, making it an ideal choice for individuals and organizations seeking an efficient knowledge base solution.","Discover QAnything, the versatile question and answer system designed for any file format. Learn about its key features, including data security, cross-language support, and how it seamlessly handles massive datasets for enterprise use.",Data Ingestion Tool,"Python





        5,178





        434


        Built by

          









        70 stars today",https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/qanything_logo.png; https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/qanything_arch.png; https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/two_stage_retrieval.jpg; https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/assets/rag_eval_multiple_domains_summary.jpg; https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/Wechat.jpg,,5179,2024-01-03T03:24:18Z
2024-03-12,https://github.com/Misaka-blog/chromego_merge,https://raw.githubusercontent.com/Misaka-blog/chromego_merge/main/README.md,"The text provides guidance on bypassing website blocks experienced with ChromeGo by using the Warp service or leveraging built-in browser DNS-over-HTTPS (DoH) and client tun mode, with a reference link for enabling Chrome's DoH. It outlines how to customize Warp nodes by replacing the configuration file's content with one's own Warp+ node information using a Warp WireGuard profile generator and a bot. Subscription links for different versions of proxy services without and with Warp, general Base64 links, and sing-box subscriptions are shared for various devices. Recommended clients for Windows, Android, iOS, and macOS include Clash Verge, NekoRay, NekoBox, and Shadowrocket. The text acknowledges contributions from Alvin9999 and sing-box-subscribe, mentioning that regional setting codes were taken from chromegopacs.",Bypassing Internet Blocking with Chromego and Warp: A Comprehensive Guide,"Learn how to bypass internet blocks using Chromego and Warp, including steps to modify your own Warp node and share subscription links. This guide provides useful resources and links for seamless setup across various platforms including Windows, Android, iOS, and macOS, enhancing your browsing experience while maintaining access to restricted sites.",Unlock the full potential of your internet browsing with our guide on using Chromego and Warp to bypass blocks. Get tips on setting up your own Warp node and accessing subscription links for different devices.,Cybersecurity Tool,"Python





        122





        251


        Built by

          






        28 stars today",,,122,2024-03-09T00:12:14Z
2024-03-12,https://github.com/KhoomeiK/LlamaGym,https://raw.githubusercontent.com/KhoomeiK/LlamaGym/main/README.md,"LlamaGym is designed to facilitate the fine-tuning of LLM (Large Language Model) agents using online reinforcement learning (RL) in a simplified manner. Unlike traditional LLM-based agents that do not learn online in real-time, LlamaGym aims to address this gap by providing a single `Agent` abstract class. This class helps manage conversation context, episode batches, rewards, and PPO setup across any Gym environment, thereby streamlining experiments with agent prompting and hyperparameter adjustments. To use LlamaGym, one needs to install it via pip, implement three abstract methods (`get_system_prompt`, `format_observation`, and `extract_action`), and integrate it with a base LLM and RL loop. Although online RL poses challenges in terms of convergence and requires hyperparameter tuning, LlamaGym simplifies the process compared to other frameworks and encourages contributions. It highlights relevant research and suggests that LLMs can potentially benefit from a supervised fine-tuning stage on sampled trajectories to enhance RL performance.",Revolutionize Your AI: Fine-Tuning LLM Agents with LlamaGym,"Discover LlamaGym, the breakthrough in fine-tuning LLM agents through online reinforcement learning, making it easier than ever to integrate into Gym environments. Dive into the simplicity of the Agent abstract class that streamlines the entire process from training to implementation. Learn how to effortlessly install, configure, and deploy your LLM-based agents in any RL scenario. Explore code examples that demonstrate how you can enhance your agent's learning efficiency. Whether you're new to online RL or looking to refine your approaches, LlamaGym offers the tools to advance your projects swiftly.",Unlock the potential of LLM agents with LlamaGym's easy integration into Gym environments for online reinforcement learning. Perfect for developers looking to accelerate agent training and implementation.,Deep Learning Platform,"Python





        653





        19


        Built by

          





        123 stars today",https://raw.githubusercontent.com/khoomeik/LlamaGym/main/llamagym.png,,654,2024-03-01T03:03:38Z
2024-03-12,https://github.com/vllm-project/vllm,https://raw.githubusercontent.com/vllm-project/vllm/main/README.md,"The vLLM project offers an easy, fast, and economical solution for serving large language models (LLMs) to everyone. It features state-of-the-art serving throughput, efficient management of attention key and value memory via ""PagedAttention,"" and supports a wide range of Hugging Face models. vLLM provides fast model execution through various optimizations, including CUDA/HIP graph and quantization techniques. It offers high-throughput serving with different decoding algorithms and supports both NVIDIA and AMD GPUs. The project has received support from Andreessen Horowitz and integrates seamlessly with different architectures, including BLOOM, GPT-NeoX, LLaMA-2, and many others. Installation is straightforward via pip, and comprehensive documentation is available for getting started and integration. The project encourages contributions and has published a paper detailing its efficient memory management technique for LLM serving.","Revolutionizing LLM Serving: vLLM's Easy, Fast, and Affordable Solution","Discover the future of Large Language Model (LLM) serving with vLLM, offering unprecedented speed, flexibility, and affordability. From seamless integration with popular models to state-of-the-art serving throughput, vLLM is designed for everyone. Whether you're running 7B or 70B models, vLLM ensures efficient attention key management, fast execution, and support for both NVIDIA and AMD GPUs. Join our community and explore how vLLM is transforming LLM serving, making it accessible to a wider audience.","Learn how vLLM is changing the game in LLM serving with its fast, easy-to-use, and cost-effective library solution. Explore the features, community engagement, and how you can get started with vLLM today.",Language Models,"Python





        15,917





        1,979


        Built by

          









        51 stars today",https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png,,15917,2023-02-09T11:23:20Z
2024-03-12,https://github.com/jackhawks/rectg,https://raw.githubusercontent.com/jackhawks/rectg/main/README.md,"This text lists over 5000 high-quality Telegram groups, channels, and bots, carefully selected to provide users with optimal choices. It originates from the internet and serves for learning and technical research purposes. Users are encouraged to report sensitive content found in the listed entities for timely management. Among the mentioned categories are discussion groups for Google Voice learning in North America, recommended apps, sweepstakes, surrounding RSS technology, cat lovers, beta testing, V2EX technology discussions, Python trends, science and technology updates, Notion techniques, immersive translation discussions, Tesla news, beauty galleries, resource sharing, and many more focusing on a wide range of interests including technology, education, entertainment, photography, and lifestyle.","Discover the Best Telegram Groups, Channels, and Bots with Over 5000+ Curated Options","Navigate the world of Telegram with ease! Our project compiles an extensive list of over 5000+ critically selected Telegram groups, channels, and bots, perfect for quick access to diverse content. From North American SIM cards and Google Voice beginners to comprehensive discussions on RSS technology, Python trends, and ardent cat communities, find your niche effortlessly. Stay updated with tech circles, and engage in translations, with our guide ensuring you're always in the loop.","Explore our comprehensive guide to over 5000+ Telegram groups, channels, and bots, making your search for information, discussions, and communities efficient and tailored. Dive into a world of curated content ranging from tech trends to cat lover channels!",Public APIs Collection,"Python





        4,368





        254


        Built by

          





        125 stars today",,,4368,2023-06-17T02:15:48Z
2024-03-12,https://github.com/Farama-Foundation/Gymnasium,https://raw.githubusercontent.com/Farama-Foundation/Gymnasium/main/README.md,"Gymnasium is an open-source Python library aimed at the development and comparison of reinforcement learning algorithms. It offers a standard API for interactions between algorithms and a set of compliant environments, continuing the work initially started by OpenAI's Gym. Maintained by the Farama Foundation, it includes classic control, Box2D, Toy Text, MuJoCo, Atari, and third-party environments, catering to a range of complexities. Installation is simple via pip, supporting Python 3.8 to 3.11 on Linux and macOS. Gymnasium facilitates environment interaction through simple Python classes and provides compatibility for various dependencies. It emphasizes version control for reproducibility, offers a roadmap for future updates, and encourages community support through donations. Key associated libraries include CleanRL, PettingZoo, and Comet, with the latter offering an integration tutorial. The project, committed to future development and research facilitation, invites citations for academic and practical use.",Explore the World of Reinforcement Learning with Gymnasium: The Ultimate Python Library,"Discover Gymnasium, the open-source Python library that's revolutionizing the development and comparison of reinforcement learning algorithms. Offering a standard API for seamless interaction between algorithms and environments, Gymnasium stands out as the preferred choice for reinforcement learning endeavors. With extensive documentation and a supportive Discord community, it's designed to facilitate your projects. Whether a beginner or an expert, Gymnasium provides environments from classic control to complex Atari challenges, making it the go-to toolkit for your reinforcement learning needs.","Unlock the potential of reinforcement learning with Gymnasium, the comprehensive Python library for creating and comparing algorithms. Dive into a world of diverse environments and community support.",Deep Learning Platform,"Python





        5,298





        613


        Built by

          









        23 stars today",https://raw.githubusercontent.com/Farama-Foundation/Gymnasium/main/gymnasium-text.png,,5300,2022-09-08T01:58:05Z
2024-03-13,https://github.com/deepseek-ai/DeepSeek-VL,https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/README.md,"DeepSeek-VL is an open-source Vision-Language Model designed for comprehensive vision and language understanding in real-world applications. It offers general multimodal understanding capabilities suitable for processing various data types, including logical diagrams, web pages, scientific literature, and natural images. The DeepSeek-VL family includes models with 7B and 1.3B parameters in both base and chat variants, catering to different integration scenarios. These models are publicly available, supporting academic and commercial use under specified licenses. Installation and usage instructions are provided for quick start, alongside a simple inference example, CLI chat, and a Gradio demo for interactive exploration. The code repository is MIT licensed, whereas using DeepSeek-VL Base/Chat models is subject to the DeepSeek Model License, permitting commercial applications.",Unveiling DeepSeek-VL: A Revolution in Vision-Language Models,"Discover the latest in AI innovation with DeepSeek-VL, an advanced open-source Vision-Language model designed for comprehensive real-world applications. From understanding complex scientific literature to recognizing web elements and natural images, DeepSeek-VL is equipped with multimodal understanding capabilities that set a new benchmark in AI technology. Explore the potential of DeepSeek-VL in various real-world scenarios, promising unparalleled efficiency and accuracy in AI-driven tasks.","Explore DeepSeek-VL, the cutting-edge Vision-Language Model for next-level AI applications in real-world scenarios. Discover its capabilities in processing diverse multimodal content with unparalleled accuracy.",Vision-Language Model,"Python





        673





        38


        Built by

          









        156 stars today",https://github.com/deepseek-ai/DeepSeek-VL/blob/main/images/sample.jpg; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/./images/gradio_demo.png,,673,2024-03-07T08:32:57Z
2024-03-13,https://github.com/ELLA-Diffusion/ELLA,https://raw.githubusercontent.com/ELLA-Diffusion/ELLA/main/README.md,"ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment"" is a research project focused on integrating Large Language Models (LLMs) with diffusion models to improve semantic alignment. The research is contributed to by Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, and Pei Cheng, with Gang Yu as the corresponding author. The project has released its official code and has introduced DPG-Bench on March 11, 2024, as a guideline for generating and evaluating images based on specific prompts. The evaluation requires images to be generated in a 2x2 grid format and renamed according to the prompt's filename for uniform assessment. The project acknowledges the work of DSG for their instructions on generating question and answers for DPG-Bench. Future plans include releasing a checkpoint and inference code, with DPG-Bench already available. They encourage citing their work if found useful for research or application purposes.",ELLA's Breakthrough: Enhancing Semantic Alignment with LLM in Diffusion Models,"Discover how ELLA revolutionizes diffusion models by integrating LLM for superior semantic alignment, a collaboration by leading researchers. This innovation offers a detailed guide and introduces DPG-Bench for community engagement. From generating images following precise prompts to evaluating outcomes, ELLA marks a significant leap in AI's creative and analytical capabilities. Explore the official release, participate in DPG-Bench, and contribute to the evolving landscape of artificial intelligence.","ELLA introduces a groundbreaking approach to enhance diffusion models with LLM for improved semantic alignment, featuring the launch of DPG-Bench for comprehensive community engagement and evaluation.",Collaborative AI Framework,"Python





        323





        16


        Built by

          







        55 stars today",https://raw.githubusercontent.com/ELLA-Diffusion/ELLA/main/./assets/ELLA-Diffusion.jpg; https://raw.githubusercontent.com/ELLA-Diffusion/ELLA/main/./assets/teaser_3img.png; https://raw.githubusercontent.com/ELLA-Diffusion/ELLA/main/./assets/teaser1_raccoon.png,,323,2024-03-07T13:57:12Z
2024-03-13,https://github.com/Ciphey/Ciphey,https://raw.githubusercontent.com/Ciphey/Ciphey/master/README.md,"The text provides information about Ciphey, an automated decryption, decoding, and cracking tool that utilizes natural language processing, artificial intelligence, and some common sense to identify and decode encrypted text. Ciphey can support over 50+ encryption and encoding forms, including binary, Base64, classical ciphers like Caesar, and more advanced cryptography. It operates quickly, often decrypting within 3 seconds. The project emphasizes ease of use, requiring minimal setup and knowledge of the encrypted material beforehand. Installation instructions are provided for various platforms, including Python, Docker, MacPorts, and Homebrew, along with compatibility across Linux, macOS, and Windows systems. Comparisons highlight Ciphey's efficiency over alternatives like CyberChef. The project is open-source, inviting contributions from the community. It was developed with support from the Cyber Security Society at the University of Liverpool. Contributing to Ciphey is encouraged, with detailed guidance available for interested individuals. Financial contributions are also appreciated, supporting both future development and the cybersecurity community.",Unlock the World of Ciphey: Multilingual Decryption Tool Guide,"Explore the realms of cryptography with Ciphey, a fully automated decryption, decoding, and cracking tool powered by natural language processing and artificial intelligence. With support for over 50 encryptions and encodings, Ciphey can decipher texts in seconds. From classical ciphers to modern encryptions, Ciphey's custom-built AI, AuSearch, efficiently identifies encryption types, offering results in under 3 seconds. Ideal for anyone interested in cryptography, Ciphey simplifies the decryption process, making it accessible to both beginners and experts.","Discover how Ciphey uses AI to decrypt over 50 types of codes and ciphers in seconds. Perfect for cryptography enthusiasts, this tool simplifies decoding tasks.",Cybersecurity Tool,"Python





        16,133





        1,020


        Built by

          









        122 stars today",https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/binoculars.png; https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/python.png; https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/docker.png; https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/macports.png; https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/homebrew.png; https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/index.gif; https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/ciphey_gooder_cyberchef.gif; https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/not_dying.gif; https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/3ways.gif,,16133,2019-07-16T20:20:39Z
2024-03-13,https://github.com/d78ui98/APKDeepLens,https://raw.githubusercontent.com/d78ui98/APKDeepLens/main/README.md,"APKDeepLens is a sophisticated tool built with Python, designed for scanning Android applications (APKs) to uncover security vulnerabilities, focusing on the OWASP Top 10 mobile risks. It aids developers, penetration testers, and security researchers in evaluating an app's security. Key features include deep APK analysis, coverage of OWASP Top 10 vulnerabilities, advanced detection techniques, extraction of sensitive information, detection of insecure data storage and intent filter exploits, local file vulnerability detection, customizable report generation, CI/CD pipeline integration, and a user-friendly interface with color-coded terminal outputs. Installation requires Python 3.8 or higher and follows specific steps for Linux and Windows systems. Usage involves command-line inputs for scanning APKs, with options for detailed reporting. Contributions to the project are encouraged through GitHub. APKDeepLens has been featured at Blackhat MEA 2023 and is scheduled for Blackhat ASIA 2024.",Maximizing Android App Security with APKDeepLens: Your Ultimate Guide,"Discover how APKDeepLens, a powerful Python-based tool, revolutionizes the security scanning of Android applications by targeting the OWASP Top 10 mobile vulnerabilities. With features like APK analysis, sensitive information extraction, and detailed report generation, it is designed to bolster the security of apps against common vulnerabilities. Its integration into CI/CD pipelines ensures ongoing security assessment during development. The user-friendly interface and comprehensive OWASP coverage make APKDeepLens indispensable for developers, penetration testers, and security researchers alike.","Explore APKDeepLens, the Python-based security tool for Android APKs, offering features like OWASP Top 10 coverage, advanced detection, and CI/CD integration for enhanced app security. Learn how to install and use it for your security assessments.",Cybersecurity Tool,"Python





        160





        21


        Built by

          







        12 stars today",,,160,2023-11-24T14:02:39Z
2024-03-13,https://github.com/FujiwaraChoki/MoneyPrinter,https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinter/main/README.md,"MoneyPrinter (MP) is a tool that has recently been made active again for contributions. It enables users to automate YouTube Shorts creation by simply providing a video topic. MP emphasizes checking for existing issues before creating new ones and directs inquiries to their Discord community. The tool requires Python 3.11 and involves a setup process involving cloning the repository, installing dependencies, setting up environment variables, and running backend and frontend servers. Users can personalize their videos with their own music and fonts and have the option for automatic YouTube uploads, which requires Google Cloud Platform setup and OAuth consent. MP includes troubleshooting tips, encourages donations, and notes that it currently does not accept pull requests.",Reviving MoneyPrinter: A New Era for YouTube Shorts Automation,"MoneyPrinter has been revitalized, inviting contributions on automating YouTube Shorts creation. With a simple setup requiring Python 3.11, users can effortlessly generate videos by merely entering a topic. This reinvigoration not only simplifies content creation but also integrates features for automatic YouTube uploading, enhancing the platform's accessibility and efficiency for creators. Explore the easy installation, usage steps, and broaden your video creation capabilities with MoneyPrinter today.","Discover how MoneyPrinter transforms YouTube Shorts creation with its latest unarchiving, enabling easy automation, automatic uploads, and a streamlined installation process for creators.",Video Translation Tool,"Python





        8,477





        1,081


        Built by

          









        45 stars today",,https://www.youtube.com/watch?v=mkZsaDA2JnA,8477,2024-01-31T18:53:24Z
2024-03-13,https://github.com/yihong0618/xiaogpt,https://raw.githubusercontent.com/yihong0618/xiaogpt/main/README.md,"XiaoGPT is a software project that allows users to interact with various AI models, including ChatGPT, GPT-3, and others, using a Xiaomi AI Speaker. It supports multiple AI types, including New Bing, ChatGLM, Gemini, Bard, and é€šä¹‰åƒé—®, and is configurable to work with different services for enhanced functionality, such as voice synthesis and internet search. To use XiaoGPT, users need a Xiaomi speaker, a ChatGPT id, and a Python 3.8+ environment. Installation involves running several commands to set environment variables and install XiaoGPT via pip. The software allows customization and extended functionality through various flags, supporting different APIs for AI interaction, changing TTS capabilities, and configuring network settings for users behind firewalls or in special network environments. It's developed and maintained with community contributions, offering detailed documentation and support for issues. XiaoGPT offers Docker support for easier deployment, and it is open for contributions and improvements from the community.",Unlocking the Power of Xiaomi AI with XiaoGPT: A Comprehensive Guide,"Discover how XiaoGPT transforms the Xiaomi AI speaker into a powerful tool for interacting with ChatGPT, New Bing, Gemini, and more AI services. Learn to easily install, configure, and utilize XiaoGPT for enhanced communication with your Xiaomi device, opening a world of possibilities for AI-driven assistance at home. This guide provides a step-by-step approach to integrating cutting-edge AI technology with Xiaomi's hardware for an unrivaled smart home experience.","Learn how to elevate your Xiaomi AI speaker's capabilities with XiaoGPT. This guide covers installation, setup, and usage tips to harness the power of ChatGPT and other AI services seamlessly with your Xiaomi device.",AI Python Client,"Python





        4,861





        702


        Built by

          









        5 stars today",https://user-images.githubusercontent.com/15976103/220028375-c193a859-48a1-4270-95b6-ef540e54a621.png; https://user-images.githubusercontent.com/15976103/226802344-9c71f543-b73c-4a47-8703-4c200c434dec.png,https://www.youtube.com/watch?v=K4YA8YwzOOA,4861,2023-02-16T03:41:28Z
2024-03-13,https://github.com/skills-cogrammar/C7-Lecture-Backpack,https://raw.githubusercontent.com/skills-cogrammar/C7-Lecture-Backpack/main/README.md,"The Lecture Backpack is a rich educational resource for students and enthusiasts in software engineering, data science, full stack web development, and coding interview preparation. It offers a well-structured repository that includes basic programming fundamentals to advanced topics across different tech fields. The content is organized into five main sections: Python programming basics, advanced Software Engineering, comprehensive Data Science, Full Stack Web Development, and Coding Interview Workshops. Each section contains tailored learning materials ranging from basic concepts and tools to specialized subjects like machine learning, application security, and algorithms. To start, users are encouraged to clone the repository and select a learning track that matches their educational phase or interest area. This project is freely available under the MIT License, promoting an open and accessible learning environment for everyone interested in tech.",Maximize Learning with the Ultimate Lecture Backpack for Tech Enthusiasts,"Dive into the tech world with the Lecture Backpack, your go-to repository for mastering software engineering, data science, and full stack web development. Whether you're preparing for coding interviews or looking to reinforce your programming foundations, this comprehensive resource has you covered. Explore our curated learning tracks - from Python basics to advanced application development and data analysis techniques. Get ready to elevate your skills with hands-on workshops and practical exercises. Start your journey today and unlock the potential of every tech domain at your own pace.","Unlock your tech potential with the Lecture Backpack, offering tailored tracks in software engineering, data science, web development, and coding interviews. Master the skills needed for a thriving career in tech.",Data Science Learning,"Python





        41





        13


        Built by

          









        6 stars today",,,41,2024-03-05T15:43:26Z
2024-03-13,https://github.com/friuns2/Leaked-GPTs,https://raw.githubusercontent.com/friuns2/Leaked-GPTs/main/README.md,"The document lists various GPT (Generative Pre-trained Transformer) prompts designed for a wide range of applications. These include general activities like creative writing, game explanations, tech support, and recipe suggestions, as well as more specialized roles such as mocktail recipes, math mentoring, laundry advice, image modification, and more. Some GPTs are themed around pop culture or specific professional advice, including negotiation strategies, sticker design, sous chef roles, and even a mixologist for non-alcoholic drinks. There are also GPT prompts for educational support in subjects like math, and others offer entertainment or utility, like playing board games, designing coloring book pages, or providing tech support. The document also includes links to more prompts hosted on GitHub, indicating a community-driven effort to expand the applications of GPT technology across various fields and interests, showcasing the versatility and adaptability of GPT models for both fun and practical use cases.",Unveiling the Diversity of GPTs: From Tech Support to Creative Writing,"Discover the multifaceted world of GPTs tailored for various needs, from tech support to creative writing. From assisting in setting up printers, understanding memes, to becoming a mocktail mixologist, these AI agents offer a wide range of services. They cover everything from helping with math homework to turning ideas into coloring book pages. Embrace the future with GPTs designed to enhance your daily life and spark creativity.","Explore the vast array of specialized GPTs designed to assist in tech support, negotiations, creative projects, and more. Enhance your daily life and unleash creativity with AI.",Language Models,"Python





        1,595





        273


        Built by

          





        9 stars today",,,1595,2023-11-27T09:42:12Z
2024-03-13,https://github.com/levihsu/OOTDiffusion,https://raw.githubusercontent.com/levihsu/OOTDiffusion/main/README.md,"The OOTDiffusion repository hosts the official implementation of a model designed for controllable virtual try-ons, named OOTDiffusion. It leverages outfitting fusion with latent diffusion techniques, providing a tool for realistic garment fitting in digital environments. The model has been trained on two key datasets: VITON-HD for half-body garments and Dress Code for full-body outfits. Users can experiment with the model via a demo hosted on Hugging Face, backed by ZeroGPU for GPU support. Installation involves cloning the repository, setting up a conda environment, and installing necessary dependencies. The inference process supports both half-body and full-body models, with specific instructions provided for running each. The system requirements include Linux (Ubuntu 22.04) and downloading a specific checkpoint. The creators aim to complete the project by adding training code to accompany the already available paper, Gradio demo, inference code, and model weights.",Exploring OOTDiffusion: A Leap in Virtual Fashion Try-Ons,"Discover OOTDiffusion, the cutting-edge AI for seamless virtual try-ons, enabling highly controllable outfitting fusion using latent diffusion. This innovative tech supports full and half-body trials, paving the way for a revolutionized fashion experience. Its application spans from personal styling to design, supported by accessible model checkpoints and environmental compatibility. Dive into the future of fashion with OOTDiffusion's user-friendly interface and promising ONNX support for enhanced human parsing. A must-try for tech enthusiasts and fashion innovators.","Unveil the future of virtual fashion with OOTDiffusion, offering state-of-the-art outfitting fusion for an immersive try-on experience. Available now with comprehensive guides and ONNX support.",Image Generation Platform,"Python





        3,432





        468


        Built by

          







        68 stars today",https://raw.githubusercontent.com/levihsu/OOTDiffusion/main/images/demo.png; https://raw.githubusercontent.com/levihsu/OOTDiffusion/main/images/workflow.png,,3432,2024-01-24T07:39:15Z
2024-03-13,https://github.com/cubiq/ComfyUI_IPAdapter_plus,https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/README.md,"The ComfyUI IPAdapter Plus integrates IPAdapter models with ComfyUI, ensuring memory efficiency, speed, and compatibility with Comfy updates. The tool supports a range of functionalities, including handling non-square reference images for upscaling, FaceID models, and batch processing for animations. Key features include support for FaceID and FaceID Plus models, including an exclusive node for FaceID Plus v2, and the introduction of an experimental `noise` parameter for potentially better outcomes. It requires specific pre-trained models and image encoders downloadable from Huggingface. Installation includes placing pre-trained models and dependencies in designated directories within the ComfyUI environment. It offers a variety of workflows and examples for different applications, such as animation, image weighting, and attention masking. Troubleshooting guidance is provided, and the project also has a counterpart implementation for Huggingface Diffusers. Credits are given to the original IPAdapter, ComfyUI projects, and contributors.",Unlocking Creative Workflows with ComfyUI IPAdapter Plus: A Comprehensive Guide,"Discover the power of ComfyUI IPAdapter Plus for advanced image manipulation and enhancement. This essential tool offers memory-efficient, fast updates and compatibility with ComfyUI, ensuring your creative projects stay ahead of the curve. From experimental tiled IPAdapter support to crucial updates for FaceID Portrait models, our guide covers everything you need to know to leverage this innovative technology. Plus, dive into the myriad functionalities with our example directory and get up to speed with video tutorials.","Explore the latest ComfyUI IPAdapter Plus features for groundbreaking image-to-image conditioning. Learn about experimental tiled IPAdapter, FaceID updates, and access comprehensive tutorials for maximizing your creative potential.",Computer Vision Platform,"Python





        1,547





        122


        Built by

          







        11 stars today",https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./ipadapter_workflow.png; https://img.youtube.com/vi/7m9ZZFU3HWo/hqdefault.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/noise_example.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/prep_images.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/canny_controlnet.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/face_swap.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/inpainting.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/batch_images.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/image_weighting.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/weight_types.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/masking.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/timestepping.jpg; https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/./examples/face_id_wf.jpg,https://www.youtube.com/watch?v=7m9ZZFU3HWo; https://www.youtube.com/watch?v=7m9ZZFU3HWo; https://www.youtube.com/watch?v=mJQ62ly7jrg; https://www.youtube.com/watch?v=vqG1VXKteQg; https://www.youtube.com/watch?v=ddYbhv3WgWw,1547,2023-08-30T09:50:45Z
2024-03-14,https://github.com/BAAI-Agents/Cradle,https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/README.md,"""Cradle"" is a framework developed as a first attempt towards ensuring general computer control (GCC). It helps agents excel at any computer task by promoting strong reasoning abilities, self-improvement, and skill curation. The system operates in a standardized general environment with bare minimum requirements. Set up requires specific software and environment adjustments, including CUDA environment, GroundingDino, and video subfinder installations. It's been tested and is ready for use in the RDR2 game, available on various PC platforms, though now only fully tested on MS Windows. Developers seek to continually clean, update, and extend Cradle to more games and software.",Cradle: Pioneering General Computer Control,"Cradle is a novel framework pushing the boundaries of General Computer Control (GCC). Imbued with strong reasoning skills, self-improvement capabilities, and skill curation, Cradle empowers computer agents to ace any task. It operates in a standardized general environment with minimal requirements.","Explore how the Cradle framework paves the way to General Computer Control by enabling agents to perform any computer task efficiently, with a focus on reasoning ability, self-improvement, and skill curation. Discover this breakthrough in computing here.",Game Development Tool,"Python





        302





        15


        Built by

          









        44 stars today",https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/cradle-intro.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/rd2_task_grid_03.gif; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/rd2_task_grid_02.gif; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/rd2_task_grid_01.gif; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/rd2_task_grid_04.gif; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/video1.jpg; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/video2.jpg; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/raw_input.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/direct_input.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/move_control_previous.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/move_control_now.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/game_position.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/resolution.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/enlarge_minimap.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/minimap_setting.png; https://raw.githubusercontent.com/BAAI-Agents/Cradle/main/docs/images/subtitles.png,https://www.youtube.com/watch?v=Cx-D708BedY; https://www.youtube.com/watch?v=Oa4Ese8mMD0,302,2024-03-03T09:39:05Z
2024-03-14,https://github.com/imartinez/privateGPT,https://raw.githubusercontent.com/imartinez/privateGPT/main/README.md,"PrivateGPT is an AI project which uses Large Language Models (LLMs) to answers queries about documents while preserving privacy, as the data doesn't leave the execution environment. This project provides an API that facilitates the building of private AI applications. It supports both regular and streaming responses and offers a high-level API that manages complex RAH (Retrieval Augmented Generation) pipeline implementation and a low-level API for creating advanced pipelines. PrivateGPT comes with a Gradio UI client for testing the API and several other useful tools. Feedback and contributions are encouraged to refine the project. Documentation is available at docs.privategpt.dev.","Maximize AI Potential with PrivateGPT: Secure, Offline Language Processing","PrivateGPT is an innovative, secure AI project allowing query-based interaction with your documents. Ensuring 100% data privacy even without an internet connection. It extends OpenAI API standards and supports various response types. Ideal for developers building private,  context-aware AI apps, promising experimentation, and production-ready tools.","Learn how to leverage PrivateGPT - a production-ready AI project for private, offline interaction with your documents using language models. Ideal for building secure, context-aware AI applications.",Private AI Framework,"Python





        48,549





        6,379


        Built by

          









        89 stars today",,,48549,2023-05-02T09:15:31Z
2024-03-14,https://github.com/geekan/MetaGPT,https://raw.githubusercontent.com/geekan/MetaGPT/main/README.md,"MetaGPT is a multi-agent framework that assigns roles to Generative Pre-training Transformers (GPT) in order to enable them to work together to tackle complex tasks. By taking a single line instruction as input, the system can produce various outputs such as user stories, competitive analysis, requirements, data structures, API's, and documents. Inside MetaGPT, there are simulated roles of a software company including product managers, architects, project managers, and engineers. Additionally, it has released updates that included new features and tools, and its paper has been accepted for oral presentation at the International Conference on Learning Representations 2024.",MetaGPT: Revolutionizing Complex Tasks through Multi-Agent Collaboration,"MetaGPT is a GPT-based multi-agent framework dedicated to handling complex tasks. It assigns different roles to GPTs, enabling creative problem solving. Users will benefit from recent updates, presentations and its application in a software company.","Learn about MetaGPT, a powerful GPT-based multi-agent software providing unique solutions for intricate issues. Explore the latest updates and understand its practical application in a software company.",Collaborative AI Framework,"Python





        35,171





        4,022


        Built by

          









        233 stars today",https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-new-log.png; https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/software_company_cd.jpeg,https://www.youtube.com/watch?v=uT75J_KG_aY,35171,2023-06-30T09:04:55Z
2024-03-14,https://github.com/openai/transformer-debugger,https://raw.githubusercontent.com/openai/transformer-debugger/main/README.md,"Transformer Debugger (TDB) is a toolkit developed by OpenAI, designed to examine the behaviors of small language models. It combines automated interpretability techniques and sparse autoencoders to allow investigation into language models. TDB can answer questions regarding a model's output and can also intervene in the forward pass to gauge its effect on model behavior. It identifies components contributing to the behavior, generates explanations for these components' activation, and traces circuits between components. The tool is equipped with a neuron viewer, an activation server, and a simple inference library for GPT-2 models. Detailed setup instructions are provided.",Investigating Language Model Behaviors with Transformer Debugger Tool,"OpenAI's Superalignment team developed the Transformer Debugger (TDB) tool for investigating specific behaviors of small language models. This tool integrates automated interpretability techniques and sparse autoencoders to examine why models output certain tokens and attend to specific prompts. It exposes neural, attention head, and autoencoder latent components that contribute to the behavior, providing an exploration platform before the need for coding.","Discover the capacities of OpenAI's Transformer Debugger (TDB). Understand small language model behaviors and core components by probing automated interpretability and sparse autoencoders, assisting in AI behavior investigation and explanation generation.",Language Models,"Python





        2,779





        133


        Built by

          









        426 stars today",,,2780,2024-03-11T23:06:25Z
2024-03-14,https://github.com/princeton-nlp/SWE-bench,https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/README.md,"SWE-bench, introduced in a paper for ICLR 2024, is a benchmark for evaluating large language models on software issues taken from GitHub. Given a codebase and a related issue, a language model aims to generate a solution. Users can train their own models on pre-processed datasets, use SWE-bench with models they have access to, evaluate these models, and run SWE-bench data collection on their own repositories. This tool has been developed by Princeton-NLP, and can be used under the MIT license. The researchers behind it invite the broader NLP, Machine Learning, and Software Engineering research communities to contribute to its development.",SWE-bench: Evaluating Language Models on Real-World GitHub Issues,"SWE-bench presents a unique benchmark for gauging large language model's effectiveness in resolving real-world software issues from GitHub. This language model attempts to generate a patch, given a codebase and an issue. Explore the in-depth setup, usage, downloads, and tutorials on our page. Feedback and contributions from the NLP, Machine Learning, and Software Engineering research communities are welcomed and appreciated.","Join us in resolving real-world GitHub issues using large language models with SWE-bench. Being an interactive part of the exploration, from setting it up to evaluating the results, and making a difference!",Language Models,"Python





        457





        38


        Built by

          









        94 stars today",https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/assets/swellama_banner.png; https://raw.githubusercontent.com/princeton-nlp/SWE-bench/main/assets/teaser.png,,457,2023-10-04T01:22:46Z
2024-03-14,https://github.com/Ableton/maxdevtools,https://raw.githubusercontent.com/Ableton/maxdevtools/main/README.md,"The repository includes resources and tools used by Ableton in developing and maintaining Max devices. These include Max for Live Production Guidelines, Ableton's patch code standard and maxdiff for readable git diffs for Max devices and Max patch files. These resources are mainly for experienced Max for Live builders. For beginners, they are advised to check out the Building Max Devices Pack. The maintainer of these resources is Mattijs Kneppers.",Max Device Development Tools: Essential Guide for Ableton Users,"This blog post provides a comprehensive collection of development tools for Max devices used at Ableton, beneficial primarily for experienced Max for Live builders. It includes Ableton's patch code standard, Max for Live production guidelines and a unique tool, 'maxdiff', for getting readable git diffs for Max devices.","Explore the essential Max device development tools used by Ableton. Understand the Ableton's Max for Live production guidelines, patch code standard, and learn about 'maxdiff' tool, perfect for experienced Max builders.",Software Development,"Python





        187





        8


        Built by

          






        71 stars today",,,187,2023-12-08T11:13:13Z
2024-03-14,https://github.com/pytorch/pytorch,https://raw.githubusercontent.com/pytorch/pytorch/main/README.md,"PyTorch is a Python package featuring tensor computation (similar to NumPy) with strong GPU acceleration and deep neural networks developed on a tape-based autograd system. It allows utilizing favorite Python packages like NumPy, SciPy, and Cython. Pytorch offers features like a GPU-ready tensor library, dynamic neural networks through tape-based autograd, flexibility via Python integration, and quick operations. It is used as a replacement for NumPy to leverage GPU power and a deep learning research tool for maximum flexibility and speed. The website includes comprehensive instructions on PyTorch's installation, extending its functionalities, building documentation, and communication means for users.
",Deep Dive into PyTorch: Python-first GPU Accelerated Deep Learning,"Discover the secrets of PyTorch, a Python-first deep learning platform. Learn about its powerful GPU support, dynamic neural networks, integration with Python packages, and more.",This blog post explores PyTorch - a Python package with tensor computation and dynamic deep neural networks on GPUs. Utilize Python packages to extend PyTorch performance.,Deep Learning Platform,"Python





        76,510





        20,679


        Built by

          









        59 stars today",https://github.com/pytorch/pytorch/blob/main/docs/source/_static/img/pytorch-logo-dark.png; https://raw.githubusercontent.com/pytorch/pytorch/main/./docs/source/_static/img/tensor_illustration.png; https://github.com/pytorch/pytorch/blob/main/docs/source/_static/img/dynamic_graph.gif,,76510,2016-08-13T05:26:41Z
2024-03-14,https://github.com/pyodide/pyodide,https://raw.githubusercontent.com/pyodide/pyodide/main/README.md,"Pyodide is a Python distribution for web browsers and Node.js, built on WebAssembly. Created in 2018 by Michael Droettboom at Mozilla, Pyodide allows users to install and run Python packages in the browser, supporting both pure Python packages and those with C extensions. It features a robust Javascript-Python interface, enabling mixed-language coding, and gives Python full access to Web APIs when used in a browser. Pyodide is open source and contributions are encouraged. It also has a try-before-you-commit REPL for user convenience and can be used in three ways to suit different needs, including hosted distribution and building from source.
",The Ultimate Guide to Pyodide: Python Distribution for the Browser and Node.js,"Discover Pyodide, a port of CPython to WebAssembly that makes it possible to run Python packages in the browser and Node.js. Learn about its features, installation methods, and how it can seamlessly integrate Python and Javascript code.","A comprehensive overview of Pyodide - a Python distribution for the browser and Node.js based on WebAssembly. Learn how to get started, its key features, and the benefits of integrating Python and JavaScript code.",Python Web Development,"Python





        11,145





        751


        Built by

          









        6 stars today",https://raw.githubusercontent.com/pyodide/pyodide/main/./docs/_static/img/pyodide-logo-readme.png; https://circleci.com/gh/pyodide/pyodide.png,,11145,2018-02-23T19:21:29Z
2024-03-14,https://github.com/arcee-ai/mergekit,https://raw.githubusercontent.com/arcee-ai/mergekit/main/README.md,"Charles Goddard, author of `mergekit`, a toolkit for merging pre-trained language models, has joined arcee.ai. He will continue developing `mergekit` with arcee.ai's backing and maintain its open source nature. `mergekit` can perform comprehensive merges using a minimalistic CPU or just 8GB of VRAM, supporting various platforms like Llama, Mistral, and GPT-NeoX, among others. It also allows the piecewise assembly of language models from layers. Once a merge is completed, users can upload and share it on Hugging Face Hub. `mergekit` supports various model merging methods like linear, SLERP, task arithmetic, TIES, DARE, and passthrough.
",Maximize Machine Learning Efficiency with Mergekit: A Comprehensive Guide,"Become an expert in using `mergekit` and enhance your machine learning endeavors. Learn tips and tricks about its installation, usage and key features like lazy loading of tensors, CPU or GPU execution, and supporting numerous merging algorithms. Explore how to effectively leverage pre-trained language models while facing resource-constrained situations.",Unlock the true potential of pre-trained language models with our guide to `mergekit`. Experience seamless integration under resource-constrained situations with an array of merging methods supported. Find out more!,Language Models,"Python





        2,571





        201


        Built by

          









        34 stars today",,,2572,2023-08-21T03:50:04Z
2024-03-14,https://github.com/materialsproject/pymatgen,https://raw.githubusercontent.com/materialsproject/pymatgen/main/README.md,"Pymatgen (Python Materials Genomics) is an open-source Python library used for materials analysis. It provides flexible classes for representing elements, molecules, and structures as well as extensive input/output support for multiple file formats. This tool offers powerful analysis features like phase diagrams, Pourbaix diagrams, diffusion analyses, and more. Pymatgen is well documented and reliable, and used by a significant number of researchers along with the Materials Project, which survives rigorous scrutiny consistently. It's open for contributions from the user community, boosting visibility and impact of their research. The library is also optimized for speed and continues to evolve with a growing ecosystem of developers and add-ons.",Pymatgen: Your Comprehensive Guide to Python Materials Genomics Library,"Pymatgen is a robust, open-source Python library used for materials analysis. It offers flexible classes for element, site, molecule, and structure representation with extensive input/output support. Pymatgen includes powerful tools for analysis and electronic structure analyses along with seamless integration with the Materials Project REST API.","Explore the world of materials analysis with Pymatgen, a robust, open-source Python library. Offering a wide range of features including class representation, analysis tools, the ability to integrate with the Materials Project REST API and more, it is a must-have for anyone in the materials genomics field.",Open Source Tool,"Python





        1,316





        809


        Built by

          









        14 stars today",,,1316,2011-10-23T04:10:39Z
2024-03-14,https://github.com/PygmalionAI/aphrodite-engine,https://raw.githubusercontent.com/PygmalionAI/aphrodite-engine/main/README.md,"Aphrodite is the official backend engine for PygmalionAI, designed to offer fast inference speeds to numerous users. It features continuous batching, efficient K/V management, optimized CUDA kernels, quantization support, distributed inference, and a variety of sampling methods. It supports varied applications through different options for LLM inference. Aphrodite's development compute capacity is provided by Arc Compute and it builds upon the work of various other projects. It is designed to consume 90% of a GPU's VRAM and can limit memory utilization based on the server's launch parameters. Aphrodite's installation and usage guidelines are available on their Wiki page.",Introducing Aphrodite: The Powerful Backend Engine for PygmalionAI,"Aphrodite is the official backend engine for PygmalionAI, designed to serve as the inference endpoint for the PygmalionAI website. It is equipped with advanced features such as continuous batching and efficient K/V management. It boasts of optimized CUDA kernels for better inference along with a variety of sampling methods. Aphrodite ensures faster context lengths and throughput, thanks to the 8-bit KV Cache. It provides easy deployment with Docker support.","Unveiling Aphrodite, the backend engine for PygmalionAI, delivering fast injection points, continuous batching, efficient K/V management, and optimized CUDA kernels for improved inference. Explore the power of Aphrodite for high-speed AI operations.",Artificial Intelligence,"Python





        376





        53


        Built by

          









        8 stars today",https://raw.githubusercontent.com/PygmalionAI/aphrodite-engine/main/assets/aphrodite.png; https://raw.githubusercontent.com/PygmalionAI/aphrodite-engine/main/assets/bsz1.png,,376,2023-06-23T02:34:29Z
2024-03-15,https://github.com/lavague-ai/LaVague,https://raw.githubusercontent.com/lavague-ai/LaVague/main/README.md,"LaVague is an AI-based tool that automates browser interactions using natural language instructions. It mainly targets automating menial tasks, freeing up time for more important activities. LaVague turns natural language queries into Selenium code, simplifying automation for users and other AI. It is useful for automating personal tasks requiring login, such as paying bills, filling out forms, or extracting data from specific websites. Built on open-source projects, LaVague believes in transparency and alignment with user interests. It integrates various features, including natural language processing, selenium integration for browser automation, open-source elements for transparency, and advanced AI techniques for improved efficiency. Interested individuals can contribute to this early-stage project.",Exploring LaVague: Revolutionizing Web Interactions with Natural Language Processing,"Welcome to LaVague, an innovative platform redefining internet surfing by translating natural language instructions into seamless browser interactions. Discover LaVague's outstanding features including natural language processing, Selenium integration, open-source base, and advanced AI techniques. Examples of LaVague in action reveal how it interacts with various websites, automating menial tasks and web workflows. Aimed at creating more time for meaningful endeavors, LaVague strives to align with users' interests and guarantees privacy via local models. Get started with LaVague through easy-to-follow, accessible routes.","An introduction to LaVague, an advanced AI tool leveraging natural language processing to automate browser interactions. Discover its unique features and learn how to get started.",AI Browser Automation,"Python





        2,180





        154


        Built by

          







        1,086 stars today",https://raw.githubusercontent.com/lavague-ai/LaVague/main/static/logo.png; https://raw.githubusercontent.com/lavague-ai/LaVague/main/static/hf_lavague.gif; https://raw.githubusercontent.com/lavague-ai/LaVague/main/static/irs_lavague.gif,,2181,2024-02-26T23:40:23Z
2024-03-15,https://github.com/tiangolo/fastapi,https://raw.githubusercontent.com/tiangolo/fastapi/master/README.md,"FastAPI is an advanced, high-performance framework for building APIs with Python 3.8+ that's based on standard Python type hints. Its key features include being high performance, quick for feature development, having fewer bugs, being easy to use and intuitive, and offering great editor support. FastAPI additionally includes support for OAuth2 with JWT tokens and HTTP Basic auth, WebSocket and dependency injection, among many others. FastAPI is also one of the fastest Python frameworks available according to TechEmpower benchmarks. This project is licensed under the terms of the MIT license.",Mastering FastAPI: High-Performance Python Framework for APIs,"Discover FastAPI, a modern, fast, web-based framework for building APIs with Python 3.8+. Learn how to leverage its key features to develop features rapidly, reduce bugs, and get production-ready code. Plus, it's based on open standards for APIs and is designed to be easy to use and learn.","A comprehensive guide to using FastAPI, a high-performance Python framework for building APIs. Learn how to develop features quickly, reduce bugs, and get your code production-ready.",Python Web Development,"Python





        69,328





        5,797


        Built by

          








        54 stars today",https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png; https://fastapi.tiangolo.com/img/sponsors/platform-sh.png; https://fastapi.tiangolo.com/img/sponsors/porter.png; https://fastapi.tiangolo.com/img/sponsors/reflex.png; https://fastapi.tiangolo.com/img/sponsors/propelauth.png; https://fastapi.tiangolo.com/img/sponsors/coherence.png; https://fastapi.tiangolo.com/img/sponsors/talkpython-v2.jpg; https://fastapi.tiangolo.com/img/sponsors/speakeasy.png; https://fastapi.tiangolo.com/img/sponsors/codacy.png; https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png; https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png; https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png; https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png; https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png; https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png; https://fastapi.tiangolo.com/img/vscode-completion.png,,69328,2018-12-08T08:21:47Z
2024-03-15,https://github.com/Project-MONAI/MONAI,https://raw.githubusercontent.com/Project-MONAI/MONAI/main/README.md,"The Medical Open Network for AI (MONAI) is a PyTorch-based, open-source framework developed for deep learning in healthcare imaging. Its objectives include fostering a community of academic, industrial, and clinical researchers to collaborate on a common foundation, creating advanced training workflows for healthcare imaging, and providing researchers with an optimized way to create and evaluate deep learning models. MONAI offers pre-processing for multi-dimensional medical imaging data, domain-specific implementations for networks and metrics, a customizable design, and multi-GPU support. Examples and tutorials are available for getting started and a place for sharing models is provided in the MONAI Model Zoo.",Unlock the Future of Healthcare with MONAI: An Open Network for Medical AI,"Explore MONAI, a PyTorch-based, open-source platform for deep learning in healthcare imaging. Designed for collaboration between academic, industrial, and clinical researchers, MONAI offers state-of-the-art training workflows and optimized model creation and evaluation.","Delve into the capabilities of MONAI - an open-source network for AI in healthcare with state-of-the-art training workflows, multi-dimensional imaging data preprocessing, and multi-GPU support. Become part of the future of healthcare imaging.",Deep Learning Framework,"Python





        5,141





        945


        Built by

          









        5 stars today",https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/docs/images/MONAI-logo-color.png,,5141,2019-10-11T16:41:38Z
2024-03-15,https://github.com/luijait/DarkGPT,https://raw.githubusercontent.com/luijait/DarkGPT/main/README.md,"DarkGPT is an AI assistant designed to perform queries on leaked databases. To install, ensure Python (version 3.8 or higher) is on your system. Clone the DarkGpt repository from GitHub and set up environment variables by copying the '.env.example' file to a new file named '.env'. After specifying your 'dehashed_api_key' in it, install the Python packages required with 'pip install -r requirements.txt'. Finally, run the project via 'python3 main.py'.",Step by Step Guide: How to Install DarkGPT Project | AI Assistant Setup,"Learn how to install DarkGPT, an AI assistant project based on GPT-4-200K. This guide provides a simple step-by-step process to get DarkGPT running in your local environment, covering prerequisites, environment setup, cloning the repository, setting environment variables, installing necessary dependencies, and finally, running the project.","This blog post provides a comprehensive guide on installing the DarkGPT project, an AI assistant based on GPT-4-200K. Learn how to set up your local environment, clone the repository, configure variables, and install dependencies.",AI Coding Assistant,"Python





        110





        15


        Built by

          





        35 stars today",https://i.imgur.com/bYW6pai.jpg,,110,2024-03-12T10:46:45Z
2024-03-15,https://github.com/phospho-app/phospho,https://raw.githubusercontent.com/phospho-app/phospho/main/README.md,"Phospho is a text analytics platform primarily intended for language learning model (LLM) applications. The platform allows users to detect issues and extract insights from text messages. Included within Phosphoâ€™s key features are flexible logging, automated evaluation, insights extraction, data visualization, and collaboration. For utilization, Phospho offers both a self-deploy option and access to a hosted version. The self-deploy option involves a set of steps to configure the platform via Docker, while the hosted version simply requires user registration. The platform also provides pre-generated Codes, and software packages for ease of use. The project is Apache 2.0 licensed.",Unleash your App's Potential with Phospho: The Text Analytics Platform for LLM Apps,"Phospho is the go-to text analytics platform for LLM apps. It detects issues and extracts insights from text messages, aids in gathering user feedback, and is instrumental in measuring success. The platform allows you to iteratively improve your app and create the best conversational experience for users. Confidence in shipping your LLM app in production is assured with Phospho. Step into the world of informed and data-driven app development with Phospho.","Utilize Phospho, an advanced text analytics platform, for your LLM apps. Identify issues, extract insights, receive user feedback, measure success, and improve the conversational experience on your app.",Language Learning Platform,"Python





        175





        8


        Built by

          









        25 stars today",https://raw.githubusercontent.com/phospho-app/phospho/main/./platform/public/image/phospho-banner.png; https://raw.githubusercontent.com/phospho-app/phospho/main/./phospho_diagram.png,,175,2023-07-21T14:47:27Z
2024-03-16,https://github.com/Skyvern-AI/skyvern,https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/README.md,"Skyvern is a platform used to automate browser-based workflows using LLMs (Large Language Models) and computer vision. Unlike traditional scripting methods which can be impaired by website layout changes, Skyvern uses visual elements and reasoning capabilities to interact with and navigate websites, even unvisited ones before. This makes it more resilient to website layout differences and is able to handle complex interactions. It can also deal with tasks such as materials procurement for a manufacturing company, registering accounts on government websites or retrieving insurance quotes from providers in any language. Useful for users who wish to automate manual workflows.",Automate Browser Workflows with Skyvern: Leveraging LLMs & Computer Vision,"Skyvern provides a new approach to automate browser-based workflows using LLMs and computer vision. Unlike traditional methods, Skyvern can operate on websites it has never seen before and is resistant to website layout changes. It uses LLMs to reason through interactions, covering complex situations without customized code.",Discover how Skyvern can help you automate browser-based workflows using LLMs and computer vision. Learn how our approach can adapt to different websites and handle complex situations with ease.,AI Browser Automation,"Python





        1,594





        67


        Built by

          








        367 stars today",https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/skyvern_logo_blackbg.png; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/geico_shu_recording_cropped.gif; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/skyvern-system-diagram-light.png; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/skyvern_visualizer_run_task.png; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/skyvern_visualizer_debug_llm_response.png; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/skyvern_visualizer_debug_action_screenshot.png; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/finditparts_recording_crop.gif; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/edd_services.gif; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/bci_seguros_recording.gif; https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/images/geico_shu_recording_cropped.gif,,1594,2024-02-28T15:45:19Z
2024-03-16,https://github.com/facefusion/facefusion,https://raw.githubusercontent.com/facefusion/facefusion/main/README.md,"FaceFusion is a next-generation face swapper and enhancer tool that leverages complex algorithms and models to process images or audio. The software requires technical skills for installation. It runs various commands for different functions like adjusting message severity, specifying parallel threads for processing, detecting and analysing faces, selecting face masks, trimming frames in video and modulating output creation among other tasks. The tool has an extensive documentation and a supportive Discord community for assistance. It also offers options for user-interface layouts. Although powerful, it's not recommended for beginners due to its complexity.",FaceFusion: Exploring the Next Generation Face Swapper and Enhancer,"Discover FaceFusion, the next-gen solution for face swapping and enhancement. With unprecedented detail and precision, this software transforms your face swapping experience. Technical know-how is recommended for the installation process. Join our Discord community for comprehensive guidance or dive into our thorough documentation for further information.","Unleash your creativity with FaceFusion, a cutting-edge face swapper and enhancer. From installation guides to user manuals, everything you need is right here. Join the revolution of face swapping today.",Image Generation Platform,"Python





        12,844





        1,703


        Built by

          









        20 stars today",,,12844,2023-08-17T19:59:55Z
2024-03-16,https://github.com/huggingface/safetensors,https://raw.githubusercontent.com/huggingface/safetensors/main/README.md,"The Safetensors library allows safe storage of tensors. This open-source tool, developed by Hugging Face, can be installed through pip or from source, and it supports both Python and Rust. The Safetensors format provides benefits such as preventing DOS attacks, enabling faster loading times, and allowing lazy loading, which is crucial in distributed settings. The library is an alternative to formats like H5, Pickle, and SavedModel, offering advantages such as safety, zero-copy reading, no file size limit, layout control, and support for features like bfloat16/fp8. The functions and usage of Safetensors are fully documented.",Exploring Safetensors: A Comprehensive Guide to Storing Tensors Safely,"The blog delves into the new safetensors repository that presents a novel, secure way of storing tensors. It outlines the process of installation, getting started, the format details, and compares safetensors with other formats for a broader view of its utilities. The post also sheds light on some of the added benefits including prevention of DOS attacks and faster loading. The safetensors repository implements a new tensor storing format that balances safety and speed.","A detailed guide to the Safetensors repository for secure and fast storing of tensors. Discover its installation steps, comparisons with other formats, and benefits like DOS attacks prevention and speedy loading.",Open Source Tool,"Python





        2,221





        135


        Built by

          









        12 stars today",,,2221,2022-09-22T14:54:51Z
2024-03-16,https://github.com/zurdi15/romm,https://raw.githubusercontent.com/zurdi15/romm/master/README.md,"RomM is a self-hosted ROM manager that allows users to enrich and browse their game collection. The platform supports multiple emulators, naming schemes and even includes custom tags. RomM version 3.0 brings updated features and requires changes to the setup and configuration. It enhances your games library with metadata from IGDB and supports various platforms. It also includes built-in authentication and parsing of tags in filenames. It offers community projects for users and information resources including a wiki page for troubleshooting and other guides. However, users of version 2.x are requested to read migration guide before upgrading.
",Unveiling RomM: Your Ultimate Self-Hosted Rom Manager,"Discover RomM, the powerful self-hosted Rom manager, that enhances your gaming experience on emulators. Version 3.0 introduces exciting new features making it a must-have for gaming enthusiasts. Stay tuned for insights on setup, configuration, features, and much more.","Step into the world of RomM, an efficient self-hosted Rom Manager, and redefine your gaming experience. Explore the multi-platform support, custom tags, and new features in the latest 3.0 version.",Game Development Tool,"Python





        1,014





        46


        Built by

          









        17 stars today",https://raw.githubusercontent.com/zurdi15/romm/master/.github/resources/screenshots/romm-desktop-slider.gif; https://raw.githubusercontent.com/zurdi15/romm/master/.github/resources/screenshots/romm-mobile-slider.gif,,1014,2023-03-08T16:11:01Z
2024-03-16,https://github.com/OpenBMB/MiniCPM,https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/README.md,"MiniCPM, developed by Mianbi Intelligent and Tsinghua University's Natural Language Processing Laboratory, is a series of large-scale models open-sourced for endpoint use. Its main language model, MiniCPM-2B, outperformed models like Llama2-13B, MPT-30B, and Falcon-40B in a comprehensive public evaluation set after Smart ForTwo (SFT) and Dynamic Partial Order (DPO) treatment. The multi-modal large model, MiniCPM-V, built on MiniCPM-2B, achieved top performance of the same scale models. MiniCPM was quantified into Int4 and can be deployed for inference on mobile devices. The cost of secondary development is relatively low as high-efficiency parameter fine-tuning can be efficiently achieved on a single graphic card and full parameter fine-tuning on a single machine.
",Exploring the Infinite Potential of MiniCPM: Fully Open-source End-Side Large Language Models,Discover the unlimited potential of MiniCPM: the series of end-side large language models collaboratively open-sourced by BMB Intelligence and Tsinghua University. Immerse in the superior performance of MiniCPM-2B with total 2.7B parameters and explore its versatility for academic research and commercial use.,"Unleashing the potential of MiniCPM, a series of end-side large language models, jointly open-sourced by BMB Intelligence and Tsinghua University. Dive into the capabilities of MiniCPM-2B and its prowess for academic and commercial applications.",Large-scale Language Models,"Python





        3,223





        218


        Built by

          









        2 stars today",https://github.com/OpenBMB/OmniLMM/blob/main/assets/Snake_cn_Mushroom_en.gif; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/creation.case1.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/creation.case2.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/creation.case3.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/code.case1.gif; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/code.case2.gif; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/math.case1.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/math.case2.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/translation.case1.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/translation.case2.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/instruction_following.case1.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/instruction_following.case2.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/special_char.case1.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM/main/./assets/special_char.case2.png,,3223,2024-01-29T08:21:15Z
2024-03-16,https://github.com/ZiqiaoPeng/SyncTalk,https://raw.githubusercontent.com/ZiqiaoPeng/SyncTalk/main/README.md,"SyncTalk is an innovative technology that synthesizes high-resolution, synchronized talking head videos for a more realistic experience. By using tri-plane hash representations, it can maintain the subject's identity while generating synchronized lip movements, facial expressions, and stable head poses. It also restores hair details for a more authentic feel. The methodology, launched under the paper ""SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis"", is available on its official repository along with the project's code. A usage guide ensures easy installation and application of the technology.",Exploring SyncTalk: Advanced Synchronization for Talking Head Synthesis [CVPR 2024],"Explore SyncTalk, an advanced technology that synthetizes talking head videos with high resolution maintaining the identity of the subject. It leverages tri-plane hash representation to genertate synchronized lip movements, facial expressions and head poses, whilst restoring hair details.","Unravel the functionality of SyncTalk: Learn how to install, prepare data, and use for generating high-quality synthesized talking head videos. SyncTalk promises accurate synchronization, maintaining subject identity, and high-resolution outputs.",Video Generation Tool,"Python





        498





        29


        Built by

          





        24 stars today",,,498,2023-11-29T14:32:24Z
2024-03-16,https://github.com/yl4579/StyleTTS2,https://raw.githubusercontent.com/yl4579/StyleTTS2/main/README.md,"The paper introduces StyleTTS 2, a model that creates human-level Text-to-Speech (TTS) synthesis using style diffusion and adversarial training with large speech language models. Unlike its predecessor, it models styles as a random variable using diffusion models, without needing reference speech. It uses large pretrained models like WavLM as discriminators, resulting in improved speech naturalness. The model performs excellently on both single-speaker and multi-speaker datasets, surpassing human recordings in certain cases. In addition, it outperforms previous models when trained on the LibriTTS dataset, thereby achieving zero-shot speaker adaptation.
",Human-Level Text-to-Speech: An Insight into StyleTTS 2 and Adversarial Training,"StyleTTS 2 leverages style diffusion and adversarial training with large speech language models to give a human-level text-to-speech (TTS) synthesis. It outperforms its predecessor by introducing a latent random variable that models styles through diffusion models. This model is even capable of surpassing human recordings in some instances. Trained on the LibriTTS dataset, StyleTTS 2 outperforms previous models in zero-shot speaker adaptation. The work is the first to achieve human-level TTS synthesis on both single and multispeaker datasets.","Discover StyleTTS 2, a cutting-edge text-to-speech model that uses style diffusion and adversarial training to enable human-level TTS synthesis. Learn how it surpasses previous models in zero-shot speaker adaptation and excels in both single-speaker and multispeaker datasets.",Natural Language Processing,"Python





        3,706





        253


        Built by

          









        6 stars today",,,3706,2023-06-14T00:48:11Z
2024-03-16,https://github.com/tiangolo/sqlmodel,https://raw.githubusercontent.com/tiangolo/sqlmodel/main/README.md,"SQLModel is a Python library designed for simplicity, compatibility, and robustness, enabling easy interaction with SQL databases through Python objects. It intuitively minimizes code duplication and simplifies written code, with autocomplete and other editing features for enhancing usability and learning. The library is compatible with FastAPI, Pydantic, and SQLAlchemy and is designed to be extensible with the underlying power of SQLAlchemy and Pydantic. The library was designed by the author of FastAPI for simplified SQL database interaction in FastAPI apps. SQLModel requires a supported Python version and automatically installs Pydantic and SQLAlchemy upon its installation.",Simplify Your Python-SQL Workflow with SQLModel,"SQLModel is an intuitive and robust library for Python-SQL interaction. This tool harnesses Python type annotations, Pydantic, and SQLAlchemy for unassailable compatibility, seamless learning, and ultimate user-friendliness. Use SQLModel to limit code repetition, simplify your work, and enhance your coding experience!","Harness the power of SQLModel for efficient interaction with SQL databases from Python. Experience better compatibility, simplicity, and robustness in your Python-SQL workflow. Learn more in this blog post.",Python Libraries Collection,"Python





        12,575





        556


        Built by

          








        13 stars today",https://sqlmodel.tiangolo.com/img/sponsors/govcert.png; https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png; https://sqlmodel.tiangolo.com/img/index/autocompletion01.png; https://sqlmodel.tiangolo.com/img/index/inline-errors01.png; https://sqlmodel.tiangolo.com/img/index/autocompletion02.png,,12575,2021-08-24T14:26:53Z
2024-03-16,https://github.com/naver-ai/Visual-Style-Prompting,https://raw.githubusercontent.com/naver-ai/Visual-Style-Prompting/main/README.md,"The paper ""Visual Style Prompting with Swapping Self-Attention"" addresses challenges in the field of text-to-image generation. The authors introduce a unique approach to generate a range of images that maintain specific style elements. Without fine-tuning, the process swaps key and value with those from reference features in late self-attention layers while keeping the query from original features. This technique ensures that the style in generated images is preserved. It has shown superiority over existing approaches in terms of reflecting the style of the references and matching the text prompts accurately for the generated images.
",Revolutionizing Text-to-Image Generation with Visual Style Prompting,"In the ever-evolving world of text-to-image generation, we introduce 'Visual Style Prompting with Swapping Self-Attention', a novel approach from NAVER AI lab and Yonsei University. This ground-breaking methodology ensures controlled generation of images, maintaining specific style elements and nuances for a consistent style. Our approach surpasses traditional routes, without necessitating expensive fine-tuning, and minimizing content leakage.","Discover the innovation in text-to-image generation with Visual Style Prompting. Explore the latest and most effective approach for controlled and consistent stylized image generation, with comprehensive insights into the methodology, demonstrations, and usage.",Image Generation Platform,"Python





        149





        8


        Built by

          





        27 stars today",https://raw.githubusercontent.com/naver-ai/Visual-Style-Prompting/main/./assets/git_image/teaser.png; https://raw.githubusercontent.com/naver-ai/Visual-Style-Prompting/main/./assets/git_image/vsp.png; https://raw.githubusercontent.com/naver-ai/Visual-Style-Prompting/main/./assets/git_image/vsp_control.png; https://raw.githubusercontent.com/naver-ai/Visual-Style-Prompting/main/./assets/git_image/vsp_real.png; https://raw.githubusercontent.com/naver-ai/Visual-Style-Prompting/main/./assets/git_image/attention_map.png,,149,2024-02-20T08:08:41Z
2024-03-16,https://github.com/graphdeco-inria/gaussian-splatting,https://raw.githubusercontent.com/graphdeco-inria/gaussian-splatting/main/README.md,"The researchers detail an improved method for real-time rendering of 3D radiance fields. They introduce 3D Gaussian Splatting, which enables high-quality image synthesis at a much faster speed, useful in VR and AR applications. The method uses 3D points derived from camera calibration and generates Gaussian volumes that map the scene's radiance field optimally. This approach avoids unnecessary computation, speeding up training and rendering times. The authors provide a detailed breakdown of training requirements, commands and FAQs, guiding users on how to use their method. They also provide links to datasets, viewer software, and a step-by-step tutorial.
",Exploring 3D Gaussian Splatting for Real-Time Radiance Field Rendering,"Delve into the groundbreaking world of '3D Gaussian Splatting for Real-Time Radiance Field Rendering' â€“ a revolutionary new approach elevating the standards of real-time rendering. Discover this technology's innovative solutions to existing challenges in neural networks, offering high visual quality while maintaining competitive training times","An in-depth exploration of '3D Gaussian Splatting for Real-Time Radiance Field Rendering', highlighting its key features including increased efficiency, exceptional visual quality, and competitive training times within neural networks.",3D Image Rendering,"Python





        10,484





        1,223


        Built by

          









        20 stars today",https://raw.githubusercontent.com/graphdeco-inria/gaussian-splatting/main/assets/teaser.png; https://raw.githubusercontent.com/graphdeco-inria/gaussian-splatting/main/assets/logo_inria.png; https://raw.githubusercontent.com/graphdeco-inria/gaussian-splatting/main/assets/logo_uca.png; https://raw.githubusercontent.com/graphdeco-inria/gaussian-splatting/main/assets/logo_mpi.png; https://raw.githubusercontent.com/graphdeco-inria/gaussian-splatting/main/assets/logo_graphdeco.png; https://raw.githubusercontent.com/graphdeco-inria/gaussian-splatting/main/assets/select.png,https://www.youtube.com/watch?v=T_kXY43VZnk; https://www.youtube.com/watch?v=UXtuigy_wYc; https://www.youtube.com/watch?v=UXtuigy_wYc,10484,2023-07-04T07:51:38Z
2024-03-16,https://github.com/sgl-project/sglang,https://raw.githubusercontent.com/sgl-project/sglang/main/README.md,"SGLang is a structured generation language for large language models (LLMs) aimed at making interactions with LLMs faster and more efficient. It's defined by two core features: A Flexible Front-End Language for easy programming of LLM applications and a High-Performance Runtime with RadixAttention to accelerate complex LLM program execution. SGLang has enabled 3x faster JSON decoding, powers the serving of the LLaVA v1.6 release demo, and provides up to 5x faster inference with RadixAttention. Installation is available through pip or source, and a quick start guide featuring examples of how to use SGLang's functions is provided.
",Exploring SGLang: A Structured Generation Language for Large Language Models,"SGLang, or Structured Generation Language, is a new tool for interacting with large language models. It offers a flexible front-end language and a high-performance runtime, accelerating language processing tasks and enhancing control. Discover how to install, use and optimize SGLang in this comprehensive guide.","A complete guide to understanding and using SGLang, a structured generation language designed to interact efficiently with large language models. Learn about key features, installation process, and different use cases of SGLang.",Large-scale Language Models,"Python





        1,893





        103


        Built by

          









        14 stars today",https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png; https://raw.githubusercontent.com/sgl-project/sglang/main/assets/llama_7b.jpg; https://raw.githubusercontent.com/sgl-project/sglang/main/assets/mixtral_8x7b.jpg,,1893,2024-01-08T04:15:52Z
2024-03-16,https://github.com/replicate/cog,https://raw.githubusercontent.com/replicate/cog/main/README.md,"Cog, an open-source tool, allows you to package machine learning models in a standard, production-ready container. It simplifies docker container creation, automatically sets up compatible CUDA/PyTorch/TensorFlow/Python combinations, defines model inputs/outputs using Python, and allows reading/writing directly to Amazon S3 and Google Cloud Storage. Cog generates an OpenAPI schema, validates inputs and outputs, and creates a RESTful HTTP API. It also creates an automatic queue worker for deep learning models and batch processing. It is production-ready and can be deployed to any infrastructure that runs Docker images. Contributors can use cog.yaml to define their Docker environment and setup model predictions.",Leveraging Cog for Machine Learning: Efficient Containerization and Deployment,"Explore the power of Cog, an open-source tool simplifying machine learning model packaging and deployment. Get to grips with its impressive features including Docker container support, compatibility handling, and automatic HTTP server generation for streamlined machine learning application production.",A comprehensive guide to using Cog for efficient machine learning model packaging and deployment. Uncover its impressive array of features designed to simplify your machine learning operations and streamline your production process.,Machine Learning Tool,"Python





        6,736





        454


        Built by

          









        5 stars today",,,6736,2021-02-26T23:43:09Z
2024-03-16,https://github.com/wbt5/real-url,https://raw.githubusercontent.com/wbt5/real-url/master/README.md,"The 'Real-Url' repository contains Python code to obtain real streaming media addresses (live sources) and bullet screen content from various live streaming platforms. It works successfully with players like PotPlayer, VLC, DPlayer, etc. Currently, it can get sources from 59 live platforms and bullet screen content from 18 platforms. The project uses simple Python code and runs only in a Python 3 environment. Users can reference the ""Real-Url"" in their projects. The code for each platform's live source and bullet screen feature is independent. Any issues with the platform or additional platform analysis can be raised on the given GitHub link.",Unlock True Streaming URLs & Comments with Real-Url Python Code,"Discover how Real-Url, a repository of Python codes, allows you to access real streaming media addresses and comments across various live platforms. Real-Url supports 59 channels for streaming and 18 channels for comments. Any queries about the program can be addressed through the provided issue link.",Unearth the true streaming media URLs from 59 live platforms and comments from 18 platforms using Real-Url. A versatile Python code solution that offers you real-time streaming insights.,Livestream Data Extraction,"Python





        6,988





        1,507


        Built by

          









        13 stars today",https://i.loli.net/2020/10/03/E4h5FZmSfnGIgap.png,,6988,2019-09-18T17:26:42Z
2024-03-17,https://github.com/amazon-science/chronos-forecasting,https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/README.md,"Chronos is a set of pretrained time series forecasting models using language model architectures. Time series data is turned into a token sequence, which the language model is trained on using the cross-entropy loss. The Chronos models are probabilistic, creating forecasts by sampling multiple future trajectories from the historical context. These models are based on the T5 architecture, but use less different tokens, resulting in fewer parameters. Chronos models have been trained on publicly available time series data and synthetic data. The models can be installed and used via a Python package and used in time series forecasting tasks.
",Chronos: Mastering Time Series Forecasting with Pretrained Models,"Chronos offers pretrained time series forecasting models, transforming a time series into a tokens sequence and training via cross-entropy loss. This method allows for multiple future trajectories prediction. Chronos models rely on a huge corpus of publicly available time series data and synthetic data, generated through Gaussian processes.","Discover Chronos, a novel tool offering pretrained time series forecasting models. Learn how time series is transformed into sequences and used in language model training. Ideal for predicting multiple future trajectories.",Time Series Forecasting,"Python





        367





        62


        Built by

          









        33 stars today",https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/figures/main-figure.png,,367,2024-02-23T10:35:42Z
2024-03-17,https://github.com/BerriAI/litellm,https://raw.githubusercontent.com/BerriAI/litellm/main/README.md,"LiteLLM is a tool used to manage APIs like Bedrock, Huggingface, VertexAI, TogetherAI, Azure, and OpenAI using the OpenAI format. It allows users to translate inputs to provider's endpoints, ensures consistent output, manages retry/fallback logic across multiple deployments, and also lets users set budgets and rate limits per project, API key, and model. LiteLLM supports several providers and offers an easy setup procedure. It includes features like async completion, streaming, setting up the proxy, and much more. Additional resources include the documentation for better functionality understanding, usage, migration guides, and quick installation steps using pip.",Mastering LiteLLM: A Comprehensive Guide to Universal API Management,"Discover the streamlined capabilities of LiteLLM to manage LLM APIs using the OpenAI format including Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, and more. Learn to leverage its features to translate inputs, set project budgets, manage API keys and much more.","Simplify API management with LiteLLM. Gain insights into using this tool for varying requirements such as translating inputs for all API providers, establishing API project budgets, rate limit management, and more.",API Management Tool,"Python





        6,233





        668


        Built by

          









        31 stars today",,,6233,2023-07-27T00:09:52Z
2024-03-17,https://github.com/cbh123/narrator,https://raw.githubusercontent.com/cbh123/narrator/main/README.md,"The text includes instructions to create an artificial intelligence application using APIs from Replicate, OpenAI, and ElevenLabs. Users need to clone a repository, set up a virtual environment, and install dependencies. After creating accounts on Replicate, OpenAI, and ElevenLabs and obtaining necessary API keys, they can create a new voice on Eleven and retrieve its ID. The final steps involve running the webcam capture and the narrator by executing specific Python scripts. The final product seems to narrate your life as if it was done by David Attenborough.
",Experience Life Narrated by David Attenborough: A Guide to Create an AI App,"Explore how to integrate AI into apps and imagine life narrated by the iconic voice of David Attenborough. Learn how to create your AI app using Replicate, OpenAI, and ElevenLabs. This guide covers everything from setting up your environment to running your AI model.","In this blog post, learn how to make your AI app using resources like Replicate, OpenAI, and ElevenLabs. Start experiencing life with the voice of David Attenborough with our step-by-step guide.",AI Coding Assistant,"Python





        3,958





        470


        Built by

          









        63 stars today",,,3958,2023-11-14T20:10:10Z
2024-03-17,https://github.com/Kanaries/pygwalker,https://raw.githubusercontent.com/Kanaries/pygwalker/main/README.md,"PyGWalker is a Python library designed for simplifying data analysis and visualization workflow in Jupyter Notebook, by transforming a pandas dataframe into an interactive UI for visual exploration. Named as a playful abbreviation for ""Python binding of Graphic Walker"", PyGWalker integrates Jupyter Notebook with Graphic Walker, an open-source alternative to Tableau. It allows data scientists to visualize, clean, and annotate the data using simple drag-and-drop functions and natural language queries. PyGWalker also provides examples in local and cloud notebooks and is compatible with many tested environments like Google Colab or Kaggle Code.
",Exploring PyGWalker: A Python Library for Data Analysis and Visualization,"Meet PyGWalker, a Python library specially designed for exploratory data analysis with visualization. Discover how PyGWalker simplifies your Jupyter Notebook data workflows by effortlessly converting your pandas dataframe into an interactive visual exploration tool.","This post introduces PyGWalker, a Python Library that redefines data analysis in Jupyter Notebook, turning pandas dataframes into an intuitive, interactive visual exploration interface.",Data Science Tool,"Python





        9,213





        433


        Built by

          









        40 stars today",https://docs-us.oss-us-west-1.aliyuncs.com/img/pygwalker/kaggle.png; https://docs-us.oss-us-west-1.aliyuncs.com/img/pygwalker/colab.png; https://docs-us.oss-us-west-1.aliyuncs.com/img/pygwalker/travel-ani-0-light.gif; https://docs-us.oss-us-west-1.aliyuncs.com/img/pygwalker/travel-ani-1-light.gif; https://user-images.githubusercontent.com/8137814/221894699-b9623304-4eb1-4051-b29d-ca4a913fb7c7.png; https://user-images.githubusercontent.com/8137814/224550839-7b8a2193-d3e9-4c11-a19e-ad8e5ec19539.png; https://user-images.githubusercontent.com/8137814/221894480-b5ec5df2-d0bb-45bc-aa3d-6479920b6fe2.png; https://user-images.githubusercontent.com/22167673/271170853-5643c3b1-6216-4ade-87f4-41c6e6893eab.png,,9213,2023-02-16T05:17:24Z
2024-03-17,https://github.com/Hillobar/Rope,https://raw.githubusercontent.com/Hillobar/Rope/main/README.md,"Rope is a face-swapping software implementing the insightface inswapper_128 model with a user-friendly GUI. It offers features such as face swapping, upscalers, likeness modifiers, orientation management, mask options, source face merging, and auto save filename generation. Additionally, it allows users to swap images or videos and provides real-time video play with segment recording capabilities. The software had several updates that fixed bugs and added more convenience features. However, with this technology available, the developers implore the users to use it responsibly while respecting privacy, and complying with ethical guidelines and legal considerations.",Rope Implements Insightface Inswapper_128 Model: Enhancing GUI Experience,"Rope now incorporates the insightface inswapper_128 model, enhanced with a user-friendly graphical user interface (GUI). Experience lightning speed face swapping, auto-save filename generation, real-time playing and fine-tuning video capabilities. Rope-Opal update presents a plethora of new features, improvements and bug fixes. Be aware of the stringent ethical guidelines this software adheres to, promoting responsible and ethical use. Join our discord for more information and connect with other users.","Explore the latest advancements Rope brings with Insightface Inswapper_128 model implementation. Enjoy lightning speed swapping, enhanced features, and upgrades focusing on responsible usage of this face-swapping software.",Computer Vision Platform,"Python





        3,005





        428


        Built by

          





        6 stars today",,https://www.youtube.com/watch?v=4Y4U0TZ8cWY,3005,2023-07-07T20:43:52Z
2024-03-17,https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI,https://raw.githubusercontent.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/main/README.md,"The Retrieval-based-Voice-Conversion-WebUI repository provides a straightforward voice conversion framework based on VITS. Even on a relatively poor graphics card, the system can be trained rapidly. It requires minimal data, supports merging models to alter voice tones, and incorporates efficient vocal pitch extraction algorithm InterSpeech2023-RMVPE. The model is trained using an open-source VCTK training set with no copyright issues. The repository includes detailed installation guides for operating systems and instructions on how to download and use pre-trained models. There are links to online demonstrations and the platform supports multiple languages.
",Voice Conversion made Easy with Retrieval-based-Voice-Conversion-WebUI,"Providing an innovative, top-notch Voice Conversion Framework using VITS, the Retrieval-based-Voice-Conversion-WebUI offers optimized transformation capabilities. The feature-rich framework ensures fast training even on average graphic cards and offers the convenience of using little data for training with good results. Apart from its simple-to-use web interface, it also takes advantage of the state-of-the-art vocal pitch extraction algorithm, thereby eliminating the dumb sound problem.",Learn how to effectively change the voice tone using the Retrieval-based-Voice-Conversion-WebUI based on VITS. Get optimum results even with limited data and GPU resources. Experience hassle-free voice conversion with its user-friendly interface and advanced algorithms.,Voice Conversion Tool,"Python





        17,399





        2,727


        Built by

          









        31 stars today",,,17399,2023-03-27T09:59:10Z
2024-03-17,https://github.com/bmaltais/kohya_ss,https://raw.githubusercontent.com/bmaltais/kohya_ss/master/README.md,"Kohya's GUI repository provides Gradio GUI for Kohya's Stable Diffusion trainers mainly for Windows and Linux, with partial Mac support. It provides a platform for setting training parameters, generating required CLI commands, and training the model. Instructions for installation on Windows, Linux, and Runpod are included, along with optional contributions for Linux and Runpod git docker builds and manual installation. The GUI allows you to train LoRA and generate sample images during training. The document also provides troubleshooting tips and change history. Information on image configurations, GUI service launch, and parameter settings is also included.",Exploring Kohya's GUI: A Comprehensive Guide,"Get to grips with Kohya's GUI, an interface offering users a more simple way of setting parameters for Stable Diffusion trainers. This blog post delves into how to navigate the tool, offering support for Linux OS contributions and the potential for MacOs compatibility. Find out more now!","A detailed guide to using Kohya's GUI, the Gradio GUI for Stable Diffusion trainers. Learn how to set parameters, run CLI commands and get support for Linux OS.",AI Training Platform,"Python





        7,895





        1,014


        Built by

          









        16 stars today",,,7895,2022-10-30T15:15:32Z
2024-03-17,https://github.com/philz1337x/clarity-upscaler,https://raw.githubusercontent.com/philz1337x/clarity-upscaler/main/README.md,"The High Resolution Image Upscaler is a tool designed to enhance the quality of images by increasing their resolution. It utilises advanced algorithms to extrapolate the additional detail, creating high-definition images from lower resolution inputs. The technology fills in missing information to produce clear, detailed results. This tool is valuable for a range of applications, including photography, design and video production. It can improve the quality of images significantly, making them clearer, sharper, and more detailed. It's a potent tool for those where image quality matters, such as photographers, designers, and content creators.",Boost Your Visual Content with High Resolution Image Upscaler,Discover the wonders of high-resolution image upscale. Transform your low-quality images into high definition versions. Unleash the full potential of your visual content with the cutting-edge technology.,Learn how to enhance your images with high-resolution upscaling. Turn blurry and low-quality images into stunning high-definition visuals. Perfect for professionals and graphic enthusiasts alike.,Image Generation Platform,"Python





        831





        87


        Built by

          





        282 stars today",,,831,2024-03-15T15:21:40Z
2024-03-17,https://github.com/fofr/cog-face-to-many,https://raw.githubusercontent.com/fofr/cog-face-to-many/main/README.md,"The 'face-to-many' model transforms faces into 3D, pixel art, video game, claymation, or toy formats. It can be run on the Replicate or ComfyUI platforms, and requires specific custom nodes for operation. The 3D, video game, pixel art, claymation and toy ""loras"" are creations of 'artificialguybr', who accepts donations via Patreon and Ko-fi. You can develop locally by cloning the repository and running a script to install all custom nodes. The web UI can be run from a Cog container by following a series of steps to start the server and access it via the GPU machine's IP.","Transform Faces to 3D, Pixel Art and More with Face-to-Many","Explore the capabilities of Face-to-Many, a model that allows you to convert any face into 3D, pixel art, video game, claymation, or toy. Discover how to run it on Replicate or ComfyUI and learn about the required custom nodes. Get insights into creating your own loras through the examples provided by artificialguybr.","This blog post introduces the Face-to-many model that aids in turning any face into 3D, pixel art, video game, claymation, or toy. Learn how to operate it on platforms like Replicate and ComfyUI and delve into the unique creations made by artificialguybr.",Custom Node Pack,"Python





        150





        25


        Built by

          






        38 stars today",https://replicate.delivery/pbxt/R1ayGe5efoQbaoRzgDEJdLsIZ20lWRiprvoW1F4uKAZIha6kA/ComfyUI_00001_.png,,150,2024-03-05T11:48:35Z
2024-03-18,https://github.com/openai/grok,https://raw.githubusercontent.com/openai/grok/main/README.md,"The provided text refers to OpenAI Grok Curve Experiments. This is the code repository for a research paper titled 'Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets'. The authors of the paper are Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Installation and training instructions for the code are also included in the text.",Exploring OpenAI Grok Curve Experiments: A Comprehensive Guide,"Dive into our comprehensive guide on OpenAI Grok Curve Experiments. Understand, implement and utilize the code for the paradigm-shifting paper: 'Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets'.","OpenAI's Grok Curve Experiments unveiled. This post guides you through installation, training, and comprehending the transformative 'Grokking' paper.",AI Training Platform,"Python





        2,561





        282


        Built by

          








        433 stars today",,,2561,2021-04-12T14:42:35Z
2024-03-18,https://github.com/mealie-recipes/mealie,https://raw.githubusercontent.com/mealie-recipes/mealie/master/README.md,"Mealie is a self-hosted recipe manager and meal planner with a RestAPI backend and a Vue developed reactive frontend application. Its features allow easy addition of recipes into a database via URL or UI editor, and third-party applications interactions through an API. The open-source community contributions are encouraged and appreciated, with both coders and non-coders (through financial contributions or translations) having ways to participate and benefit. The project is licensed under AGPL. Sponsors support the project's continuation, while Depot provides build instances for Docker image builds.
",Explore Mealie: Your Self-Hosted Recipe Manager and Meal Planner,"Mealie is a self-hosted recipe manager delivering a delightful user experience for the whole family. It enables easy imports of recipes with automatic data updates and a user-friendly UI editor. Apart from managing your family recipes, Mealie provides a powerful API for 3rd party interactions.","Discover Mealie- a self-hosted recipe manager with a RestAPI backend, Interactive UI, 3rd party API interactions, translation feature, and contribution options.",Open Source Community,"Python





        4,750





        512


        Built by

          









        15 stars today",https://cdn.buymeacoffee.com/buttons/v2/default-green.png,,4750,2020-12-07T01:04:17Z
2024-03-18,https://github.com/apple/axlearn,https://raw.githubusercontent.com/apple/axlearn/main/README.md,"AXLearn is a deep learning library built on JAX and XLA, designed for developing large-scale models. It uses an object-oriented approach and integrates with other libraries like Flax and Hugging Face transformers. The library supports training of models with hundreds of billions of parameters across thousands of accelerators. AXLearn is designed to operate on public clouds and provides tools for job and data management. It can handle a variety of applications, including natural language processing, computer vision, and speech recognition. The library is still under development, with potential changes to its API.",Mastering Deep Learning with AXLearn Library: A Comprehensive Guide,"Discover the AXLearn Library for large-scale deep learning model development. This object-oriented solution is designed for scalability and supports natural language processing, computer vision, and speech recognition. Dive into its core components, design, and utilisation in this informative blog post.","A deep-dive into the AXLearn Library, designed for the development of large-scale deep learning models supporting various applications. Leverage AXLearn for high-utilization training and seamless integration with other libraries.",Deep Learning Framework,"Python





        800





        121


        Built by

          









        111 stars today",,,800,2023-02-25T01:33:06Z
2024-03-18,https://github.com/ytdl-org/youtube-dl,https://raw.githubusercontent.com/ytdl-org/youtube-dl/master/README.md,"The text provides detailed information on how to use the command-line tool ""youtube-dl"" to download videos from YouTube.com and other video platforms. The tool offers a wide range of functions, including options for format selection, output templates, multiple-download formats, and automatic credentials storage. Installation instructions for UNIX and Windows users are also provided. The tool requires the Python interpreter and can modify or redistribute videos for public use. It also includes options for resolving various issues such as fragment retries, geographic restrictions, file size limitations, and more.",Effective Guide to Using Youtube-dl for Video Downloads,"Explore how to effectively use youtube-dl, a command-line program, to download videos from various platforms such as YouTube. The article includes a detailed list of instructions for installing and using the program, including configuration and format selection.","Discover how to download and utilize youtube-dl to effortlessly download videos from various platforms. This article covers key steps, from installation to advanced configuration and output formatting, to enhance your video downloading experience.",Video Generation Tool,"Python





        127,749





        9,592


        Built by

          









        33 stars today",,https://www.youtube.com/watch?v=-wNyEUrxzFU; https://www.youtube.com/watch?v=BaW_jenozKcj; https://www.youtube.com/watch?v=FqZTN594JQw; https://www.youtube.com/watch?v=FqZTN594JQw; https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKc; https://www.youtube.com/watch?v=BaW_jenozKcj; https://www.youtube.com/watch?v=BaW_jenozKc,127749,2010-10-31T14:35:07Z
2024-03-18,https://github.com/oobabooga/text-generation-webui,https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/README.md,"The Gradio web UI for Large Language Models aims to be the comprehensive tool for text generation. It features various interface modes, model backends, a quick dropdown menu for switching models, support for multiple extensions, and the ability to chat with custom characters. The service integrates with the Transformers library and supports loading models in 4-bit or 8-bit precision. It can be installed easily via cloning the repository, running the appropriate script per OS, and navigating to the local host URL. Additionally, features can be added or managed via command-line flags.",Mastering Language Models with Gradio's User Interface: A Comprehensive Guide,"Explore the ins and outs of managing large language models using Gradio's user interface. Learn how to work with multiple model backends, interact with a wide range of extensions, and integrate your models into your workflows. Discover the power of text generation with Gradio today.","A detailed guide on using Gradio's web UI for managing large language models. Covers the handling of multiple model backends, use of extensions, and model integration. Perfect for those looking to harness the power of text generation in their projects.",Large-scale Language Models,"Python





        34,456





        4,604


        Built by

          









        61 stars today",https://github.com/oobabooga/screenshots/raw/main/print_instruct.png; https://github.com/oobabooga/screenshots/raw/main/print_chat.png; https://github.com/oobabooga/screenshots/raw/main/print_default.png; https://github.com/oobabooga/screenshots/raw/main/print_parameters.png,,34456,2022-12-21T04:17:37Z
2024-03-18,https://github.com/almandin/fuxploider,https://raw.githubusercontent.com/almandin/fuxploider/master/README.md,"Fuxploider is an open-source penetration testing tool designed to automate the detection and exploitation of file upload form flaws. The tool can identify the file types permitted to upload and understand the most effective technique for uploading web shells or malicious files to the desired web server. It requires at least Python 3.6 to function. It gets installed via GitHub and Python's pip package manager, with Docker installation also available. However, its misuse for illegal attacks without obtaining prior mutual consent is strictly forbidden, and developers take no responsibility for any damage caused.
",Fuxploider: Your Open Source Penetration Testing Tool,"Fuxploider is an open-source tool designed for penetration testing, automating the detection and exploitation of file upload forms flaws. It can identify the allowed upload file types, and determine the optimal technique for uploading web shells or malicious files on the targeted web server.","Discover Fuxploider, an open-source penetration testing tool perfect for exploiting file upload forms flaws. Enhance your cybersecurity with this automation solution.",Cybersecurity Tool,"Python





        2,871





        482


        Built by

          









        22 stars today",https://raw.githubusercontent.com/almandin/fuxploider/master/screenshot.png,,2871,2017-07-14T09:30:06Z
2024-03-18,https://github.com/Doubiiu/DynamiCrafter,https://raw.githubusercontent.com/Doubiiu/DynamiCrafter/main/README.md,"DynamiCrafter is an open-source technology for animating open-domain still images based on text prompts. It uses pre-trained video diffusion priors and has various application sub-features such as generative frame interpolation and looping video generation. Pretrained models for different resolutions are available on Hugging Face. This software is developed and maintained by Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. It comes with detailed instructions for installation, inference, and setup via command-lines or a local Gradio demo.
",Animating Images with DynamiCrafter: Making Use of Video Diffusion Priors,"Explore the wonders of DynamiCrafter, the newly released technology bringing still, open-domain images to life. This innovation leverages pre-trained video diffusion priors, offering better dynamics, improved resolution, and stronger coherence.","This post unveils the latest update on DynamiCrafter - a technology for animating open-domain still images with text prompts using pre-trained video diffusion priors. Learn how this tool brings high dynamics, resolution, and coherence to images animation.",Image Generation Platform,"Python





        1,143





        90


        Built by

          





        49 stars today",https://img.youtube.com/vi/0NfmIsNAg-g/0.jpg,https://www.youtube.com/watch?v=0NfmIsNAg-g; https://www.youtube.com/watch?v=0NfmIsNAg-g,1143,2023-11-27T12:34:23Z
2024-03-18,https://github.com/CognitionAI/devin-swebench-results,https://raw.githubusercontent.com/CognitionAI/devin-swebench-results/main/README.md,"This text refers to the results and methodology of Cognition's work on SWE-bench, which is discussed in detail on their blog post. You can find more comprehensive information about their findings and procedures on the SWE-bench technical report from the given link.",Unveiling SWE-Bench Results & Methodology: An Insightful Approach by Cognition Labs,Cognition Labs presents an illuminating discussion on SWE-bench results and methodology. Dive into our technical report to understand our streamlined processes.,"Explore Cognition Labs' approach towards SWE-bench, detailing our results and methodology in an enlightening technical report.",Software Development,"Python





        43





        6


        Built by

          








        8 stars today",,,43,2024-03-15T03:56:41Z
2024-03-19,https://github.com/xai-org/grok-1,https://raw.githubusercontent.com/xai-org/grok-1/main/README.md,"The Grok-1 open-weights model, held in a repository, can be utilized by downloading its checkpoint and running given example code, ideally on a machine with sufficient GPU memory due to the model's large size of 314B parameters. It features a Mixture of 8 Experts architecture and SentencePiece tokenizer, among other specifications. The model's weights can be accessed via a torrent client or directly from HuggingFace Hub. The source files and model weights of Grok-1 are licensed under the Apache 2.0 license. The model's MoE layer's implementation is intentionally inefficient to avoid needing custom kernels for validation.",Exploring Grok-1: A Comprehensive Guide to Using the Open-Weights Model,"Experience the power of Grok-1, an open-weights model boasting of 314B parameters and a MoE architecture. Understand its implementation, download the necessary checkpoints and learn to execute the code. Equip your machine with proper GPU memory to effectively run and test the model.","Dive into the functionalities of Grok-1, the open-weights model. Learn how to download checkpoints, run tests, and understand the model specifications. Ensure your machine is capable enough to handle the magnitude of Grok-1.",Large-scale Language Models,"Python





        34,995





        5,644


        Built by

          









        12,940 stars today",,,34998,2024-03-17T08:53:38Z
2024-03-19,https://github.com/albertan017/LLM4Decompile,https://raw.githubusercontent.com/albertan017/LLM4Decompile/main/README.md,"LLM4Decompile is a project focused on reverse engineering binary code using large language models (LLMs). The project uses a dataset of one million code samples compiled into assembly code to finetune the DeepSeek-Coder model. The main goal is to decompile assembly instructions and assess the decompiled code for its ability to be recompiled and its functionality through test assertions. Different models of LLM4Decompile, having sizes between 1.3 billion and 33 billion parameters, are available on Hugging Face. This open-source effort helps in syntax recovery and semantic preservation, critical for robust decompilation. Future plans include pre-training the model with assembly code and supporting multiple languages/platforms.",Decompiling Binary Code with Large Language Models: Introducing LLM4Decompile,"Discover LLM4Decompile, the first open-source Large Language Model (LLM) dedicated to decompiling binary code. Get insight into our evaluation benchmark, Decompiler-Eval, which assesses re-compilability and re-executability. Learn how to use model, assess its performance and know about our ongoing plans to support more languages and platforms.",This blog guides you through the world of binary code decompiling using LLM4Decompile - an open-source Large Language Model. Understand how Decompiler-Eval measures decompilation efficiency and get started with using the model yourself.,Large-scale Language Models,"Python





        1,551





        97


        Built by

          






        326 stars today",https://github.com/albertan017/LLM4Decompile/blob/main/samples/pipeline.png; https://github.com/albertan017/LLM4Decompile/blob/main/samples/results_decompile.png,,1551,2024-02-28T03:16:44Z
2024-03-19,https://github.com/google/jax,https://raw.githubusercontent.com/google/jax/main/README.md,"JAX is a Python library by Google designed for high-performance numerical computing and large-scale machine learning. It can differentiate native Python and NumPy functions through an updated version of Autograd, support reverse-mode differentiation or backpropagation, and differentiate through loops, branches, recursion, and closures. JAX also uses XLA to compile and run NumPy programs on GPUs and TPUs. The library allows for just-in-time compilation of userâ€™s Python functions into XLA-optimized kernels. It supports parallel programming of multiple accelerators and is an extensible system for composable function transformations. Though a research project, the authors encourage users to report bugs and suggestions.
",Exploring JAX: Python library for Optimal Numeric Computing and Machine Learning,"JAX is a Python library designed for high-performance numeric computing. With its updated version of Autograd, JAX can automatically differentiate Python and NumPy functions and compile your NumPy programs on GPUs and TPUs for astonishing speed.",An overview of JAX capabilities: a Python library for high-performance numeric computation and machine learning. Discover how JAX with Autograd optimizes numeric Python functions in this blog post.,Machine Learning Tool,"Python





        27,294





        2,499


        Built by

          








        60 stars today",https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png,https://www.youtube.com/watch?v=iDxJxIyzSiM,27294,2018-10-25T21:25:02Z
2024-03-19,https://github.com/phidatahq/phidata,https://raw.githubusercontent.com/phidatahq/phidata/main/README.md,"Phidata is an AI toolkit that enables building of AI assistants using function calling and by connecting large language models (LLMs) to external tools. These assistants can perform tasks like web searches, data analysis, and sending emails. Phidata comes with built-in memory, storage, and knowledge base, allowing users to build AI applications that can take actions. Creation of an assistant involves adding tools, knowledge in the form of vectordb, and storage in the form of a database. The toolkit can be installed via pip and provides several examples, like assistants performing web search, or executing Python code, or analyzing data using SQL.
",Take Your AI Assistants to the Next Level with Phidata,"Phidata is a toolkit for creating AI assistants with function calling and integration to external tools. It allows language models to do web searches, data analysis, and send emails, amongst other tasks. With built-in memory, networking, knowledge, and tool access, you can go beyond text-based interactions and develop AI applications that can actually take actions.","Learn how to use Phidata to create AI assistants that connect with external tools, and can perform tasks such as web searching, data analysis, and emailing. Includes information on installation and available Python-based examples.",AI Coding Assistant,"Python





        2,773





        287


        Built by

          









        41 stars today",https://img.youtube.com/vi/VNoBVR5t1yI/0.jpg; https://img.youtube.com/vi/EVQLYncsDVI/0.jpg,https://www.youtube.com/watch?v=VNoBVR5t1yI; https://www.youtube.com/watch?v=EVQLYncsDVI,2773,2022-05-04T15:23:02Z
2024-03-19,https://github.com/ReaVNaiL/New-Grad-2024,https://raw.githubusercontent.com/ReaVNaiL/New-Grad-2024/main/README.md,"This is a collection of full-time job opportunities for 2024 graduates in various tech fields, such as software engineering, product management, and quantitative analysis. Jobs are primarily located in the United States, Canada, and remote positions. The listing also provides resources for technical interview prep and resources for international students like checking H1B sponsorship history and checking a company's e-verification status. The list only includes job openings in the United States, remote positions, and Canada. The list also includes the status of the role, indicating whether the position is still open or closed.","Tech Jobs for Grads 2024: Opportunities in Programming, Analysis, and Product Management","Discover exciting full-time opportunities for new graduates in 2024 in Software Engineering, Quantitative Analysis, Product Management, and other tech roles across the U.S. and Canada. Join our community and browse a regularly updated list of job openings.","A comprehensive guide to full-time job openings for new graduates in 2024 in tech-related fields across the U.S. and Canada. Explore opportunties in Software Engineering, Quantitative Analysis, Product Management, and more.",Job Opportunities Collection,"Python





        5,614





        517


        Built by

          









        6 stars today",,,5614,2023-04-02T21:22:38Z
2024-03-19,https://github.com/jumpserver/jumpserver,https://raw.githubusercontent.com/jumpserver/jumpserver/master/README.md,"JumpServer is a widely used open-source security audit system, in accordance with 4A standards, designed to enhance the operation and management of various types of assets in enterprises. It offers a safe and secure way to control and login to assets such as SSH, Windows, Databases (MySQL, MariaDB, etc.), NoSQL, GPT, cloud services, web platforms, and applications. The system boasts of multiple features such as being open-source, plugin-free, supporting distributed deployment, multi-cloud, multi-tenant support, and cloud-based storage. Users can gain an online experience of its environment, with available resources such as quick start guides, product tutorials, knowledge bases, and real-world case studies. Please note, sensitive information should not be added to this environment.",Explore JumpServer: The Popular Open-Source Bastion Machine,"JumpServer, a popular open-source bastion machine, provides professional operational security audit systems compliant with 4A standards. It assists businesses in managing and logging into various assets securely, while offering diverse features such as being open-source, browser exclusive usage, and supporting distributed deployment.","Discover the features and benefits of JumpServer, a popular open-source bastion machine providing security audit systems for businesses. Learn how it can help securely manage a variety of assets, from databases to cloud services.",Open Source ERP,"Python





        23,305





        5,163


        Built by

          









        10 stars today",https://docs.jumpserver.org/zh/v3/img/dashboard.png,,23305,2014-07-04T03:54:59Z
2024-03-20,https://github.com/Stability-AI/generative-models,https://raw.githubusercontent.com/Stability-AI/generative-models/main/README.md,"Stability AI has developed several generative models for research purposes. These models primarily focus on image-to-video, text-to-image, and image-to-image functionalities. Key releases include SV3D, an image-to-video model which generates frames from a single context frame and two variants accommodating additional inputs and capabilities. Other artificial intelligence tools include SDXL Turbo, a quick text-to-image model, and Stable Video Diffusion, another image-to-video tool. They also offer open models under the permission CreativeML Open RAIL ++ M license, including improved versions of existing models. All models are accompanied by detailed guidance on usage and installation codes.
",Unlocking the Potential of Generative Models: A Deep Dive Into Stability AI's Innovations,"Explore Stability AI's latest contributions in the field of Generative Models. Discover the power of their novel image-to-video models, detailed technical reports, and advanced model capabilities. Understand how to leverage these models for your research, underscored by the company's commitment to openness.","This blog post provides an in-depth look at Stability AI's innovations in generative models, their state-of-the-art image-to-video systems, and instructions on how to utilize these models for your own research. ",Artificial Intelligence,"Python





        21,138





        2,272


        Built by

          









        84 stars today",https://raw.githubusercontent.com/Stability-AI/generative-models/main/assets/000.jpg; https://raw.githubusercontent.com/Stability-AI/generative-models/main/assets/sv3d.gif; https://raw.githubusercontent.com/Stability-AI/generative-models/main/assets/turbo_tile.png; https://raw.githubusercontent.com/Stability-AI/generative-models/main/assets/tile.gif; https://raw.githubusercontent.com/Stability-AI/generative-models/main/assets/001_with_eval.png,https://www.youtube.com/watch?v=Zqw4-1LcfWg,21138,2023-06-22T00:36:35Z
2024-03-20,https://github.com/zylon-ai/private-gpt,https://raw.githubusercontent.com/zylon-ai/private-gpt/main/README.md,"PrivateGPT is an AI platform allowing users to query their documents without the need for an internet connection - providing privacy as no data leaves the user's environment. It provides APIs for building personal, context-aware AI applications. It offers a high-level API abstracting complex processes for document ingestion and context retrieval and a low-level API for implementing complex pipelines. The PrivateGPT API adheres and extends the OpenAI API standard. The project includes a Gradio UI client to test the API. PrivateGPT was created to bring a greater degree of privacy to Generative AI usage in sensitive industries.",Unlocking AI Privacy with PrivateGPT: Your Comprehensive Guide,"PrivateGPT is a robust AI project designed for privacy-centric applications. Immune to Internet connections, it empowers users to extract insightful questions from their documents, thanks to its powerful Large Language Models. With in-built APIs, it lays the groundwork for innovative AI applications that prioritize privacy and context-awareness. Additionally, it provides Gradio UI client for API testing, alongside handy tools like document folder watch, model download script, etc.","Explore the world of private AI with PrivateGPT. Deep-dive into its features, from Large Language Models to high-level APIs, Gradio UI testing, and more. Discover how it's advancing AI privacy in today's digital age.",Private AI Framework,"Python





        49,869





        6,632


        Built by

          









        269 stars today",,,49869,2023-05-02T09:15:31Z
2024-03-20,https://github.com/triton-inference-server/server,https://raw.githubusercontent.com/triton-inference-server/server/main/README.md,"The Triton Inference Server is an open-source inference serving software that facilitates Artificial Intelligence (AI) inferencing. It streamlines the deployment of models across different deep learning and machine learning frameworks. It is compatible with various platforms, including cloud, data centre, edge, and embedded devices, and is optimized for performance across many query types, including real-time, batched, ensembles, and audio or video streaming. Major features include the support of multiple learning systems, concurrent model execution, sequence batching, custom backend APIs and more. It also provides tutorials for new users and offers enterprise support through the NVIDIA AI Enterprise software suite.
",Unleashing NVIDIAâ€™s Triton Inference Server: Streamlined AI Inferencing,"Explore NVIDIA's Triton Inference Server, an open-source software that simplifies AI inferencing. Discover its ability to deploy any AI model from multiple deep learning and machine learning frameworks. Leverage Triton's optimized performance across platforms including cloud, data center, edge and embedded devices.","Learn about the robust features of NVIDIAâ€™s Triton Inference Server, its broad compatibility with multiple AI frameworks, and its optimized performance across multiple platforms, from cloud infrastructures to edge devices.",AI Inference Platform,"Python





        7,121





        1,352


        Built by

          









        53 stars today",,https://www.youtube.com/watch?v=NQDtfSi5QF4,7122,2018-10-04T21:10:30Z
2024-03-20,https://github.com/pandas-dev/pandas,https://raw.githubusercontent.com/pandas-dev/pandas/main/README.md,"Pandas is a powerful Python package for data analysis. It provides fast, flexible, and expressive data structures designed to work with relational or labeled data. Its goals are to be the fundamental high-level building block for practical, real world data analysis in Python, and the most potent open-source data analysis tool available in any language. It handles missing data, size mutability, data alignment, group by operations, converting ragged data, slicing, indexing, merging, and joining data sets. The source code is hosted on GitHub, and it can be installed from PyPI or Conda.
",Mastering Data Analysis with pandas: A Comprehensive Python Toolkit Guide,"This blog provides an insightful guide exploring pandas, a Python package offering fast, flexible, and expressive data structures. Learn how this toolkit enhances data analysis in Python, its impressive features, installation, and more. Unleash the benefits of pandas for effective data manipulation and analysis.","Become adept at data analysis using pandas, a Python package with focus on powerful data structures to manipulate and analyse data. This blog details its features, installation process and benefits.",Data Science Tool,"Python





        41,626





        17,203


        Built by

          









        19 stars today",,,41626,2010-08-24T01:37:33Z
2024-03-20,https://github.com/stas00/ml-engineering,https://raw.githubusercontent.com/stas00/ml-engineering/master/README.md,"The Machine Learning Engineering Open Book is a collection of methodologies, tools, and instructions designed to guide engineers in training Large Language Models (LLMs) and multi-modal models. Its contents include various scripts and commands based on the author's experience training open-source models like BLOOM-176B and IDEFICS-80B. The book covers topics from insights in AI engineering and hardware considerations to model training and debugging. It also has resources and guides to quickly address common problems. The open-source nature of the book invites community discussions, contributions, and updates, which will be announced on the author's Twitter channel.","Comprehensive Guide to Machine Learning Engineering: Methods, Tools, and Best Practices","Unlock the secret to effective machine learning engineering with this comprehensive guide. Discover methodologies, helpful tools, and step-by-step instructions for successfully training large language models and multi-modal models. Suitable for LLM/VLM training engineers and operators, this resource discusses insights from training open-source models such as BLOOM-176B and IDEFICS-80B.","A complete resource on Machine Learning Engineering offering insights into methodologies, tools and instructions for successful training of large language models. Includes experiences from training open-source models, BLOOM-176B and IDEFICS-80B.",Machine Learning Tool,"Python





        9,366





        548


        Built by

          









        300 stars today",,,9366,2020-09-02T19:23:01Z
2024-03-20,https://github.com/NVIDIA/GenerativeAIExamples,https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/README.md,"NVIDIA provides a range of state-of-the-art Generative AI examples that are easy to deploy, test and extend. These examples utilize resources from NVIDIA NGC AI Development Catalog and run on NVIDIA CUDA-X software and NVIDIA GPUs. Specific RAG pipelines have been developed which embed various data types into a database connected to a LLM, allowing for interactive user experiences. These examples can be deployed using various tools including Docker. NVIDIA RAG pipelines also come with example tools and tutorials to enhance usage and productivity, and open source connectors for integration with API endpoints. Community contributions and feedback are encouraged.",Understanding NVIDIA Generative AI: Applications and Optimized Resources,"Explore the state-of-the-art Generative AI examples that are easy to deploy, test, and extend with NVIDIA CUDA-X software stack and NVIDIA GPUs. Discover more about NVIDIA NGC and Retrieval Augmented Generation. Get to know about Developer RAG and Enterprise RAG examples, and a variety of tools available to enhance LLM development and productivity.","Get an in-depth understanding of NVIDIA Generative AI examples. Implement and extend these high-performance examples with the NVIDIA CUDA-X software stack and NVIDIA GPUs. Explore Developer and Enterprise RAG examples, available tools, and open-source integrations.",AI Development Platform,"Python





        1,250





        185


        Built by

          









        18 stars today",,,1250,2023-10-19T13:46:31Z
2024-03-21,https://github.com/kongzhecn/OMG,https://raw.githubusercontent.com/kongzhecn/OMG/master/README.md,OMG is a framework for generating multi-concept image creation. It works by combining Character and Style LoRAs (Large-scale Overparameterized Representational Adapters) with occlusion-friendly diffusion models. This framework supports using multiple IDs from a single image. It can produce high-quality images and allows the user to control details such as the visual comprehension or the style that the image is generated in. The OMG framework also includes a gradio demo and is runnable in a python 3.10.6 environment with specific pytorch and torchvision dependencies.,OMG Framework: Personalized Multi-concept Generation in Diffusion Models Explained,"Dive into the cutting-edge OMG framework for personalized multi-concept image generation. Explore its functionalities and learn how it integrates with tools like InstantID for innovative image generation approaches. With character and style LoRAs, you can create engaging and dynamic images easily.","Unveil the complex world of multi-concept image generation with the OMG Framework. Designed for character and style LoRAs, this blog post explores how the framework supports image generation and can be combined with tools like InstantID. Read now to master the use of OMG Framework.",Image Generation Platform,"Python





        297





        19


        Built by

          






        21 stars today",https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/teaser.png; http://img.youtube.com/vi/5BI_a7nTb8Q/0.jpg; http://img.youtube.com/vi/c-dYmPo7rVM/0.jpg; https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/lora.png; https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/instantid.png; https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/controlnet.png; https://raw.githubusercontent.com/kongzhecn/OMG/master/assets/style.png,https://www.youtube.com/watch?v=5BI_a7nTb8Q,297,2024-02-06T08:24:40Z
2024-03-21,https://github.com/Kiteretsu77/APISR,https://raw.githubusercontent.com/Kiteretsu77/APISR/main/README.md,"APISR is a project aiming to restore and enhance low-quality, low-resolution anime images and videos, dealing with various degradations from real-world scenarios. It includes a visualization tool, an installation guide, a dataset curation pipeline, and training instructions. It allows users to create their own datasets from video sources to obtain the least compressed and most informative images. A Gradio option is available for quick inference, with an online demo hosted by HuggingFace and Colab. The project uses Python and Pytorch and the code can be cloned or downloaded from GitHub. This project is intended for academic use only.
",Revitalizing Anime Images with APISR: Anime Production Inspired Super-Resolution (CVPR2024),"APISR is designed to restore and enhance low-quality and low-resolution anime images and videos. With a variety of applications in real-world scenarios, it transforms degraded anime sources into clear, high-quality images. APISR presents remarkable advancements in Anime Super-Resolution, offering a remarkable visual experience for anime viewers.","Discover APISR: the innovative solution for enhancing and restoring low-resolution, low-quality anime images and videos. Experience real-world application of APISR and marvel at the transformation of degraded anime sources into high-definition visuals.",Image Generation Platform,"Python





        257





        19


        Built by

          






        45 stars today",https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/logo.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/workflow.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/0079_visual.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/0079_2_visual.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/pokemon_visual.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/pokemon2_visual.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/eva_visual.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/kiteret_visual.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/f91_visual.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/visual_results/wataru_visual.png; https://raw.githubusercontent.com/Kiteretsu77/APISR/main/__assets__/AVC_RealLQ_comparison.png,,257,2024-03-02T04:18:53Z
2024-03-21,https://github.com/getsentry/sentry,https://raw.githubusercontent.com/getsentry/sentry/master/README.md,"Sentry is a developer-first platform focused on tracking errors and monitoring performance. It aims to help developers quickly solve problems and learn continuously about their applications' functions. Sentry provides various official SDKs compatible with a host of programming languages such as JavaScript, Electron, React-Native, Python, Ruby, PHP, Laravel, Go, Rust, Java/Kotlin, Objective-C/Swift, C#/F#, Dart, Perl, Clojure, Elixir, Unity, Unreal Engine, and others. Resources for Sentry include documentation, a community forum, Discord, contribution guidelines, a bug tracker, source code, and Sentry translations.",Maximizing Developer Efficiency with Sentry: A Comprehensive Overview,Discover Sentry - a developer-first platform for error tracking and performance monitoring. Get insights faster and continuously learn about your applications. Explore its vast number of SDKs and extensive resources that maximize developer efficiency.,"This blogpost provides a comprehensive overview of Sentry. It reveals its developer-first approach for error tracking and performance monitoring, its range of SDKs, and useful resources for developers.",Developer Monitoring Platform,"Python





        36,566





        3,950


        Built by

          









        20 stars today",https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png; https://github.com/getsentry/sentry/raw/master/.github/screenshots/projects.png; https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png; https://github.com/getsentry/sentry/raw/master/.github/screenshots/transaction-summary.png; https://github.com/getsentry/sentry/raw/master/.github/screenshots/releases.png,,36566,2010-08-30T22:06:41Z
2024-03-21,https://github.com/mrphrazer/reverser_ai,https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/README.md,"ReverserAI is a tool created by Tim Blazytko to assist in reverse engineering tasks using locally-hosted large language models (LLMs). By operating offline, it ensures data privacy and can suggest high-level function names from decompiler output. Although unable to match the performance of cloud-based language models, it is a significant progress as it caters to performance and confidentiality demands. It is compatible with consumer-grade hardware and integrates as a Binary Ninja plugin, but can be extended to other platforms. Despite its limited capabilities, the contribution of efficient models is welcomed to improve it. The tool is optimized to work best with powerful GPUs or multi-thread CPUs.
",Unleashing the Power of AI in Reverse Engineering: An In-Depth Look at ReverserAI v1.0.1,"ReverserAI is a research project that employs large language models (LLMs) to automate and augment reverse engineering tasks. It operates offline, creating meaningful function names from decompiler output. Designed for easy integration with a range of reverse engineering platforms, it's currently provided as a Binary Ninja plugin. While its performance may not match cloud-based LLMs, it offers a balance between competent performance and confidentiality requirements. This initial version of ReverserAI paves the way for more advancement in AI-assisted reverse engineering.","Explore the features and capabilities of ReverserAI, a research project aimed at automating reverse engineering tasks using local large language models. Learn how this Binary Ninja plugin blazes new trails in the AI-assisted reverse engineering field.",AI Development Platform,"Python





        219





        12


        Built by

          





        43 stars today",https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/imgs/plugin_menu.png; https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/imgs/plugin_results.png; https://raw.githubusercontent.com/mrphrazer/reverser_ai/main/imgs/plugin_settings.png,,219,2024-03-17T07:14:56Z
2024-03-21,https://github.com/ShishirPatil/gorilla,https://raw.githubusercontent.com/ShishirPatil/gorilla/main/README.md,"Gorilla is a large language model (LLM) project developed by Berkeley which enables LLMs to utilize tools by invoking APIs. It identifies semantically and syntactically correct API calls when given a natural language query and can accurately invoke over 1600 APIs. Gorilla's function capabilities are competitive with GPT-4 and also support multiple languages. Gorilla is licensed under Apache 2.0, allowing for commercial usage with no obligations. The project also offers a Gorilla OpenFunctions feature as an alternative for function calling. The project encourages community contributions to further expand the API store.
",Exploring Gorilla: A Large Language Model with Massive APIs,"Discover Gorilla, a revolutionary language model that invokes APIs based on natural language queries. Gorilla offers an unprecedented method to implement over 1,600 APIs accurately to minimize hallucination. See how Gorilla contributes to APIBench, the largest API collection for training purposes, and learn how you can get involved!","An inside look at Gorilla, a language model that allows API invocation from natural language queries. Discover its contribution to APIBench and its role in crafting the future of API usage.",Large-scale Language Models,"Python





        9,441





        684


        Built by

          









        38 stars today",https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png,,9441,2023-05-19T00:46:45Z
2024-03-22,https://github.com/harry0703/MoneyPrinterTurbo,https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/README.md,"The MoneyPrinterTurbo software tool fully automates the creation of high-quality short videos by generating video scripts, materials, subtitles, and background music based on a provided video theme or keyword. The tool is built on a complete MVC architecture, supports multiple high-definition video sizes, and can manage English and Chinese video scripts, several kinds of voice synthesis, subtitle generation, and background music. It also ensures video resources are copyright-free. Planned future enhancements include API interface improvements, voice synthesis and subtitle effect optimization, and enhanced video transition effects and material relevance. The software can be installed and launched through the command line interface.",MoneyPrinterTurbo: Automated Video Generation and Editing For the Next Level,"MoneyPrinterTurbo helps in automating the video generation process along with subtitles, background music, and editing. It offers clear code structure, multi-language support, variety of video dimensions, music files without copyright issues, and many more. Future plans include improving transition effects, optimizing synthesized voices, and making subtitles more effective.","Discover MoneyPrinterTurbo's effortless video generation and editing tool. Create high-definition videos by providing a theme or keyword. Perfectly structured code, multi-language support, variety of video dimensions, and copyright-free music files available.",Video Generation Tool,"Python





        325





        96


        Built by

          





        58 stars today",https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/webui.jpg; https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/api.jpg; https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/douyin.jpg; https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/shipinghao.jpg,,325,2024-03-11T02:57:34Z
2024-03-22,https://github.com/lewis-007/MediaCrawler,https://raw.githubusercontent.com/lewis-007/MediaCrawler/main/README.md,"This Chinese text pertains to a repository that hosts crawl technologies for various social media platforms including Xiaohongshu, TikTok, Kuaishou, Bilibili, and Weibo. The repository and its contents can be only used for learning and reference purposes and not for commercial use or illegal activities. The repository can fetch videos, images, comments, likes, shares, etc. using Playwright. The text includes detailed instructions for setting up Python virtual environment, installing dependencies, and running the crawler program. It also offers support options and provides information on how to save data. Reward contributions are acknowledged, and solutions for common issues are addressed.
","Master the Art of Web Crawling with Small Red Book, TikTok, Kuaishou, Bilibili and Weibo","Discover the secrets to effective web crawling with platforms like Small Red Book, TikTok, Kuaishou, Bilibili, and Weibo. Harness the power of Playwright, understand core encryption, and streamline your web crawling tasks, all in one comprehensive guide.","A detailed guide exploring web crawling on various platforms using effective techniques and strategies. Learn how to gather video, image, comment, and likes data from popular platforms and minimize reverse engineering challenges using Playwright",Social Media Crawler,"Python





        137





        481


        Built by

          








        30 stars today",https://raw.githubusercontent.com/lewis-007/MediaCrawler/main/static/images/wechat_pay.jpeg; https://raw.githubusercontent.com/lewis-007/MediaCrawler/main/static/images/zfb_pay.jpeg,,137,2024-03-11T02:57:05Z
2024-03-22,https://github.com/OpenInterpreter/open-interpreter,https://raw.githubusercontent.com/OpenInterpreter/open-interpreter/main/README.md,"Open Interpreter is a tool recently launched in new computer '01', that lets language learning models run code in Python, Javascript, Shell, and more, locally. This involves chatting with Open Interpreter through a ChatGPT-like interface in the terminal. The tool has a natural-language interface to access your computer's general-purpose capabilities, providing the ability to run numerous tasks like creating and editing various media, controlling Chrome browsers for research, and managing large datasets. It is compared to OpenAI's Code Interpreter but possesses advantages like being unrestricted and running in a local environment.",Open Interpreter: Transforming Code Execution with Natural-Language Interface,"Introducing Open Interpreter, an accessible way to run Python, Javascript, and Shell code on your local machine using a language similar to ChatGPT. With Open Interpreterâ€™s natural-language interface, you can directly interact with your computer's general-purpose capabilities. Gain early access to Open Interpreter now.","Explore the world of code interpretation with Open Interpreter. Execute Python, Javascript, Shell code locally through an AI chat, giving you an intuitive access to your computer's capabilities. Join the journey towards achieving more efficient programming.",Language Learning Platform,"Python





        42,793





        3,747


        Built by

          









        424 stars today",,https://www.youtube.com/watch?v=CEs51hGWuGU,42794,2023-07-14T07:10:44Z
2024-03-22,https://github.com/GaParmar/img2img-turbo,https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/README.md,"The text introduces img2img-turbo, a method for adapting a single-step diffusion model for new tasks and domains through adversarial learning. This allows efficient inference while retaining the internal knowledge of pre-trained diffusion models. The one-step conditional models, CycleGAN-Turbo and pix2pix-turbo, can handle various image-to-image translation tasks for both unpaired and paired settings. CycleGAN-Turbo outperforms other existing methods while pix2pix-turbo matches recent works but with one-step inference. The architecture integrates three separate modules into one end-to-end network, utilizing small trainable weights. To quickly set up the functionality, the user can follow the provided environmental setup instructions.",Revolutionizing Image Translation with CycleGAN-Turbo and Pix2Pix-Turbo,"We introduce a general method for adapting single-step diffusion models to new tasks through adversarial learning. This allows for efficient image-to-image translation, including unpaired and paired settings. Our models CycleGAN-Turbo and pix2pix-turbo outperform exiting GAN-based and diffusion-based methods.","Discover our innovative approach to image-to-image translation tasks using single-step diffusion models, CycleGAN-Turbo and pix2pix-turbo, which achieve efficient inference and outperform previous methods.",Image Generation Platform,"Python





        406





        30


        Built by

          






        59 stars today",https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/cat_2x.gif; https://raw.githubusercontent.com/GaParmar/img2img-turbo/main/assets/fish_2x.gif,,406,2024-02-23T13:50:15Z
2024-03-22,https://github.com/jxnl/instructor,https://raw.githubusercontent.com/jxnl/instructor/main/README.md,"Instructor is a Python library designed to streamline working with structured outputs from large language models (LLMs). Using Pydantic, it offers a user-friendly API for handling validation, retry attempts, and streaming responses. Key features include the ability to define the structure of your LLM outputs with Pydantic models, effortlessly manage retry attempts and Pydantic validation, and integrate with various LLM providers including OpenAI. It simplifies validation of LLM outputs and provides a supportive online community on Discord to help users properly utilize Instructor and share their projects. The tool is open for contributors.",Maximize your LLM Workflows with Python Instructor Library,"This blog post explores the powerful utility of the Python library, Instructor, for working with large language model (LLM) outputs. With features like Pydantic-based response models, validation, and retry management, 'Instructor' simplifies LLM interactions. Additionally, it supports streaming and offers integration with various LLM providers, paving the way for a supercharged language modeling experience!","Discover how to leverage the Python Instructor library to effortlessly manage and streamline working with structured LLM outputs. Features include response models, validation, retry management, streaming support, and flexible backends.",Language Models Tool,"Python





        3,946





        321


        Built by

          









        22 stars today",,,3946,2023-06-14T10:42:23Z
2024-03-22,https://github.com/Doriandarko/maestro,https://raw.githubusercontent.com/Doriandarko/maestro/main/README.md,"""Maestro"" is a Python script utilising Opus and Haiku, two AI models from the Anthropic API, to automate task breakdown and execution. The Opus model breaks down objectives into smaller tasks, then executes each via the Haiku model. Features include an exchange log to track the process, converting log into a Markdown file, and the use of a refined prompt for task assessment, incorporating a specific phrase to indicate task completion. The script can be customised according to user needs. Requirements include Python, the Anthropic API key, and certain Python packages. The script is available under the MIT License.
",Unveiling Maestro: An AI-Assisted Task Breakdown Framework Using Anthropic API,"Discover how the Maestro framework utilizes Python script and the Anthropic API to achieve task breakdown. Learn about its use of AI models, Opus and Haiku, to execute subtasks and refine the end results. The post also highlights its features, prerequisites, installation, usage, code structure, and customization options.",A comprehensive guide to the Maestro framework - an intelligent AI-assisted task breakdown and execution tool using the Anthropic API that simplifies even the most complex tasks.,AI Task Automation,"Python





        362





        56


        Built by

          







        45 stars today",,,362,2024-03-19T20:48:29Z
2024-03-22,https://github.com/explodinggradients/ragas,https://raw.githubusercontent.com/explodinggradients/ragas/main/README.md,"Ragas is a new framework designed to assist in evaluating the performance of Retrieval Augmented Generation (RAG) pipelines, particularly for Language Model (LLM) applications. Ragas allows users to assess and enhance their pipeline's performance quantitatively. The framework incorporates the newest research tools for evaluating LLM-generated texts, providing insights on RAG pipelines. Ragas can be integrated within Continuous Integration/Continuous Deployment systems for ongoing performance checks. Ragas is created with Python and can be installed from the source or via pip. The platform welcomes engagement through their Discord community, and they practice open analytics, sharing non-identifiable usage metrics.
",Optimizing RAG Pipelines with Ragas: A Comprehensive Evaluation Framework,"The Ragas framework provides effective solutions to evaluate, monitor, and enhance the performance of your Retrieval Augmented Generation (RAG) applications. Capitalizing on the latest research, Ragas offers crucial insights into your RAG pipeline's efficacy. It can also be integrated into your CI/CD to ensure consistent performance checks.",Learn to optimize and monitor the performance of your Retrieval Augmented Generation application with the Ragas framework. Get critical insights and consistent checks with continuous integration and continuous delivery integration. ,AI Development Platform,"Python





        3,923





        330


        Built by

          









        189 stars today",,,3923,2023-05-08T17:48:04Z
2024-03-22,https://github.com/zyddnys/manga-image-translator,https://raw.githubusercontent.com/zyddnys/manga-image-translator/main/README.md,"The Manga/Image Translator is a software tool used to translate texts in mangas or images. It's a useful project for untranslated manga or image translations. It supports various features including online demo, samples, installation options through Pip/venv and Poetry, usage in different modes (batch, demo, web, and API), an array of options for translation, and API documentation for synchronous, asynchronous, and manual translations. The project provides an extensive list of potential areas for development and invites contributions from the user community. It also provides a detailed guide for using different modules such as GPT Config Reference, Gimp for rendering, and various language code references, translators reference, and many more.",Unlocking the Power of Image and Manga Translation Tools,"Delve into the revolutionary project of Image/Manga translator. Discover how it facilitates manga or image translations. Learn how it handles image detection, installation, utilization and more.","A comprehensive guide to the innovative Image/Manga translation tool. Explore how it aids in translating texts in manga or images, enhancing your reading experience.",Image Translation Tool,"Python





        3,846





        398


        Built by

          









        118 stars today",https://user-images.githubusercontent.com/31543482/232265329-6a560438-e887-4f7f-b6a1-a61b8648f781.png; https://user-images.githubusercontent.com/31543482/232265339-514c843a-0541-4a24-b3bc-1efa6915f757.png; https://user-images.githubusercontent.com/31543482/232265479-a15c43b5-0f00-489c-9b04-5dfbcd48c432.png; https://user-images.githubusercontent.com/31543482/232265480-f8ba7a28-846f-46e7-8041-3dcb1afe3f67.png; https://user-images.githubusercontent.com/31543482/232264684-5a7bcf8e-707b-4925-86b0-4212382f1680.png; https://user-images.githubusercontent.com/31543482/232264644-39db36c8-a8d9-4009-823d-bf85ca0609bf.png; https://user-images.githubusercontent.com/31543482/232265794-5ea8a0cb-42fe-4438-80b7-3bf7eaf0ff2c.png; https://user-images.githubusercontent.com/31543482/232265795-4bc47589-fd97-4073-8cf4-82ae216a88bc.png,,3846,2021-02-18T03:03:23Z
2024-03-22,https://github.com/Lightning-AI/lightning-thunder,https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/README.md,"Lightning Thunder is a source-to-source compiler for PyTorch that boosts speed by utilizing different hardware executors simultaneously. Thunder offers enhanced performance over standard PyTorch eager code, exhibiting considerable speedups, with reported a 40% increase in training throughput. This software supports distributed strategies and implements best-in-class executors such as nvFuser, torch.compile, cuDNN, and TransformerEngine FP8. It is also extensible and easy to understand, albeit in its alpha phase. Installation and running instructions are provided, and its open-source nature encourages community contribution. Thunder is released under the Apache 2.0 license.
",Supercharge Your PyTorch Models with Lightning Thunder,"Discover how Lightning Thunder can significantly enhance the performance of your PyTorch models. Learn about its features, including source-to-source compilation, utilization of best-in-class executors for speedups, and its extensibility. Get started today!",Lightning Thunder optimizes PyTorch models for enhanced performance. Explore its features and understand how it can transform your machine learning tasks. Start your journey with Lightning Thunder now.,AI Development Platform,"Python





        407





        27


        Built by

          









        199 stars today",https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/docs/source/_static/images/training_throughput_single.png; https://raw.githubusercontent.com/Lightning-AI/lightning-thunder/main/docs/source/_static/images/normalized_training_throughput_zero2.png,,407,2024-03-18T15:30:56Z
2024-03-22,https://github.com/localstack/localstack,https://raw.githubusercontent.com/localstack/localstack/master/README.md,"LocalStack has announced the release of version 3.2. LocalStack is a fully functional local cloud stack allowing the development and testing of AWS applications locally. The software emulates cloud services and offers AWS application capabilities without requiring connection to remote cloud providers. It supports a number of AWS services like AWS Lambda, DynamoDB, and many more. A premium version with additional features is also available. As a cloud developer tool, it offers easier development, testing, and continuous integration. Moreover, its community encourages contributions, and there's also an option to donate or sponsor the project.
",Announcing LocalStack 3.2: Your Local AWS Cloud Development Framework,"Exciting news! We're thrilled to announce the release of LocalStack 3.2. This local cloud software development framework allows you to develop and test your AWS applications locally, speeding up and simplifying your testing and development workflow.","LocalStack 3.2 has just been released. Discover how this cloud software development framework brings the power of AWS to your local dev environment for faster, simpler testing and development.",Cloud Development Tool,"Python





        51,718





        3,774


        Built by

          









        17 stars today",,,51718,2016-10-25T23:48:03Z
2024-03-22,https://github.com/SYSTRAN/faster-whisper,https://raw.githubusercontent.com/SYSTRAN/faster-whisper/master/README.md,"Faster Whisper is a reimplementation of OpenAI's Whisper model using CTranslate2, improving speeds up to four times while using less memory. The code integrates the Silero VAD model to minimize audio parts without speech, has a wide range of installation methods including using Docker or pip, and allows transcriptions to run to completion by gathering segments in a list or a loop. Unlike openai-whisper, it doesn't require FFmpeg to be installed. It offers several community integrations and a script to convert any Whisper models compatible with the Transformers library.",Enhancing Transcription Speed and Efficiency with Faster-Whisper Transcription & CTranslate2,"Discover the efficiency and power of Faster-Whisper, a reimplementation of OpenAI's Whisper model using CTranslate2. This method significantly boosts model performance, promising up to 4 times the speed and less memory usage, all while maintaining the same accuracy level.",Learn how to use Faster-Whisper and CTranslate2 to enhance audio transcription capabilities. Its efficiency can even be further improved with 8-bit quantization on both CPU and GPU.,AI Inference Platform,"Python





        8,005





        645


        Built by

          









        24 stars today",,https://www.youtube.com/watch?v=0u7tTptBo9I,8005,2023-02-11T09:17:27Z
2024-03-22,https://github.com/ezelikman/quiet-star,https://raw.githubusercontent.com/ezelikman/quiet-star/main/README.md,"The Quiet-STaR project modifies the base Mistral implementation in Huggingface's 'transformers' package to teach the models to ""think"" before speaking. Patches were implemented on version 4.37.0.dev0, and it is advised to use this version for replication. However, the model does not learn to avoid generating start and end thought tokens, which need to be masked out during inference. An 8-thought-token model, including start and end tokens, is available on Huggingface, but users are warned that changes to the original code may impact the function of this project.
",Utilizing Quiet-STaR: Learning How Language Models Think Before Speaking,"Uncover the workings of Quiet-STaR - a language model that learns to think before speaking. Its implementation patches the base Mistral in Huggingface transformers. However, be cautious, the model isn't taught to generate start/end thought tokens which need to be masked during inference.","Explore the implementation of Quiet-STaR in this guide, teaching how this language model has self-taught to think before acting. Learn about its integration with Mistral on Huggingface transformers.",AI Development Platform,"Python





        120





        19


        Built by

          





        37 stars today",,,120,2024-03-17T18:58:18Z
2024-03-22,https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator,https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/README.md,"The ""Chat with your data"" Solution Accelerator is a tool that merges Azure AI Search and Large Language Models (LLMs) to form a conversational search interface. This tool integrates an Azure OpenAI GPT model and an Azure AI Search index, produced from user data, into a web app to allow a natural language interface for search queries. Users can manage technical setup, drag and drop files, and point to storage to convert documents. Admins can also customize the system to tailor responses for the intended audience. The accelerator includes useful features such as file uploading, easy prompt configuration and multiple chunking strategies.
",Enhance Your Conversational Searches with Chat With Your Data Solution Accelerator,"Discover how you can unlock the power of Azure AI Search and Large Language Models with the Chat with your data Solution Accelerator. Transform and integrate your data to create a conversational search experience, all integrated into a web application with security and authentication. Customise it to fit your specific needs and enjoy features like speech-to-text functionality and multi-chunking strategies.","Leverage the Azure AI Search and Large Language Models with the Chat with your data Solution Accelerator. Develop a customisable, integrated and secure conversational search experience for your web application.",AI Development Platform,"Python





        409





        209


        Built by

          









        0 stars today",https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/media/userStory.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/media/cwyd-solution-architecture.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/media/web-unstructureddata.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/media/web-nlu.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/media/teams-cwyd.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/media/oneClickDeploy.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/./media/admin-site.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/./media/web-unstructureddata.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/media/supportingDocuments.png; https://raw.githubusercontent.com/Azure-Samples/chat-with-your-data-solution-accelerator/main/media/customerTruth.png,,409,2023-06-06T01:40:48Z
2024-04-06,https://github.com/FoundationVision/VAR,https://raw.githubusercontent.com/FoundationVision/VAR/main/README.md,"VAR, or Visual Autoregressive Modeling, presents a novel image generation method through ""next-scale prediction,"" distinguishing itself from traditional raster-scan approaches. This GPT-like autoregressive innovation surpasses existing diffusion models in performance and introduces scalable image generation with significant advancements. The method has been proven through the discovery of scaling laws within VAR transformers, showcasing efficiency and effectiveness in generating high-quality images. Notably, VAR demonstrates exceptional zero-shot generalizability, indicating its robust and versatile capabilities in image synthesis. The models are accessible on the official PyTorch implementation and a demo platform for interactive exploration. This breakthrough is poised to redefine visual generation technologies, offering tools and insights for further advancements in the field, as detailed in their paper and supported by various resources for practical engagement with VAR models.",Elevating Image Generation: VAR Surpasses Diffusion Models with Visual Autoregressive Techniques,"Discover the cutting-edge in image generation with VAR (Visual Autoregressive Modeling), a transformative approach that outpaces traditional diffusion models by focusing on next-scale prediction methods. This method, detailed in their latest publication, offers unprecedented image quality and efficiency, marking a significant leap forward in autoregressive models. Explore the potential of VAR through interactive demos or delve into the technical details via provided resources. Furthermore, VAR's achievement in discovering scaling laws and demonstrating zero-shot generalizability sets new standards in the field. Whether for academic purposes or practical application, VAR's novel method is reshaping the landscape of visual generation.","Explore how VAR (Visual Autoregressive Modeling) is revolutionizing image generation by outperforming diffusion models with next-scale prediction, offering cutting-edge quality and efficiency in visuals.",Image Generation Platform,"Python





        505





        11


        Built by

          






        130 stars today",,,505,2024-04-01T16:53:18Z
2024-04-06,https://github.com/princeton-nlp/SWE-agent,https://raw.githubusercontent.com/princeton-nlp/SWE-agent/main/README.md,"The SWE-agent project transforms Language Models (LMs) like GPT-4 into software engineering agents capable of resolving bugs in real GitHub repositories with a state-of-the-art success rate of 12.29% on the SWE-bench test set. Developed by Princeton University researchers, this system utilizes an Agent-Computer Interface (ACI) designed to streamline the interaction between the LM and code repositories, significantly outperforming baseline agents without such a refined interface. The project emphasizes the importance of specific features like a syntax-checking linter, a specialized file viewer, and a directory search tool for enhancing the agent's efficiency. SWE-agent is available for use and testing, with options for a Docker-based setup or a development version via conda, including detailed instructions for API key configuration. It allows users to deploy the agent on GitHub issues, aiming to automatically solve them and potentially submit pull requests. The project is open for contributions, offers a Discord community for discussions, and is licensed under MIT.",Revolutionizing Software Engineering: SWE-agent's Breakthrough with LMs,"Discover how SWE-agent, developed by Princeton University researchers, utilizes Language Models like GPT-4 to fix bugs in real GitHub repositories. Achieving remarkable success, SWE-agent resolves 12.29% of issues on the SWE-bench platform, showcasing state-of-the-art performance. The success is attributed to the Agent-Computer Interface (ACI), enhancing interaction with repositories for more effective bug fixes. SWE-agentâ€™s innovative features, including a linter and tailored file viewer, mark a significant advancement in software engineering, reshaping how we approach coding problems.","Learn how SWE-agent leverages GPT-4 and an innovative Agent-Computer Interface to solve real-world GitHub issues, delivering unprecedented success rates and pushing the boundaries of software engineering.",AI Development Platform,"Python





        6,885





        603


        Built by

          









        1,212 stars today",https://raw.githubusercontent.com/princeton-nlp/SWE-agent/main/assets/swe-agent-banner.png; https://raw.githubusercontent.com/princeton-nlp/SWE-agent/main/assets/results+preview.png,,6886,2024-04-02T04:09:47Z
2024-04-06,https://github.com/InstantStyle/InstantStyle,https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/README.md,"InstantStyle, developed by the InstantX Team, is a novel framework aimed at enhancing text-to-image generation by effectively separating style and content in images. This is achieved through two primary strategies: subtracting content text features from image features to decouple style and content, and injecting information into specific style-related blocks within a deep neural network. These blocks are identified to capture essential stylistic elements such as color and spatial layout. The framework also introduces a technical report and provides demos of its stylized synthesis capabilities, including image-based stylization and comparisons with previous work. Compatibility with existing models like IP-Adapter is ensured, and usage instructions alongside code for leveraging this method are provided. Moreover, resources for further exploration and a Gradio demo for practical application are discussed. The InstantStyle team encourages support via sponsorship and invites citations for the utilization of their framework in research. Contact details for inquiries are provided, enhancing the accessibility of this innovative approach to the community.",Exploring InstantStyle: A Revolutionary Text-to-Image Generation Framework,"InstantStyle introduces a novel approach in text-to-image generation, focusing on the separation of style and content through innovative techniques. By leveraging the capabilities of CLIP's global features and precise attention layer targeting, it achieves unparalleled style preservation. This framework's effectiveness is showcased through various demonstrations, including stylized synthesis and image-based stylized synthesis, with comparisons to previous methods highlighting its advancements. Interested users can engage with InstantStyle through a detailed technical report, downloadable pre-trained models, and an accessible Gradio demo. InstantStyle not only sets a new standard in digital imagery but also invites further exploration and support from the community.","Discover InstantStyle, the cutting-edge framework for text-to-image generation that masterfully disentangles style from content. Learn how its unique methods transform digital imagery through our comprehensive guide.",Image Generation Platform,"Python





        241





        14


        Built by

          







        56 stars today",https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/subtraction.png; https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/tree.png; https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/example1.png; https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/example2.png; https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/example3.png; https://raw.githubusercontent.com/InstantStyle/InstantStyle/main/assets/comparison.png,,241,2023-12-22T03:21:58Z
2024-04-06,https://github.com/infiniflow/ragflow,https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md,"RAGFlow is an open-source Retrieval-Augmented Generation (RAG) engine that emphasizes deep document understanding and integrates Large Language Models (LLMs) to offer accurate question-answering capabilities with reliable citations. It stands out for its deep knowledge extraction from complex formatted unstructured data, template-based chunking, reduced hallucinations through grounded citations, and compatibility with various data sources. RAGFlow facilitates an automated workflow suitable for both personal and business scales, providing configurable LLMs, intuitive APIs, and a streamlined orchestration process. The setup involves prerequisites like CPU and RAM specifications, Docker installation, and server startup instructions, including Docker image building and system configuration adjustments. The community is encouraged to contribute, and future plans are outlined in the RAGFlow Roadmap 2024.",Leveraging RAGFlow for Enhanced Document Understanding and Workflow Efficiency,"Discover how RAGFlow revolutionizes document understanding and workflow efficiency by offering a unique Retrieval-Augmented Generation (RAG) engine. This powerful tool combines Large Language Models (LLMs) with complex data understanding to provide accurate, citation-backed answers across various formats. Perfect for businesses of any size, RAGFlow supports an expansive array of data types and simplifies integration with intuitive APIs. Its key features like template-based chunking and reduced hallucinations in citations make it an indispensable asset for improving data handling and decision-making processes.","Explore how RAGFlow's RAG engine with deep document understanding and LLMs transforms business workflows through accurate data interpretation, streamlined processes, and diverse data compatibility.",Deep Learning Platform,"Python





        2,461





        152


        Built by

          









        313 stars today",https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.png,,2461,2023-12-12T06:13:13Z
2024-04-06,https://github.com/python/typeshed,https://raw.githubusercontent.com/python/typeshed/main/README.md,"Typeshed is a repository offering external type annotations for the Python standard library, builtins, and third-party packages, enhancing tools like static analysis, type checking, and autocompletion. It's targeted at Python 3.8 and newer versions. Users utilizing type checkers (e.g., mypy, pyright) don't directly interact with Typeshed, as standard library stubs come bundled with these checkers, and third-party stubs can be installed via PyPI. Typeshed's versioning for third-party stubs mirrors the versioning of the corresponding packages, aiding in compatibility. Contributors and users are encouraged to report issues and contribute to Typeshed, while further guidance on usage and versioning strategies is provided to ensure effective type checking. Typeshed also includes a special `_typeshed` package for additional utility types, not available at runtime. The project encourages discussion and feedback through its GitHub issue tracker, typing discussion forum, and a Gitter chat room for informal conversations.",Enhancing Python Development: An Overview of Typeshed's Role,"Typeshed plays a crucial role in the Python ecosystem by providing external type annotations for both the standard library and third-party packages. These annotations are essential for static analysis, type checking, type inference, and autocompletion, thereby enhancing code quality and developer productivity. Typeshed supports Python versions 3.8 and up, making it a versatile tool for modern Python developers. For seamless integration, typeshed annotations can be installed directly from PyPI, ensuring up-to-date type coverage. Furthermore, the project invites contributions, making it a collaborative effort to improve Python typing.","Explore how Typeshed enhances Python development by providing external type annotations for static analysis, type checking, and more. Learn how to use and contribute to Typeshed.",Python Libraries Collection,"Python





        4,031





        1,652


        Built by

          









        2 stars today",,,4031,2015-03-05T04:51:28Z
2024-04-06,https://github.com/PhonePe/mantis,https://raw.githubusercontent.com/PhonePe/mantis/main/README.md,"Mantis is a beta-phase command-line tool designed for asset discovery, reconnaissance, and scanning. It inputs top-level domains to discover assets like subdomains and certificates, conducts reconnaissance on active assets, and performs scans for vulnerabilities, secrets, misconfigurations, and phishing domains using a combination of open-source and custom tools. Features include automated discovery, recon and scan, distributed scanning, easy scan customization, dashboard support, vulnerability management, advanced alerting, DNS service integration, and quick tool integration. Installation options include Docker, demanding specific system requirements due to its CPU-intensive nature. The dashboard setup is detailed post-installation. Mantis offers various command-line options for different scanning modes, including the onboard and scan modes for TLDs, IPs, IP ranges, and CIDRs. Contributions are welcomed through bug reporting, feature requests, or pull requests. The development team includes Prateek Thakare, Bharath Kumar, Hitesh Kumar, Saddam Hussain, Pragya Gupta, Dhruv Shekawat, Santanu Sinha, Praveen Kanniah, with special thanks to Ankur Bhargava. Usage is legal only with consent, and the developers disclaim liability for misuse.",Maximize Your Cybersecurity with Mantis: A Comprehensive Automated Scanning Solution,"Discover Mantis, the innovative command-line framework designed for automating asset discovery, reconnaissance, and vulnerability scanning. With Mantis, users can input top-level domains to initiate a thorough process that identifies subdomains, certificates, and performs extensive scans for vulnerabilities, secrets, misconfigurations, and phishing domains. This tool is built on a combination of open-source and custom tools, ensuring a seamless and efficient workflow for cybersecurity professionals. Mantis supports a distributed scanning feature, allowing for scans to be divided across multiple machines, and boasts easy scan customization, making it an essential tool for robust cyber defense strategies.","Learn how Mantis, an advanced command-line framework, automates the entire process of asset discovery, reconnaissance, and scanning for vulnerabilities, secrets, and misconfigurations, streamlining your cybersecurity efforts.",Cybersecurity Tool,"Python





        469





        56


        Built by

          








        82 stars today",https://raw.githubusercontent.com/PhonePe/mantis/main/./images/mantis.png; https://raw.githubusercontent.com/PhonePe/mantis/main/./images/Assets.png; https://raw.githubusercontent.com/PhonePe/mantis/main/./images/Vulnerabilities.png,https://www.youtube.com/watch?v=3PCRIJEOTWo,469,2023-08-12T14:44:57Z
2024-04-06,https://github.com/myshell-ai/JetMoE,https://raw.githubusercontent.com/myshell-ai/JetMoE/main/README.md,"JetMoE-8B is a groundbreaking model trained on less than $100,000, surpassing LLaMA2-7B, a model from Meta AI with significantly higher budget resources. This achievement suggests that training large language models (LLMs) can be more cost-effective than previously believed. JetMoE-8B is open-source, relying solely on public datasets, and is designed to be fine-tuned on a minimal compute budget, making it especially suitable for academia. Despite having only 2.2B active parameters during inference, which reduces computational costs, JetMoE-8B consistently outperforms similar-sized models and even larger models like LLaMA2-7B and DeepseekMoE-16B in various benchmarks. It demonstrates superior performance across a broad spectrum, including on the Open LLM Leaderboard and MT-Bench Score, highlighting its efficiency and effectiveness. The project, contributed by Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin, encourages collaboration and is supported by MyShell.ai for further open-source endeavors.",Achieving Peak AI Performance: JetMoE Surpasses LLaMA2 with Minimal Budget,"Discover how JetMoE-8B outperforms Meta AI's LLaMA2-7B with less than $0.1 million spent, setting a new benchmark for cost-effective AI training. This breakthrough demonstrates that high-quality machine learning model training can be significantly cheaper and more accessible than previously believed. Fully open-sourced, JetMoE-8B uses only public datasets and requires minimal compute resources for finetuning, making it ideal for academic research and development. Remarkably, it maintains superior performance while using only 2.2B active parameters during inference, reducing computational costs.","JetMoE-8B revolutionizes AI training, outperforming LLaMA2 with a fraction of the budget. Explore how this open-sourced model offers superior performance with minimal costs, making advanced AI accessible to all.",Large-scale Language Models,"Python





        383





        20


        Built by

          





        120 stars today",https://raw.githubusercontent.com/myshell-ai/JetMoE/main/resources/2-performance.png,,383,2024-04-02T02:43:16Z
2024-04-06,https://github.com/posit-dev/great-tables,https://raw.githubusercontent.com/posit-dev/great-tables/main/README.md,"**Great Tables** enables easy creation of visually appealing tables in Python, focusing on flexibility and user-friendly customization options. It begins with table data in DataFrame format from Pandas or Polars, allowing users to determine the composition and formatting of their output tables for use in consoles, notebooks, or Quarto documents. The package is designed for simplicity in common tasks with options for advanced customization. An example provided illustrates creating a table from a dataset, demonstrating formatting capabilities for dates, currency, and numbers, while hiding specific columns. **Great Tables** includes ten datasets for experimentation and extensive documentation. The community can engage through GitHub Discussions, a Discord server, or Twitter for support, ideas, and discussions. Installation is straightforward via PyPI. The project encourages community participation under a contributor code of conduct and is maintained by Posit Software, PBC, under the MIT license, with governance by Rich Iannone and Michael Chow.",Master Table Creation in Python with Great Tables: A Comprehensive Guide,"Discover the power of Great Tables for generating visually appealing tables in Python effortlessly. With its user-friendly interface, you can construct a wide variety of tables using customizable components such as headers, footers, and row labels. Great Tables supports data from Pandas or Polars DataFrame, allowing for flexible table composition and formatting options. Whether for console printouts, notebook integrations, or Quarto document renderings, Great Tables enhances data presentation significantly. Dive into the comprehensive documentation to explore the myriad possibilities!","Learn how to effortlessly create stunning tables in Python using Great Tables. Explore customizable components, data integration from Pandas or Polars DataFrame, and various rendering options to enhance your data presentation.",Python Libraries Collection,"Python





        727





        23


        Built by

          









        79 stars today",https://raw.githubusercontent.com/posit-dev/great-tables/main/./docs/assets/datasets.png,,727,2022-05-06T20:11:05Z
2024-04-06,https://github.com/YaoFANGUK/video-subtitle-remover,https://raw.githubusercontent.com/YaoFANGUK/video-subtitle-remover/main/README.md,"The Video-subtitle-remover (VSR) is an AI-based software designed to remove hard subtitles from videos, offering several features including lossless resolution subtitle removal, filling in the subtitle region post-removal using advanced AI algorithms, support for custom subtitle locations, automatic removal across entire videos, and the ability to batch remove watermarks from multiple images. It is available for Windows, macOS, and Linux, requiring Python 3.8 or newer and an Nvidia GPU (GTX 1060 minimum). The software comes with a GUI version for ease of use, and installation involves downloading a package or setting up a conda environment for those who prefer or require source code access. The text also includes guidance on software usage, including troubleshooting tips for improving subtitle removal speed or quality and addresses potential issues such as CondaHTTPError or errors related to 7z file extraction. Additionally, the document mentions the importance of installing CUDA and cuDNN for leveraging GPU capabilities and concludes with a section on sponsorship, showing appreciation towards donors.",Revolutionize Video Editing with VSR: Remove Subtitles Seamlessly,"Discover the power of Video-Subtitle-Remover (VSR), the AI-based software designed to effortlessly remove hard-coded subtitles from videos without compromising resolution. With its advanced AI algorithms, VSR fills in the text-removed areas flawlessly, supports custom subtitle positions for selective removal, and even allows for the automatic elimination of all text across videos. Perfect for a variety of operating systems, VSR caters to your video editing needs with precision and ease.","Leverage Video-Subtitle-Remover (VSR) for seamless subtitle removal from videos. Utilize advanced AI for flawless filling, support custom positions, and enjoy comprehensive video editing across multiple platforms.",AI Task Automation,"Python





        1,590





        217


        Built by

          





        49 stars today",https://github.com/YaoFANGUK/video-subtitle-remover/raw/main/design/demo.png; https://github.com/YaoFANGUK/video-subtitle-remover/raw/main/design/demo2.gif; https://github.com/YaoFANGUK/video-subtitle-remover/raw/main/design/demo.gif; https://i.328888.xyz/2023/03/31/iwVoeH.png; https://i.328888.xyz/2023/03/31/iwVThJ.png; https://i.imgur.com/EMCP5Lv.jpeg,,1590,2023-10-25T02:50:01Z
2024-04-06,https://github.com/MervinPraison/PraisonAI,https://raw.githubusercontent.com/MervinPraison/PraisonAI/main/README.md,"Praison AI is a low-code framework designed to facilitate the creation and management of multi-agent systems for various large language model (LLM) applications. It focuses on ease of use, customization, and efficient human-agent interaction, utilizing either AutoGen, CrewAI, or other agent frameworks. Users can install Praison AI via pip, initialize it to create agent configurations (e.g., for generating a movie script about a dog on the moon), and run it either in basic, advanced, or full automatic modes. The system allows for the specification of distinct agent roles and tasks within an agents.yaml playbook, supporting both simple and detailed project outlines. In addition to its core functionalities, Praison AI accommodates deployment on cloud platforms and offers instructions for including the package in projects, contributing to the project, and utilizing alternative models like Ollama, FastChat, LM Studio, and Mistral API.",Unlocking the Potential of Multi-Agent Systems with Praison AI,"Discover how Praison AI simplifies the creation of multi-agent systems for LLM applications, making it easier for developers to innovate. This low-code, centralized framework focuses on ease of use, customization, and interaction between humans and agents. Whether you're initiating a project or deploying sophisticated agent frameworks, Praison AI offers a streamlined process for developing diverse applications. Learn how to install, initialize, and deploy your first agent with our step-by-step guide.","Find out how Praison AI revolutionizes the development of multi-agent systems for LLM applications with a low-code approach, emphasizing customization and human-agent interaction.",Collaborative AI Framework,"Python





        205





        56


        Built by

          





        25 stars today",,,205,2024-03-19T16:45:25Z
2024-04-06,https://github.com/Codium-ai/pr-agent,https://raw.githubusercontent.com/Codium-ai/pr-agent/main/README.md,"CodiumAI's PR-Agent is a tool designed to enhance the handling of pull requests (PRs) through AI-driven feedback and suggestions. It supports a range of commands and tools across various platforms including GitHub, GitLab, Bitbucket, and Azure DevOps, offering features like PR review, code improvement suggestions, question answering, update of changelogs, documentation generation, and more, with certain features exclusive to PR-Agent Pro. Recent updates include pip package installation ease, new review tool features, and a knowledge-base website. PR-Agent works by compressing PRs for manageable LLM prompts, emphasizing real-life usage with a single GPT-4 call per tool for fast and affordable responses. It supports multiple git providers, usage methods, and models. Its privacy policy ensures that user data is not stored or used for training in the Pro version. PR-Agent stands out for its practical use, PR compression strategy, modular and customizable tools, across-provider support, and data privacy assurances.",Streamline Your Development Process with CodiumAI's PR-Agent,"CodiumAI introduces PR-Agent, a groundbreaking tool designed to streamline the software development process. By providing AI-driven feedback and suggestions on pull requests, PR-Agent enhances code quality and collaboration. Equipped with features like similar code identification, documentation generation, and custom suggestions based on preset guidelines, it makes code review more efficient. Plus, its compatibility with various platforms and integration into different workflows make it an essential tool for developers aiming to improve their code's quality and maintainability.","Discover how CodiumAI's PR-Agent can transform your development workflow with AI-powered feedback, enhancing code quality, and making pull requests handling more efficient.",AI Coding Assistant,"Python





        3,923





        307


        Built by

          









        12 stars today",https://codium.ai/images/pr_agent/logo-light.png; https://codium.ai/images/pr_agent/multiple_pr_themes.png; https://codium.ai/images/pr_agent/similar_code_global2.png; https://codium.ai/images/pr_agent/wiki_configuration.png; https://www.codium.ai/images/pr_agent/describe_new_short_main.png; https://www.codium.ai/images/pr_agent/review_new_short_main.png; https://www.codium.ai/images/pr_agent/improve_new_short_main.png; https://www.codium.ai/images/pr_agent/geneare_custom_labels_main_short.png; https://www.codium.ai/images/reflect_and_review.gif; https://www.codium.ai/images/ask-2.gif; https://www.codium.ai/images/improve-2.gif; https://www.codium.ai/images/demo-2.gif; https://codium.ai/images/pr_agent/diagram-v0.9.png; https://raw.githubusercontent.com/Codium-ai/codiumai-vscode-release/main/media/docs/Joincommunity.png,,3923,2023-07-05T21:02:15Z
2024-04-06,https://github.com/swisskyrepo/PayloadsAllTheThings,https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/README.md,"Payloads All The Things is a comprehensive repository dedicated to Web Application Security, offering an extensive list of payloads and bypass techniques for security testing and research. The project encourages community contributions, including direct payload suggestions, method improvements, and financial sponsorships. It features a structured documentation system with chapters on various vulnerabilities, each containing a README for vulnerability exploitations, sets for Burp Intruder, images, and referenced files. Moreover, it includes a rich collection of methodologies and resources on several areas like Active Directory Attacks, cloud pentesting, Linux and Windows security, network pivoting, and more. The repository also promotes learning through a selection of books and YouTube videos. Contributions are welcome, as outlined in their contributing guide, and the project appreciates sponsorship from individual donors and companies, highlighting community collaboration and support in the cybersecurity field.",Mastering Web Application Security: Comprehensive Guide to Payloads and Bypasses,"Discover the ultimate resource for enhancing web application security through payloads and bypasses. Dive into an extensive collection compiled by cybersecurity enthusiasts. Learn, contribute, and enrich the repository with your unique techniques and payloads. Collaborate with the community through pull requests or by becoming a sponsor. Explore the vast documentation and contribute to continually fortifying web security.","Explore an exhaustive guide to payloads and bypasses for web application security. Contribute your techniques, learn from the community, and help improve web security.",Cybersecurity Tool,"Python





        56,414





        13,909


        Built by

          








        22 stars today",https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png,,56414,2016-10-18T07:29:07Z
2024-04-06,https://github.com/sczhou/ProPainter,https://raw.githubusercontent.com/sczhou/ProPainter/main/README.md,"ProPainter is a project by Shangchen Zhou, Chongyi Li, Kelvin C.K. Chan, and Chen Change Loy from S-Lab at Nanyang Technological University, presented at ICCV 2023. It focuses on advancing video inpainting through enhanced propagation and transformer techniques. The project provides a comprehensive solution for video processing tasks like object removal and video completion, showcased with impressive visual results on the project page. Since its initiation, ProPainter has introduced updates such as integration with OpenXLab and Hugging Face for online demos, features for memory-efficient inference, and the availability of the code and model for public use. However, watermark removal demos were removed to prevent unethical use. The project encourages community engagement, inviting users to star the repository if they find it helpful. Detailed installation instructions, dataset preparation guides, training, and evaluation steps are meticulously documented, making it accessible for further academic and practical applications. The project also acknowledges its basis on previous works and contributions, particularly noting the support in creating Gradio demos.",Unveiling ProPainter: A Leap in Video Inpainting Technology,"ProPainter, developed by researchers from Nanyang Technological University, demonstrates significant advancements in video inpainting with the integration of propagation techniques and transformer technology. Presented at ICCV 2023, this groundbreaking project offers practical applications for removing objects and completing videos seamlessly. With comprehensive demos available on platforms like OpenXLab and Hugging Face, ProPainter sets a new benchmark for memory-efficient inference and model accessibility. Explore the full capabilities and contribute to this open-source project to redefine the possibilities of video editing and content creation.","Discover ProPainter, the cutting-edge video inpainting tool by NTU, showcased at ICCV 2023. Explore demos and contribute to enhancing video editing innovation.",Video Generation Tool,"Python





        4,534





        552


        Built by

          







        13 stars today",https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/propainter_logo1_glow.png; https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/object_removal1.gif; https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/object_removal2.gif; https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/video_completion1.gif; https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/video_completion2.gif; https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/video_completion3.gif; https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/video_completion4.gif; https://raw.githubusercontent.com/sczhou/ProPainter/main/assets/ProPainter_pipeline.png,https://www.youtube.com/watch?v=92EHfgCO5-Q,4534,2023-09-01T13:11:57Z
2024-04-07,https://github.com/nashsu/FreeAskInternet,https://raw.githubusercontent.com/nashsu/FreeAskInternet/main/README.md,"FreeAskInternet is an innovative, completely free, and locally-run search aggregation and answer generation application that does not require a GPU. It's designed to provide users with privacy and independence from cloud services, utilizing LLM without the need for OpenAI or Google API keys. By leveraging searxng for multi-engine searches, it combines results with ChatGPT3.5 to generate responses to queries, all processed locally. Key features include being entirely free, local, private, and run without LLM hardware. It offers a fast, Docker Compose-deployable system with a user-friendly web and mobile interface. Initial setup involves cloning from GitHub and deploying via Docker Compose. This project, still in early development, might experience bugs. It's licensed under Apache-2.0, acknowledging contributions from ChatGPT-Next-Web, FreeGPT35, and searxng.",Discover FreeAskInternet: The Ultimate Private Search Platform Without GPU Needs,"FreeAskInternet revolutionizes online search by offering a completely free, private, and local search aggregator and answer generator that operates without the need for a GPU. Users can ask questions, receiving answers generated from multi-engine search results without relying on external APIs. Designed to run seamlessly on any computer, this innovative project ensures privacy and ease of use, eliminating dependency on OpenAI or Google APIs. Ideal for those seeking a web and mobile-friendly interface without the cost or privacy concerns of traditional search tools.","Unlock the full potential of online searching with FreeAskInternet - a free, local, and private search aggregator and answer generator that doesn't require a GPU. Experience fast, secure, and easy-to-deploy searches right from your device.",AI Development Platform,"Python





        1,260





        104


        Built by

          





        305 stars today",,,1260,2024-04-05T06:37:53Z
2024-04-07,https://github.com/facebookresearch/schedule_free,https://raw.githubusercontent.com/facebookresearch/schedule_free/main/README.md,"Schedule-Free Learning introduces a new way to train models in PyTorch without the need for specifying learning rate schedules or stopping times. Developed by Aaron Defazio and colleagues, it offers an innovative alternative to traditional training methods, featuring SGDScheduleFree and AdamWScheduleFree as its primary implementations. This approach, utilizing a combination of interpolation and averaging to replace momentum, allows for either superior or on-par performance compared to state-of-the-art schedules with the same memory requirements as the base optimizer. Its unique mechanism requires switching the parameter buffer for gradient calls and evaluation, with specific adjustments needed for models using BatchNorm. Schedule-Free Learning stands out by not necessitating a decreasing learning rate schedule, making training less dependent on learning rate adjustments while still being compatible with learning rate warmup. Despite its advantages, tuning, including adjustments to learning rates and regularization, is crucial for optimal results. The method is seen as a blend of primal averaging and accelerated methods, offering rapid early-stage convergence without sacrificing stability. Additionally, it remains compatible with, and potentially enhanced by, techniques like Stochastic Weight Averaging but without their additional complexity and memory demands.",Revolutionizing PyTorch Training: Introducing Schedule-Free Learning,"Discover the cutting-edge Schedule-Free Optimizers in PyTorch, a groundbreaking approach brought to life by a team of expert researchers. This innovative method eliminates the need for pre-defined stopping schedules, offering a flexible, efficient pathway to faster training results. By blending interpolation with averaging, Schedule-Free learning not only simplifies the training process but also matches or outperforms state-of-the-art scheduling methods, all with the same memory footprint as traditional optimizers. Dive into how this technique seamlessly integrates into your PyTorch projects, promising to redefine machine learning practices.","Explore Schedule-Free Learning in PyTorch for faster, more efficient training without pre-set schedules. Learn how this innovative method simplifies training workflows and achieves top performance with minimal memory requirements.",Deep Learning Tool,"Python





        678





        25


        Built by

          









        293 stars today",,,678,2024-03-27T20:15:30Z
2024-04-07,https://github.com/stanfordnlp/pyreft,https://raw.githubusercontent.com/stanfordnlp/pyreft/main/README.md,"Pyreft is a representation fine-tuning (ReFT) library developed to adapt internal language model (LM) representations effectively and efficiently. It supports fine-tuning pre-trained LMs on HuggingFace using ReFT and allows for easy sharing of fine-tuned results. Pyreft is designed to utilize a fraction of the parameters required by state-of-the-art (SoTA) fine-tuning methods, potentially leading to better performance. It also aims to make fine-tuning more interpretable. The library provides the flexibility to set ReFT hyperparameters via configurations and supports a variety of interventions for adjusting LM parameters. Corrections to the initial preprint, including swapped hyperparameter settings and a wrong citation, will be updated. Installation can be done directly from the repository, and it includes examples to guide users in applying ReFT for tasks like memorization or language model instruction tuning. Users can reproduce significant ReFT results, explore its interpretability, and even train models comparable to ChatGPT-3.5 within minutes using a single GPU. Pyreft emphasizes the advantages of ReFT over parameter-efficient fine-tuning (PEFT) methods in terms of efficiency, interpretability, and flexibility. The library is backed by the pyvene project and comes with guidelines and license notices for proper usage, alongside citations for academic referencing.",Revolutionize Language Modeling with Pyreft: The Future of Fine-Tuning,"Discover pyreft, the latest in representation fine-tuning (ReFT) for language models. Pyreft offers a robust, parameter-efficient way to enhance pre-trained language models with minimal fine-tuning cost. Its support for various models, easy HuggingFace integration, and potential for interpretable AI research marks a significant step forward. Dive into the world of efficient and effective language model fine-tuning with pyreft â€“ your gateway to innovative language processing.","Explore pyreft, a cutting-edge library for representation fine-tuning (ReFT) in language models. Learn how pyreft enables efficient, effective, and interpretable fine-tuning with minimal costs. Start your journey towards advanced language processing today.",Language Models Tool,"Python





        242





        13


        Built by

          








        50 stars today",,,242,2024-02-17T02:36:45Z
2024-04-07,https://github.com/hunshcn/gh-proxy,https://raw.githubusercontent.com/hunshcn/gh-proxy/master/README.md,"gh-proxy is a speed acceleration project for GitHub release, archive, and repository files, supporting cloning with both a serverless version on Cloudflare Workers and a Python version. A public demo is available, but due to its limited capacity, large-scale users are encouraged to deploy their own. The Python version allows for file size restrictions, specific user/repo banning or whitelisting, and pass-by features. Users can access the service by prefixing URLs with `https://gh.api.99988866.xyz/`. For accessing private repositories, a specific cloning method is provided. Instructions for deploying both the Cloudflare Workers and Python versions are detailed, including registration, script deployment, and Docker or direct installation for Python. The project has undergone several updates since its initial release, improving functionality such as clone support and raw file handling. Contributions to support the creator are welcome.",Boost Your GitHub Experience: Introducing gh-proxy for Faster Downloads,"gh-proxy is an acceleration project for GitHub release, archive, and project files, supporting cloning with both a serverless Cloudflare Workers version and a Python version. It's designed to help users facing slow download speeds or those in regions where GitHub access is restricted. Users are encouraged to deploy their own instance for heavy usage, as the public demo at https://gh.api.99988866.xyz/ faces high demand. Differences between Python and CF worker versions include file size limitations and support for specific user/repo whitelisting or blocking.","Discover gh-proxy, a GitHub acceleration tool for faster downloads of releases, archives, and project files, available in Cloudflare Workers and Python versions. Learn how it works, its benefits, and how to deploy your own instance for an enhanced GitHub experience.",Open Source Tool,"Python





        6,021





        1,514


        Built by

          









        45 stars today",https://img.maocdn.cn/img/2021/04/24/imagea272c95887343279.png; https://img.maocdn.cn/img/2021/04/24/image.md.png; https://www.helloimg.com/images/2021/04/24/BK9vmb.md.png,,6021,2020-03-22T00:56:45Z
2024-04-07,https://github.com/Marker-Inc-Korea/AutoRAG,https://raw.githubusercontent.com/Marker-Inc-Korea/AutoRAG/main/README.md,"AutoRAG is an AutoML tool designed to help users find the best Retrieve and Generate (RAG) pipeline for their specific data needs. It allows for the automatic evaluation of various RAG modules using custom evaluation data, aiming to facilitate the identification of the optimal RAG pipeline for any given use case. It supports easy evaluation of multiple RAG module combinations and offers a quick way to start with a high-performance pipeline tailored to the user's data. The tool simplifies sharing experiment results and configurations, encourages community interaction via Discord, and provides comprehensive guides through documentation and Colab tutorials. AutoRAG emphasizes ease of use, from installation with a simple pip command to the execution of evaluations and deployments of the best RAG pipelines. It is open-source, inviting contributions and further development from the community.",Maximize Your Data's Potential with AutoRAG: The Ultimate AutoML Tool for RAG Pipelines,"Discover the power of AutoRAG, the innovative AutoML tool designed to find the optimal RAG pipeline for your data effortlessly. With an intuitive setup process and support for a wide range of RAG modules, AutoRAG simplifies the evaluation of various RAG combinations, ensuring you identify the best performing pipeline for your specific use-case. Ideal for data scientists and AI engineers, AutoRAG offers a streamlined path to enhanced model performance and efficiency. Start optimizing your RAG pipeline today and unlock the full potential of your data with AutoRAG.","Explore how AutoRAG, an advanced AutoML tool, effortlessly finds the best RAG pipeline for your data, optimizing performance and efficiency for data experts.",Machine Learning Tool,"Python





        452





        23


        Built by

          







        26 stars today",,https://www.youtube.com/watch?v=2ojK8xjyXAU,452,2024-01-10T12:25:00Z
2024-04-07,https://github.com/KrisCris/Palworld-Pal-Editor,https://raw.githubusercontent.com/KrisCris/Palworld-Pal-Editor/master/README.md,"The Palworld Pal Editor, created with love by _connlost, is a tool specifically designed for use with the Steam version of Palworld. It allows players to perform various actions on their Pals (creatures in the game), such as spawning, duplicating, deleting, showing/hiding, inspecting stats, changing genders or nicknames, and editing their abilities. The tool also supports unlocking viewing cages in multiplayer servers, listing players and their Pals, and various other modifications like changing Pal levels, soul levels, passive skills, and more. It provides backup functionality to prevent save data corruption and supports multiple languages, including English, Japanese, and Simplified Chinese. Available for use in different formats, users can access it as a pre-built binary, via pip installation, through a dedicated Docker container, or by directly running the source code. Additional features include optional command-line arguments and a customizable config file. The tool welcomes contributions, bug reports, and feature requests via its GitHub issues section and encourages community engagement through its Discord server.",Maximize Your Palworld Experience: A Comprehensive Guide to the Palworld Pal Editor,"Discover how the Palworld Pal Editor, crafted with love by _connlost, revolutionizes your gaming experience. This powerful tool not only offers a plethora of features like pal spawning, stat inspections, and comprehensive save management but also supports multiple languages, ensuring accessibility for a global audience. Its intuitive design caters to both beginners and advanced players, promising a seamlessly enhanced Palworld journey. Remember, always back up your save files to prevent any loss.","Explore the Palworld Pal Editor to unlock, customize, and manage your Pals with ease. This guide covers everything from installation to advanced features, making it a must-read for all Palworld enthusiasts.",Game Development Tool,"Python





        49





        9


        Built by

          







        4 stars today",,https://www.youtube.com/watch?v=v9U60jj5Ugw,49,2024-02-24T08:08:54Z
2024-04-07,https://github.com/bregman-arie/devops-exercises,https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/README.md,"This repository contains a collection of over 2624 exercises and questions on various technical topics, including those related to DevOps and Site Reliability Engineering (SRE). While these materials can aid interview preparation, they may not directly reflect actual interview questions. The content emphasizes that becoming a DevOps engineer involves understanding certain concepts rather than mastering all the listed technologies. Contributors are encouraged to add more exercises through pull requests, with guidelines available for potential contributors. The repository showcases a wide array of topics such as DevOps, Git, Network, Hardware, Kubernetes, Software Development, Cloud technologies (AWS, Azure, GCP), Programming languages (Python, Go, Perl), Containers, CI/CD, Security, and many more, indicating a broad spectrum of areas relevant to modern computing environments.",Mastering DevOps: Prepare for Your Next Interview with These Technical Questions and Exercises,"Discover the ultimate repository for mastering DevOps and SRE principles. Boasting over 2624 exercises and questions, this collection covers a vast array of technical topics essential for prospective DevOps engineers. While the questions span across numerous technologies and concepts, they aim to provide a solid preparation ground for interviews, despite not mirroring actual interview questions precisely. Dive into a learning journey tailored for DevOps career aspirants and contribute to the ever-growing list of exercises by engaging with the community through pull requests.","Unlock your potential in DevOps and SRE roles with a comprehensive guide featuring over 2624 exercises and questions. Ideal for interview preparation, learn crucial technical topics, and contribute to the DevOps community.",Software Development,"Python





        63,061





        13,779


        Built by

          









        26 stars today",https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops_exercises.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/git.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/network.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/hardware.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/kubernetes.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/programming.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/python.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/Go.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/perl.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/regex.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/cloud.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/aws.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/azure.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/googlecloud.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/openstack.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/os.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/linux.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/virtualization.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/dns.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/bash.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/databases.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/sql.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/mongo.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/testing.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/big-data.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/cicd.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/certificates.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/containers.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/openshift.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/storage.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/terraform.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/puppet.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/you.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/ansible.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/observability.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/prometheus.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/circleci.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/datadog.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/grafana.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/argo.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/HR.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/security.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/chaos_engineering.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/general.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/elastic.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/kafka.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed/distributed_design_standby.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed/distributed_design_lb.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/cdn-no-downtime.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/producers_consumers_issue.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/producers_consumers_fix.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/input-process-output.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/system_design_notebook.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/sre_checklist.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/how_they_devops.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops_resources.png; https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/infraverse.png,,63061,2019-10-03T17:31:21Z
2024-04-07,https://github.com/huggingface/autotrain-advanced,https://raw.githubusercontent.com/huggingface/autotrain-advanced/main/README.md,"AutoTrain Advanced is a user-friendly, no-code platform that simplifies the training and deployment of machine learning models. It allows users to easily train models with just a few clicks, though it necessitates that data be uploaded in the correct format. While AutoTrain itself is free, costs are incurred based on resource usage on Hugging Face Spaces or on personal infrastructure when running locally. Installation involves using PIP for the AutoTrain-Advanced Python package, requiring Python version 3.10 or higher, along with additional dependencies like git lfs, torch, torchaudio, and torchvision. Running AutoTrain optimally involves setting up a conda environment. The platform also provides Google Colab notebooks for tasks like LLM Fine Tuning and DreamBooth Training. Detailed documentation and pricing information are accessible online.",Unlock Machine Learning Potential with AutoTrain Advanced,"Introducing AutoTrain Advanced: the no-code platform revolutionizing how you deploy state-of-the-art machine learning models quickly and efficiently. With just a few clicks, users can train models without needing to write a single line of code. Ensure your data is in the correct format to start creating projects seamlessly. Explore our documentation for guidance on data formats and to understand our cost-effective pricing structure. Dive into the future of machine learning with AutoTrain Advanced - where innovation meets simplicity.","Discover AutoTrain Advanced, the no-code solution for fast, easy training and deployment of machine learning models. Learn how it simplifies the process, provides cost-effective pricing, and supports your journey towards advanced machine learning capabilities.",AI Development Platform,"Python





        2,990





        353


        Built by

          









        17 stars today",,,2990,2020-12-15T18:23:25Z
2024-04-07,https://github.com/kwai/DouZero,https://raw.githubusercontent.com/kwai/DouZero/main/README.md,"DouZero is an innovative reinforcement learning framework designed to master DouDizhu, China's most popular card game, characterized by its complex game mechanics involving competition, collaboration, and imperfect information. Developed by Kwai Inc.'s AI Platform, DouZero uses a self-play deep reinforcement learning method, significantly outperforming existing AI programs in this domain. By integrating deep neural networks with Monte-Carlo methods, action encoding, and parallel actors, DouZero efficiently handles DouDizhu's large state and action spaces. The system's effectiveness is demonstrated through its first-place ranking on the Botzone leaderboard among 344 AI agents, showcasing the potential of classic Monte-Carlo methods in complex action spaces. The project, including an online demo, training code, and an in-depth paper discussing the DMC algorithm and other foundational aspects, is openly shared for community use and further research.",Achieving Card Game Mastery with DouZero: Insights from ICML 2021,"DouZero, an AI framework developed by Kwai Inc., stands out as a remarkable advancement in mastering DouDizhu, China's most popular card game. This framework utilizes self-play deep reinforcement learning to tackle the game's inherent challenges, including imperfect information and a vast action space. Demonstrated at ICML 2021, DouZero's application showcases the AI's capacity to learn and compete in complex environments, promising implications beyond gaming. With online demos and comprehensive resources available, DouZero paves the way for future AI research and development in game theory and strategic learning.","Explore how DouZero, presented at ICML 2021, revolutionizes playing DouDizhu through self-play deep reinforcement learning, marking a significant milestone in AI's application in complex games.",Deep Learning Tool,"Python





        3,867





        547


        Built by

          









        17 stars today",https://raw.githubusercontent.com/kwai/DouZero/main/imgs/douzero_logo.jpg; https://douzero.org/public/demo.gif,https://www.youtube.com/watch?v=inHIi8sej7Y,3867,2021-06-02T01:55:56Z
2024-04-08,https://github.com/comfyanonymous/ComfyUI,https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/README.md,"ComfyUI is a powerful, modular GUI and backend for creating and executing complex stable diffusion workflows without coding, through a graph/nodes/flowchart interface. It supports various versions of Stable Diffusion including SD1.x, SD2.x, SDXL, along with features like Stable Video Diffusion and Stable Cascade. The platform offers numerous advancements such as an asynchronous queue system, optimizations for workflow execution, low VRAM, and CPU options, among others. Users can experiment with a wide range of models and methods like embeddings/textual inversion, hypernetworks, and inpainting. It allows for complex creations like area composition, model merging, and high-res fixes. Additionally, workflows can be saved/loaded in multiple formats, and it provides multiple shortcuts for efficiency. Installation options cover Windows, Linux, and macOS, with support for AMD, NVIDIA, and Intel Arc GPUs. ComfyUI operates completely offline, ensuring fast start-ups and data privacy.",Unlock Creativity with ComfyUI: The Ultimate Stable Diffusion GUI & Backend,"Discover ComfyUI, the most advanced and modular stable diffusion GUI that transforms your creative workflow. With its intuitive nodes/graph/flowchart interface, you can experiment and create complex Stable Diffusion workflows effortlessly, no coding required. Fully compatible with various versions of Stable Diffusion, including SD1.x, SD2.x, and exclusive features like asynchronous queues, optimizations, and support for numerous models and embeddings, ComfyUI stands as the premier choice for both beginners and experts aiming to push the boundaries of AI-driven creativity.","Explore the revolutionary features of ComfyUI for Stable Diffusion - a user-friendly GUI that enables complex workflow creation without coding. Compatibility with SD1.x, SD2.x, and more, alongside unique features like async queues and model support, makes it the top choice for enhancing your creative process.",AI Development Platform,"Python





        31,057





        3,232


        Built by

          









        113 stars today",https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/comfyui_screenshot.png,,31057,2023-01-17T03:15:56Z
2024-04-08,https://github.com/Libr-AI/OpenFactVerification,https://raw.githubusercontent.com/Libr-AI/OpenFactVerification/main/README.md,"Loki is an open-source tool created to automate the fact verification process. It's designed for journalists, researchers, and anyone interested in accurate information, dissecting texts into individual claims, assessing their verification worthiness, and searching for evidence to validate them. It comprises several components, including Decomposer, Checkworthy, Query Generator, Evidence Crawler, and ClaimVerify. To use, one needs Python 3.9+, with installation instructions provided for cloning the repository and installing required packages. The tool supports various inputs (text, speech, image, video) and integrates a web app for easy interaction. Contributions are welcomed, and the project is under an MIT license. Additionally, a Supporter Edition offers advanced features for a nominal support, contributing to future innovations and community growth.",Unlock the Power of Fact-Checking with Loki: The Premier Open-Source Tool,"Discover Loki, the revolutionary open-source tool designed to transform how we verify facts. This cutting-edge solution automates the verification process, breaking down texts into manageable claims, evaluating their verification worthiness, and scouring the internet for evidence to confirm or debunk them. Ideal for journalists, researchers, and truth-seekers, Loki empowers users to distinguish fact from fiction efficiently. Learn how to get started and contribute to the future of information integrity. Subscribe for updates and become part of our community on this journey towards trustworthy information.","Explore Loki, an innovative open-source fact verification tool perfect for journalists and researchers. Streamline the process of distinguishing fact from fiction with Loki's advanced features.",AI Task Automation,"Python





        551





        16


        Built by

          








        117 stars today",https://raw.githubusercontent.com/Libr-AI/OpenFactVerification/main/./fig/librai_librai.png; https://raw.githubusercontent.com/Libr-AI/OpenFactVerification/main/./fig/cmd_example.gif; https://raw.githubusercontent.com/Libr-AI/OpenFactVerification/main/./fig/web_input.png; https://raw.githubusercontent.com/Libr-AI/OpenFactVerification/main/./fig/web_result.png,,551,2024-03-25T09:24:14Z
2024-04-08,https://github.com/open-mmlab/mmdetection3d,https://raw.githubusercontent.com/open-mmlab/mmdetection3d/main/README.md,"MMDetection3D, part of the OpenMMLab project, is an open-source object detection toolbox for 3D objects based on PyTorch, aimed at the next-generation platform for general 3D detection. Compatible with PyTorch 1.8 and newer, it supports various multi-modality/single-modality detectors like MVXNet, VoteNet, and PointPillars, applicable for both indoor and outdoor 3D detection across popular datasets (e.g., ScanNet, SUNRGB-D, Waymo). Highlighting its integration with MMDetection for 2D detection, MMDetection3D promises high efficiency and encompasses around 300+ models from over 40 research papers, with benchmarks indicating superiority in training speed. Recent updates include improvements and new supports like reflactoring the Waymo dataset and extending supports for camera-based 3D object detection models. Installation, getting started guides, and benchmark results can be found in their documentation. The platform encourages community contributions and provides a comprehensive model zoo.",Exploring 3D Object Detection with MMDetection3D: A Comprehensive Guide,"Discover the cutting-edge capabilities of MMDetection3D, the open-source toolbox for 3D object detection part of the OpenMMLab project. Learn how it supports multi-modality/single-modality detectors, offers out-of-the-box support for both indoor and outdoor 3D detection, and seamlessly integrates with 2D detection. With its high efficiency and extensive model zoo, MMDetection3D is designed to accelerate your next 3D detection project.","Unlock the full potential of 3D object detection with MMDetection3D. This blog post dives into its features, support for various datasets, natural 2D detection integration, and its high-efficiency training capabilities.",Deep Learning Tool,"Python





        4,738





        1,440


        Built by

          









        4 stars today",https://raw.githubusercontent.com/open-mmlab/mmdetection3d/main/resources/mmdet3d-logo.png; https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png; https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png; https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png; https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png; https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png; https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png; https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png; https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png; https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png; https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png; https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png; https://raw.githubusercontent.com/open-mmlab/mmdetection3d/main/resources/mmdet3d_outdoor_demo.gif,,4738,2020-07-08T03:39:45Z
2024-04-08,https://github.com/eosphoros-ai/DB-GPT,https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.md,"DB-GPT is an open-source AI framework designed to ease the development of data applications by utilizing large models and databases, transforming how enterprises and developers construct bespoke applications with minimal coding. It integrates multiple technical capabilities like multi-model management, Text2SQL optimization, RAG framework, and multi-agents framework collaboration. DB-GPT supports seamless integration with various data sources, facilitating the construction of knowledge bases and efficient data retrieval and processing. It includes a fine-tuning framework boasting an impressive accuracy rate on the Spider dataset, outperforming even GPT-4. The project also introduced the DB-GPT-Hub for high-performance Text2SQL workflows and supports a wide array of Large Language Models (LLMs). With features focusing on private domain Q&A, multi-data source integration, and automated Text2SQL fine-tuning, DB-GPT aims at revolutionizing database interactions in the Data 3.0 era. It underscores privacy and security in handling data. For installation, Docker, Linux, macOS, and Windows are supported. The project invites contributions and is licensed under MIT, offering extensive documentation and a vibrant community for users and contributors.",Revolutionizing Database Technology with DB-GPT's AI Framework,"DB-GPT introduces a groundbreaking open source AI-native data app development framework, leveraging advanced technologies like AWEL and cutting-edge multi-agent system collaborations. It streamlines the creation of large model applications, making data manipulation simpler and more efficient. With its robust infrastructure, DB-GPT aids in developing bespoke applications with minimal coding, paving the way for innovation in the Data 3.0 era. Explore how DB-GPT is setting new standards in database interactions and application development through its comprehensive features and user-friendly approach.","Discover DB-GPT, an advanced AI-native data app development framework that revolutionizes database interactions with its comprehensive features like multi-agent collaboration, Text2SQL optimization, and more for efficient application development.",Collaborative AI Framework,"Python





        10,697





        1,320


        Built by

          









        9 stars today",https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/./assets/LOGO.png; https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/./assets/dbgpt.png,,10697,2023-04-13T14:52:43Z
2024-04-08,https://github.com/TMElyralab/MuseTalk,https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/README.md,"MuseTalk introduces a real-time, audio-driven lip-syncing model capable of 30fps+ on NVIDIA Tesla V100, aimed at enhancing virtual human presentations. It modifies faces in videos according to the input audio across various languages and allows for real-time inference. Trained in the latent space with a strategy that significantly influences the generation results based on the center point of the face region, MuseTalk is paired with MuseV for full virtual human solutions. It involves cross-attention fusion of image and audio embeddings, using architectures from stable-diffusion-v1-4 and whisper-tiny models. Several cases showcase its capacity to animate still photos and dub videos, highlighting its application in creating lifelike virtual interactions. Installation instructions, necessary software components, and weights for different models are provided for users. The project, while offering advancements, notes limitations in resolution, identity preservation, and some jitter, with ongoing improvements. MuseTalk is shared openly for both academic and commercial uses, adhering to responsible AI use guidelines.",Unlocking Real-Time High-Quality Lip Sync with MuseTalk: A Comprehensive Guide,"Introducing MuseTalk, a groundbreaking real-time lip-syncing model achieving over 30fps on NVIDIA Tesla V100. This audio-driven technology, compatible with various languages, offers a seamless integration with MuseV for creating lifelike virtual humans. It revolutionizes digital communication by modifying unseen facial features in videos, enhancing user engagement in online content. Discover MuseTalk's potential in gaming, animation, and virtual meetings.","Discover MuseTalk, an innovative lip-sync model enabling real-time high-quality video modifications. Perfect for virtual human creation, this tool supports multiple languages and offers seamless integration with input videos.",Deep Learning Tool,"Python





        201





        17


        Built by

          








        24 stars today",https://raw.githubusercontent.com/TMElyralab/MuseTalk/main/assets/figs/musetalk_arc.jpg,,201,2024-03-26T04:14:37Z
2024-04-08,https://github.com/Picsart-AI-Research/StreamingT2V,https://raw.githubusercontent.com/Picsart-AI-Research/StreamingT2V/main/README.md,"The StreamingT2V repository introduces an advanced autoregressive technology for creating long videos from text descriptions, ensuring temporal consistency, dynamic motion, and high-quality imagery. Developed by a team led by Roberto Henschel, Levon Khachatryan, and Daniil Hayrapetyan, this method can generate videos up to 2 minutes long and potentially longer, with this capability being independent of the specific Text2Video model employed, allowing for future improvements. The official release of the paper and model occurred in March and April 2024, respectively, with demonstration and tool availability via GitHub, Hugging Face, and a Gradio demo. The project facilitates the generation of text-to-video or image-to-video, showcasing versatility across different base models with detailed inference time statistics provided for each. The code is under a CreativeML Open RAIL-M license, and the authors encourage citation for academic use.",Unlocking the Future of Video Generation with StreamingT2V Technology,"Discover StreamingT2V, the groundbreaking technique transforming text into dynamic, long-duration videos without losing quality or consistency. This autoregressive approach not only crafts videos that perfectly align with descriptive text but also maintains high-quality imagery over extended frames. Capable of generating videos up to two minutes or longer, StreamingT2V stands out for its adaptability across various Text2Video models, promising even greater achievements with future advancements. Explore how this innovative tool is set to revolutionize video creation, offering limitless possibilities for creators and researchers alike.","StreamingT2V introduces a novel technique for creating consistent and high-quality long videos from text, promising unprecedented advancements in video generation technology.",Video Generation Tool,"Python





        504





        21


        Built by

          






        43 stars today",https://raw.githubusercontent.com/Picsart-AI-Research/StreamingT2V/main/__assets__/github/teaser/teaser_final.png,https://www.youtube.com/watch?v=GDPP0zmFmQg,504,2024-03-18T04:33:26Z
2024-04-08,https://github.com/Vonng/ddia,https://raw.githubusercontent.com/Vonng/ddia/master/README.md,"""Designing Data-Intensive Applications,"" originally by Martin Kleppmann and translated by å†¯è‹¥èˆª, is a comprehensive guide exploring the design of data-intensive applications. It covers from fundamental data structures to high-level architecture, beneficial for architects, DBAs, backend engineers, and product managers alike. The book combines theory with practical insights gained from real-world experiences, emphasizing understanding concepts rather than memorizing definitions. It provides a conceptual framework for designing, implementing, and evaluating data systems, empowering readers to see through technical hype. The content encompasses reliability, scalability, maintainability, data models, queries, storage, retrieval, encoding, distributed data, consensus, batch processing, stream processing, and future directions for data systems. This work is heralded as one of the best technical books in recent years, vital for anyone working with data-intensive applications, aiming to contribute to the spread of advanced technology culture.",Exploring the Chinese Translation of Designing Data-Intensive Applications,"Discover the essence of data system design through the Chinese translation of 'Designing Data-Intensive Applications' by Martin Kleppmann, meticulously translated by å†¯è‹¥èˆª. This book, a blend of theory and practice, is ideal for architects, DBAs, backend engineers, and product managers. Navigate complex concepts with ease, thanks to a translation that prioritizes clarity and depth. A must-read for anyone eager to bypass technical jargon and achieve a solid understanding of data systems.","Dive into the Chinese translation of 'Designing Data-Intensive Applications' for an accessible guide on navigating the complexities of data systems, suitable for professionals across fields.",System Design Education,"Python





        19,101





        4,074


        Built by

          









        8 stars today",,,19102,2018-02-08T04:26:52Z
2024-04-08,https://github.com/karpathy/nanoGPT,https://raw.githubusercontent.com/karpathy/nanoGPT/master/README.md,"nanoGPT is a repository for training and fine-tuning medium-sized GPT models, aiming to be a simple and efficient alternative. It updates minGPT by focusing on practicality over comprehensiveness and is currently capable of reproducing GPT-2 (124M) performance on OpenWebText in about 4 days using a single 8XA100 40GB node. The codebase is intentionally minimal, making it easy to understand and modify, with the main training and model definition scripts being around 300 lines each. It also supports loading GPT-2 weights from OpenAI for fine-tuning purposes. The project includes instructions for a quick start, even on limited hardware like a MacBook, and further details on reproducing GPT-2 results, baselining with OpenAI's checkpoint results, fine-tuning on new datasets, and sampling from trained models. It mentions future to-dos for enhancing the project and provides troubleshooting advice, especially regarding the use of PyTorch 2.0 features. Acknowledgments are given to Lambda labs for GPU support, and the repository encourages community interaction via Discord.","Unlocking GPT Potentials: A Guide to nanoGPT for Fast, Efficient Model Training","Discover nanoGPT, the ultimate tool for efficiently training and finetuning GPT models, making it simpler and faster than ever. Dive into its user-friendly code and how it reproduces GPT-2 models on a single node. Whether it's hacking the code for custom training or finetuning pre-existing models, nanoGPT offers a streamlined path. Plus, with concise installation and setup processes, you're equipped to start experimenting with GPT models right away. Explore how nanoGPT stands as a pivotal development in the field of language modeling, inviting both novices and professionals to delve into the world of GPT.","Explore nanoGPT, the simplified way to train and finetune GPT models efficiently. Learn how its user-friendly design caters to both beginners and professionals, facilitating fast, effective language model training.",Language Models Tool,"Python





        30,978





        4,632


        Built by

          









        41 stars today",https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/nanogpt.jpg; https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/gpt2_124M_loss.png,https://www.youtube.com/watch?v=kCc8FmEb1nY,30978,2022-12-28T00:51:12Z
2024-04-09,https://github.com/agiresearch/AIOS,https://raw.githubusercontent.com/agiresearch/AIOS/main/README.md,"AIOS introduces a ground-breaking concept where a Large Language Model (LLM) functions as the core of an operating system, making the OS more intelligent, versatile, and capable of managing resources efficiently, facilitating agent multitasking, and providing extensive toolkits for developers. This development marks a significant step toward achieving artificial general intelligence (AGI) by embedding the ""brain"" into the operating system. Recent updates to the AIOS codebase include a shell simulator and enhancements for rapid API calls and testing. The project encourages community engagement through its Discord channel and is looking for contributions to enrich its ecosystem further. Detailed guidelines for installation, usage, and contribution are provided, alongside contacts for further inquiries and a vibrant community for discussion. This initiative opens new avenues for developing intelligent, responsive, and deeply integrated operating systems for future technology landscapes.",Exploring AIOS: The Operating System Powering Next-Generation LLM Agents,"Discover AIOS, a revolutionary Large Language Model (LLM) Agent Operating System designed to seamlessly integrate LLMs into the core of operating systems. AIOS aims to create 'operating systems with soul' by enabling advanced resource allocation, concurrent execution of agents, and providing extensive toolkits for LLM Agent developers. The system supports a dynamic AI ecosystem for building, deploying, and managing intelligent agents with ease. With its latest codebase updates, including shell simulator and API enhancements, AIOS is setting new standards in the quest towards achieving Artificial General Intelligence (AGI). Join the vibrant AIOS community on Discord to engage in discussions, development, and more.","Unveil the capabilities of AIOS, the LLM Agent Operating System designed for AGI, featuring optimized resource allocation, concurrent agent execution, and a rich toolkit for developers. Join the AIOS community today.",AI Development Platform,"Python





        2,079





        209


        Built by

          









        131 stars today",https://raw.githubusercontent.com/agiresearch/AIOS/main/images/AIOS-Architecture.png,,2079,2024-01-15T03:52:41Z
2024-04-09,https://github.com/huggingface/text-generation-inference,https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md,"Text Generation Inference (TGI) is a deployment and serving toolkit for Large Language Models (LLMs), designed to enable high-performance text generation. It supports popular LLMs like Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, among others, and offers various optimization features. These include token streaming, continuous request batching, optimized transformer codes, quantization, watermarking, and custom prompt generation. TGI is production-ready, supporting tensor parallelism for multiple GPUs and distributed tracing. It offers comprehensive API documentation and can be deployed using Docker, with support for Nvidia, AMD, Inferentia, Intel GPU, and Gaudi hardware. Users can access protected resources using the `HUGGING_FACE_HUB_TOKEN`. TGI facilitates local installations, development, and running models with options for quantization to reduce VRAM requirements. It includes testing frameworks and detailed instructions for running models, both locally and using Docker, emphasizing the importance of shared memory and distributed tracing in enhancing performance and tracing capabilities.",Maximizing Text Generation with HuggingFace's Inference Toolkit,"Discover how HuggingFace's Text Generation Inference (TGI) toolkit empowers high-performance deployment of Large Language Models (LLMs). Featuring a Rust, Python, and gRPC server, TGI supports the most popular open-source LLMs for production-ready services. It enhances text generation through features like tensor parallelism, token streaming, continuous batching, and optimized transformers code. Learn to use TGI for deploying models with Docker and leverage fine-tuning support for enhanced accuracy and performance.","Unlock the full potential of Large Language Models with HuggingFace's Text Generation Inference toolkit. Explore features for high-performance text generation, including tensor parallelism and optimized code. Get started with TGI today.",Deep Learning Tool,"Python





        7,573





        808


        Built by

          









        49 stars today",https://huggingface.co/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png; https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png,https://www.youtube.com/watch?v=jlMAX2Oaht0,7573,2022-10-08T10:26:28Z
2024-04-09,https://github.com/Vision-CAIR/MiniGPT4-video,https://raw.githubusercontent.com/Vision-CAIR/MiniGPT4-video/main/README.md,"MiniGPT4-Video is a multimodal Large Language Model (LLM) tailored for comprehending videos by processing both visual elements and textual data over time. Building on MiniGPT-v2's success in mapping visual features to LLM for images, MiniGPT4-Video extends these capabilities to sequences of frames, enabling video understanding. This model excels in answering queries that involve both visual and textual questions, outperforming current state-of-the-art methods on multiple benchmarks (MSVD, MSRVTT, TGIF, TVQA) by significant margins. Utilizing a speech to text model like Whisper to generate subtitles, it inputs both video and text to produce answers, showing remarkable improvements in video comprehension. The paper and demo links provide further resources, including the setup for environment, model checkpoints for training, and evaluation scripts for assessing model performance. Moreover, it credits MiniGPT4 and Video-ChatGPT projects and is licensed under the BSD 3-Clause License.",Revolutionizing Video Understanding with MiniGPT4-Video,"Introducing MiniGPT4-Video, a cutting-edge multimodal Large Language Model (LLM) tailored for video understanding. By interweaving visual and textual tokens, it surpasses traditional methods in comprehending the complexity of videos. The model significantly outperforms existing techniques across major benchmarks and incorporates text from videos for deeper analysis. Explore the demo and delve into the methodology behind this advancement in video understanding technology.",Discover how MiniGPT4-Video is setting new standards in video understanding by seamlessly integrating visual and textual information for unparalleled analysis.,Vision-Language Model,"Python





        246





        18


        Built by

          







        42 stars today",https://raw.githubusercontent.com/Vision-CAIR/MiniGPT4-video/main/repo_imgs/sample_1.gif; https://raw.githubusercontent.com/Vision-CAIR/MiniGPT4-video/main/repo_imgs/sample_2.gif; https://raw.githubusercontent.com/Vision-CAIR/MiniGPT4-video/main/repo_imgs/sample_3.gif; https://raw.githubusercontent.com/Vision-CAIR/MiniGPT4-video/main/repo_imgs/MiniGPT4-video_fig.jpg,,246,2024-03-23T08:10:57Z
2024-04-09,https://github.com/AiGptCode/Iphone-14-15-IRAN-Anten,https://raw.githubusercontent.com/AiGptCode/Iphone-14-15-IRAN-Anten/main/README.md,"This text provides a detailed guide on permanently enabling the antenna on iPhone 14 and 15 without tracking or blocking, using two different methods. It includes source codes and instructions to create a unique UUID with a Python-scripted uuid Generator, making profiles untraceable due to the unique identifier. The process involves modifying template codes for compatibility with all operators and utilizing tools like pyinstaller to convert Python files into executable ones. It also explains using Apple Configurator or Imazing Profile Editor for profile creation, emphasizing the importance of unique UUIDs to prevent configuration closure from multiple users sharing a single UUID. Key steps include resetting the network, configuring APN settings, testing, and troubleshooting tips like adjusting for better signal or switching to 2G at night if issues arise. The guide is geared towards maintaining 4G internet access, highlighting the necessity to customize profiles per network or device specifics. Donation details for supporting these efforts are provided at the end.",How to Permanently Enable Antenna on iPhone 14 & 15: A Comprehensive Guide,"Learn to enhance your iPhone 14 and 15's connectivity without the risk of tracking or shutting down using two distinctive methods, including source code and a unique UUID file creation. Dive deep into a simplified process involving a Python-written UUID generator, transforming it effortlessly into an executable file, alongside steps to ensure every device enjoys a unique UUID to avoid service interruptions. Tailor your deviceâ€™s proxy or IP settings for an optimized profile, ensuring reliable internet access. Plus, discover an alternative using Apple Configurator or Imazing profile editor for both MacOS and Windows users, ensuring your device's network settings are perfect for optimal internet connectivity across varied operators.",Unlock the full potential of your iPhone 14 and 15 with our expert guide on antenna activation without tracking. Discover step-by-step instructions on creating a unique UUID and configuring your device for the best connectivity.,Cybersecurity Tool,"Python





        264





        64


        Built by

          







        43 stars today",https://i.ibb.co/Ld4vsKJ/IMG-5707.jpg; https://i.ibb.co/yND1vj7/IMG-5664.jpg,,264,2023-10-23T18:36:02Z
2024-04-09,https://github.com/ManimCommunity/manim,https://raw.githubusercontent.com/ManimCommunity/manim/main/README.md,"Manim is an open-source animation engine designed for creating precise and instructional math videos, famously used by [3Blue1Brown](https://www.3blue1brown.com/). While it is inspired by 3Blue1Brown's original Manim library, this community version is maintained separately to provide regular updates and improvements. The engine allows users to script animations directly using Python, offering extensive flexibility in how contents are presented. For users interested in trying Manim without local installation, an online Jupyter environment is provided. Installation instructions vary by operating system and are detailed in the project's documentation. Manim supports numerous command line arguments for rendering, including flags for previewing, quality control, and skipping animations. The project is open for contributions, especially in documentation and testing, although it's currently undergoing major refactoring which might limit new feature additions. Community support is available through Discord and Reddit. The project emphasizes proper citation for academic use and adheres to a specific code of conduct to maintain a respectful community. Manim is dual-licensed under the MIT license by both 3blue1brown LLC and the Manim Community Developers.",Unlock the Power of Math Visualization with Manim: A Comprehensive Guide,"Delve into the world of mathematical storytelling with Manim, the dynamic animation engine famed for bringing 3Blue1Brown's math videos to life. This community-driven project offers an accessible platform for creating intricate math visuals, regardless of your coding experience. Get started with easy installation instructions and dive into crafting your very first animation, exploring a vast array of examples for inspiration. Whether you're a teacher, student, or simply a math enthusiast, Manim opens up a new realm of creativity and learning. Join the vibrant community today and transform how you experience mathematics.","Discover how to create captivating math animations with Manim, the engine behind 3Blue1Brown. Follow our step-by-step guide to easily get started. Perfect for educators and enthusiasts alike.",Animation Engine,"Python





        18,497





        1,407


        Built by

          









        233 stars today",https://raw.githubusercontent.com/ManimCommunity/manim/main/logo/cropped.png; https://raw.githubusercontent.com/ManimCommunity/manim/main/docs/source/_static/command.png,,18497,2020-05-19T02:37:13Z
2024-04-10,https://github.com/karpathy/minGPT,https://raw.githubusercontent.com/karpathy/minGPT/master/README.md,"minGPT is a compact, educative PyTorch re-implementation of OpenAI's GPT, designed for both training and inference, encapsulating the essence of GPT in approximately 300 lines of code for clarity and educative purposes. It demonstrates how a sequence of indices enters a Transformer and produces a probability distribution for the next index, emphasizing efficient batching for performance. Despite its semi-archived status as of January 2023, favoring a shift towards nanoGPT for more advanced features, minGPT remains a valuable resource, extensively referenced in various educational and development contexts. The library comprises three main files for the model, Byte Pair Encoder, and training boilerplate, along with several demos and projects illustrating its application. Installation and usage are straightforward, supporting GPT-2 configurations and offering Python training examples. Planned enhancements include GPT-2 fine-tuning, dialog agents, and various training improvements, alongside references to seminal GPT papers highlighting model intricacies across its iterations.","Understanding minGPT: A Compact, Educational PyTorch GPT Implementation","Discover minGPT, a lean PyTorch re-implementation of GPT for both training and inference, designed for clean, interpretable, and educational purposes. With just 300 lines of code, minGPT demystifies the GPT model structure by encapsulating it in a comprehensible format. It efficiently handles sequences through clever batching, providing a probability distribution over the next index in a sequence with minimal complexity. Although now semi-archived, minGPT's creator has introduced nanoGPT for further advancements, while minGPT continues to serve as a valuable educational tool for understanding GPT models.","Explore minGPT, an educational PyTorch re-implementation of GPT that simplifies model understanding and training. Ideal for learning and small projects, minGPT brings GPT closer to all.",Deep Learning Tool,"Python





        18,424





        2,219


        Built by

          









        47 stars today",https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt.jpg,,18424,2020-08-17T07:08:48Z
2024-04-10,https://github.com/LlamaFamily/Llama-Chinese,https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/README.md,"Llama-Chinese is an advanced technical community focused on optimizing and building upon the Llama model for Chinese language processing. It enhances the Chinese capabilities of the Llama2 model by leveraging large-scale Chinese data for continuous iteration and upgrades. The community warmly welcomes developers and researchers passionate about large language models (LLMs) to join and contribute. The community is supported by a team of senior NLP engineers committed to optimizing Llama2 for Chinese, fostering innovation through events, and sharing resources and models to push forward the development of Chinese NLP technology.

The community has introduced the Atom model, jointly developed with Atom Echo, ranking in the top ten of the Chinese big model leaderboard C-Eval. Atom models, including Atom-7B and Atom-13B, have been optimized for the Chinese language through large-scale Chinese data pre-training, more efficient Chinese vocabularies, and adaptive context extension.

Several resources and tools are provided for the community, including downloadable model links, model deployment guides, model fine-tuning and pre-training codes, as well as acceleration and evaluation resources. Additionally, the community integrates with LangChain for developing document retrieval, chatbots, and agent applications, and acknowledges contributions from AtomEcho, Coremesh, and community members for their support and contributions.",Unlocking the Power of Llama-Chinese: A Guide to Enhanced Chinese Language Processing,Welcome to Llama-Chinese Community! Discover how we optimize the Llama model for Chinese language processing with advanced technologies. Our dedicated NLP engineers and a supportive community offer professional guidance. Dive into our optimized Atom models for superior performance in Chinese text handling. Join us to innovate and elevate Chinese NLP together.,"Explore Llama-Chinese for advanced Chinese language processing. Learn about our community, support from NLP engineers, and our optimized Atom models for exceptional Chinese text handling performance. Join us in leading Chinese NLP innovation.",Large-scale Language Models,"Python





        9,167





        857


        Built by

          









        16 stars today",https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/llama.png; https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/./assets/ceval.jpg; https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/./assets/llama_eval.jpeg; https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/./assets/wechat.jpeg,,9167,2023-07-19T04:45:23Z
2024-04-10,https://github.com/PaddlePaddle/PaddleOCR,https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/README.md,"The text is about PaddleOCR, an enriching, advanced, and practical OCR (Optical Character Recognition) toolkit designed to help developers create better models and applications. It announces recent updates, including the launch of OCR algorithm competitions, the release of PP-ChatOCRv2, updates for PP-OCR models including PP-OCRv4, and specific improvements in document scanning applications. Besides, there are mentions of past achievements and enhancements such as optimizations and new features in PP-StructureV2 and PP-OCRv3, collaborations, open-source contributions, and achievements in community engagement. The toolkit supports a wide range of front-end OCR algorithms and offers comprehensive solutions incorporating industrial-grade models like PP-OCR, PP-Structure, and PP-ChatOCRv2, facilitating the entire workflow from data production, model training, compression, to deployment predictions. It also provides quick links for online experiences, rapid start instructions, and technical community exchanges, aiming to foster a helpful and professional open-source environment.",Unlocking the Power of PaddleOCR: A Comprehensive Guide,"Discover the versatility of PaddleOCR, a leading OCR toolset designed for developers to build and enhance models for practical applications. From the latest updates including the PP-ChatOCRv2 and PP-OCRv4 series to the extensive support for multiple languages and operating systems, PaddleOCR is shaping up to be a game-changer in OCR technology. With a community-focused approach, it promises not just superior model performance but also a pathway for developers to contribute and evolve along with the technology.","Explore the capabilities of PaddleOCR: A one-stop OCR toolkit offering advanced models like PP-ChatOCRv2 and PP-OCRv4, multi-language support, and community-driven improvements to revolutionize OCR applications.",Computer Vision Platform,"Python





        38,091





        7,204


        Built by

          









        57 stars today",https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/./doc/PaddleOCR_log.png; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/./doc/imgs_results/ch_ppocr_mobile_v2.0/test_add_91.jpg; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/./doc/imgs_results/ch_ppocr_mobile_v2.0/00006737.jpg; https://raw.githubusercontent.com/tink2123/test/master/ppocrv4.png; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic001.jpg; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic002.jpg; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/doc/imgs_results/PP-OCRv3/ch/PP-OCRv3-pic003.jpg; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/doc/imgs_results/PP-OCRv3/en/en_1.png; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/doc/imgs_results/PP-OCRv3/en/en_2.png; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/doc/imgs_results/PP-OCRv3/multi_lang/japan_2.jpg; https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/doc/imgs_results/PP-OCRv3/multi_lang/korean_1.jpg; https://user-images.githubusercontent.com/14270174/185310636-6ce02f7c-790d-479f-b163-ea97a5a04808.jpg; https://user-images.githubusercontent.com/14270174/185539517-ccf2372a-f026-4a7c-ad28-c741c770f60a.png; https://user-images.githubusercontent.com/14270174/197464552-69de557f-edff-4c7f-acbf-069df1ba097f.png; https://user-images.githubusercontent.com/14270174/185393805-c67ff571-cf7e-4217-a4b0-8b396c4f22bb.jpg; https://user-images.githubusercontent.com/14270174/185540080-0431e006-9235-4b6d-b63d-0b3c6e1de48f.jpg; https://user-images.githubusercontent.com/25809855/186094813-3a8e16cc-42e5-4982-b9f4-0134dfb5688d.png,,38091,2020-05-08T10:38:16Z
2024-04-10,https://github.com/jmerle/imc-prosperity-2-backtester,https://raw.githubusercontent.com/jmerle/imc-prosperity-2-backtester/master/README.md,"The IMC Prosperity 2 Backtester repository offers a tool for backtesting IMC Prosperity 2 algorithms, producing outputs that closely resemble those from the official submission environment, making it compatible with the IMC Prosperity 2 Visualizer. Installation and updates are done via pip. The backtester supports various commands for testing algorithms across different rounds and days, merging profit and loss data, opening results in the visualizer, customizing output files, using custom data, and debugging. It handles order matching by prioritizing order depths over market trades and enforces position limits before matching orders. Development suggestions include cloning the repository and installing it in editable mode for ease of testing changes.",Maximize Your IMC Prosperity 2 Algorithm Performance with Our Backtester,"Discover the ultimate tool for optimizing your IMC Prosperity 2 algorithms - the IMC Prosperity 2 Backtester. This powerful backtester not only offers compatibility with the official submission environment but also integrates seamlessly with the IMC Prosperity 2 Visualizer, enhancing your algorithm's performance analysis. With easy installation and comprehensive guide for varied usage scenarios, upgrading and personalizing your backtesting process has never been simpler. Dive deep into your algorithm's potential, uncover improvements, and ensure you're competition-ready with every run.","Optimize your IMC Prosperity 2 algorithms with our backtester for accurate performance analysis. Easy setup, compatible with official environments and visualizers, perfect for serious competitors.",Money Making Automation,"Python





        25





        9


        Built by

          





        2 stars today",,,25,2024-04-05T21:05:32Z
2024-04-10,https://github.com/Filimoa/open-parse,https://raw.githubusercontent.com/Filimoa/open-parse/main/README.md,"Open Parse is a flexible, easy-to-use library designed to improve the chunking of complex documents for AI applications, addressing the limitations of existing open-source libraries in handling intricate document layouts. It is visually-driven, allowing for better input to language models by analyzing documents beyond basic text splitting. It supports basic markdown for parsing document elements and offers high-precision table extraction into Markdown formats, surpassing traditional tools. Open Parse is extensible, intuitive, and easy to learn, requiring less time to understand documentation. Key examples and code snippets demonstrate its application for basic document parsing and semantic processing, highlighting its capabilities in visually discerning and effectively chunking documents, thus providing a superior solution for creating high-quality LLM inputs. Open Parse also outlines requirements, installation steps, including OCR support, and offers additional resources like cookbooks and documentation for users to engage with.",Revolutionize Document Chunking with Open Parse: The Ultimate Guide,"Open Parse emerges as a groundbreaking tool designed to transform the way we handle complex documents, surpassing traditional methods with its visually-driven, high-precision capabilities. It addresses the limitations of current open-source libraries, offering markdown support, high precision in table extraction, and extensive extensibility for custom post-processing steps. This makes it an unparalleled asset for achieving superior results in RAG systems and AI applications. Open Parse stands out by effectively grouping related content with its intuitive layout, simplifying the chunking process and enhancing document analysis.","Discover Open Parse: A superior document chunking library that offers visual analysis, markdown and high-precision table support for flawless RAG systems and AI applications. Learn how it outperforms other tools in precision and ease.",Document Conversion Tool,"Python





        1,069





        23


        Built by

          






        266 stars today",https://sergey-filimonov.nyc3.digitaloceanspaces.com/open-parse/marketing/three-sigma-wide.png,,1069,2024-03-22T05:35:12Z
2024-04-10,https://github.com/blasty/JiaTansSSHAgent,https://raw.githubusercontent.com/blasty/JiaTansSSHAgent/master/README.md,"Jia Tan's SSH Agent is a simplified tool designed to emulate certain features of the XZ sshd backdoor, making it easier for users to investigate the backdoor with a standard SSH client. Users are guided to generate their own ed448 private key and patch their liblzma.so with the ed448 pubkey. Additionally, users need to modify their SSH client to bypass certificate verification. The steps include generating the private key, patching the necessary files, activating a virtual environment, and running the agent. Finally, users can connect to SSH with any password, implying the agent allows unauthorized access.",Exploring XZ SSHD Backdoor with Jia Tan's Simple SSH Agent,"Discover the ease of testing the XZ sshd backdoor functionality with Jia Tan's Simple SSH Agent. This tool allows users to utilize their typical SSH clients to explore backdoors more effectively. Learn how to generate your ED448 private key, patch your liblzma.so with the ED448 pubkey, and adjust your SSH client for certificate verification bypassing. Perfect for enthusiasts and researchers interested in SSH security and backdoor exploration.",Step into the world of SSH security with a guide on using Jia Tan's Simple SSH Agent for exploring XZ sshd backdoor functionality. Learn the simple steps to set up and use this tool for your SSH explorations.,Cybersecurity Tool,"Python





        85





        12


        Built by

          





        30 stars today",https://raw.githubusercontent.com/blasty/JiaTansSSHAgent/master/assets/demo.png,,85,2024-04-08T13:29:54Z
2024-04-11,https://github.com/nus-apr/auto-code-rover,https://raw.githubusercontent.com/nus-apr/auto-code-rover/main/README.md,"AutoCodeRover introduces a fully automated method for resolving GitHub issues, including bug fixes and feature additions, by leveraging Large Language Models (LLMs) combined with analysis and debugging tools. This system uses code search APIs to prioritize patch locations, resolving about 22% of the 300 real-world GitHub issues in the SWE-bench lite database, which shows a significant improvement over the current AI-driven software engineering approaches. AutoCodeRover employs two main stages: context retrieval and patch generation. It uniquely uses program structure-aware code search and statistical fault localization when test suites are available, enhancing its ability to generate effective patches. The system's capability was demonstrated by successfully fixing a Django issue. AutoCodeRover encourages replication and further research by providing setup and running instructions for Docker containers, including setting up tasks in SWE-bench and running multiple tasks. For those interested in conducting similar experiments, detailed documentation and contact information are provided, acknowledging partial support from a Singapore Ministry of Education grant.",Exploring AutoCodeRover: The Future of Autonomous Program Improvement,"AutoCodeRover represents a cutting-edge leap in software engineering, offering a fully automated solution for resolving GitHub issues through combining LLMs with debugging capabilities, resulting in unprecedented patch efficiency. Tested on SWE-bench lite, it showcases a significant improvement in issue resolution, highlighting its practical application in the real-world GitHub environment. Its methodology encompasses a two-staged approach: context retrieval using code search APIs and patch generation based on the collected context, with unique features that ensure high repair rates. The potential of AutoCodeRover extends further when test cases are available, leveraging statistical fault localization to enhance the accuracy of generated patches. This innovative approach opens new horizons in automated programming and software maintenance.","Discover AutoCodeRover, an innovative automated approach for bug fixing and feature addition on GitHub using LLMs. Learn how it outperforms current AI software engineers in resolving real-world issues.",AI Development Platform,"Python





        1,454





        98


        Built by

          







        204 stars today",,,1454,2024-04-08T05:34:14Z
2024-04-11,https://github.com/aixcoder-plugin/aiXcoder-7B,https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/README.md,"The aiXcoder-7B Code Large Language Model is a state-of-the-art model designed for code completion, understanding, and generation across multiple programming languages. It outperforms similar-sized models and mainstream models like codellama 34B and StarCoder2 15B on multilingual nl2code benchmarks. Unlike previous versions, aiXcoder 7B Base focuses on real-world code generation scenarios, utilizing 1.2T unique tokens for training and designing pre-training tasks tailored for programming contexts. Future versions will include instruct-tuned models for broader programming tasks like test case generation and code debugging. The model supports environment setup via Docker or manually with requirements like Python 3.8+ and PyTorch 2.1.0+, and offers improved inference speeds with `flash attention`. Model weights are available for download, with detailed instructions for inference in command line or Python scripts. The training data is a mix of core datasets (mainstream programming and natural languages) and extended datasets, with stringent selection and filtering processes to enhance model performance. The aiXcoder-7B model not only excels in standalone method generation and code completion tasks but also demonstrates superior ability in understanding code context across files, positioning it as a robust tool for developers.",Unlock the Power of aiXcoder-7B: Revolutionizing Code Generation with Large Language Models,"Discover the groundbreaking aiXcoder-7B Code Large Language Model, designed to enhance code productivity across multiple programming languages. With its state-of-the-art performance in code completion, generation, and more, aiXcoder-7B outperforms mainstream models, making it a game-changer for developers. Explore its comprehensive training on 1.2T unique tokens, exceptional benchmark achievements, and future development plans for even more specialized programming tasks. Dive into the new era of coding with aiXcoder-7B's robust support.","Explore aiXcoder-7B, the advanced Code Large Language Model revolutionizing programming with unparalleled code completion and generation capabilities. Learn how it surpasses mainstream models and its bright future in specialized coding tasks.",AI Development Platform,"Python





        285





        14


        Built by

          







        60 stars today",https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_1.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_3.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_0.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_1.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_2.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_3.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_4.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_5.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_6.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_7.png; https://raw.githubusercontent.com/aixcoder-plugin/aiXcoder-7B/main/./assets/table_8.png,,285,2024-03-30T13:24:55Z
2024-04-11,https://github.com/InternLM/InternLM-XComposer,https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/README.md,"InternLM-XComposer2 is a cutting-edge vision-language large model (VLLM) built upon InternLM2-7B, achieving remarkable capabilities in text-image composition and comprehension. It generates coherent text-image content from diverse input forms and accurately solves complex vision-language tasks, surpassing most multimodal models and challenging benchmarks. The model is available in several versions, including a high-resolution (4KHD) version, a 7B-parameter version focused on text-image composition, and lighter versions for easier access. This release marks significant progress in multimodal AI, providing powerful tools for content creation, problem-solving, and benchmarking in the vision-language domain. The models and their accompanying technical reports, demo code, and evaluation details are openly accessible for community use, contributing to advances in multimodal research and applications.",Exploring the Capabilities of InternLM-XComposer2: A Revolution in Vision-Language Modeling,"InternLM-XComposer2 emerges as a transformative Vision-Language Large Model, setting new benchmarks with its multimodal prowess. It stands out by generating coherent text-image compositions and solving complex vision-language problems with remarkable accuracy. The model showcases groundbreaking performance, surpassing existing models and even rivaling GPT-4V in specific benchmarks. Its versatility extends over three versions, catering to high-resolution understanding and efficient content creation across various platforms.","Discover InternLM-XComposer2, a groundbreaking Vision-Language Large Model, mastering text-image compositions and challenging Q&A tasks with unparalleled accuracy and performance, outperforming leading models in diverse benchmarks.",Vision-Language Model,"Python





        1,480





        93


        Built by

          









        42 stars today",https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/logo_en.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/4k.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/4k.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/4k.png; https://raw.githubusercontent.com/ShareGPT4V/ShareGPT4V-Resources/master/images/logo_tight.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/assets/Benchmark_radar.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/assets/4khd_radar.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/InternLM/InternLM-XComposer/main/./assets/modelscope_logo.png,,1480,2023-09-26T02:42:20Z
2024-04-11,https://github.com/Azure-Samples/azure-search-openai-demo,https://raw.githubusercontent.com/Azure-Samples/azure-search-openai-demo/main/README.md,"This text introduces an Azure-based solution for creating ChatGPT-like experiences using enterprise data. Announced on November 15, 2023, the solution involves Azure OpenAI and AI Search (formerly Azure Cognitive Search) services to implement the Retrieval Augmented Generation pattern, enabling chat and Q&A interfaces over proprietary data. The provided sample, focusing on a fictitious company, demonstrates data indexation, question-answering, and source citation features. It supports multiple programming languages and offers comprehensive guidance on deployment, cost estimation, customization, and troubleshooting within Azureâ€™s ecosystem. Additional resources include sample data, setup guides for various environments, and instructions for local application running. Key aspects like Azure account requirements, monitoring with Application Insights, and tips for transitioning to production are discussed. The document emphasizes adhering to Azureâ€™s service access criteria, managing deployment costs, and ensuring security and data accuracy in application development.",Revolutionizing Enterprise Data Management with ChatGPT on Azure OpenAI,"Learn how to leverage ChatGPT and Azure OpenAI to manage and query enterprise data effectively. In this post, we explore a sample application using Azure AI Search and OpenAI's GPT model to create ChatGPT-like experiences for internal data queries at Contoso Electronics. Discover the benefits of integrating Azure OpenAI with AI Search for powerful data insights, and see how you can customize the application for your business needs.",Discover how to enhance your enterprise data management with ChatGPT and Azure OpenAI. This guide covers integrating Azure AI Search and OpenAI GPT models for effective data handling and insights.,AI Development Platform.,"Python





        5,220





        3,209


        Built by

          









        10 stars today",https://raw.githubusercontent.com/Azure-Samples/azure-search-openai-demo/main/docs/images/appcomponents.png; https://raw.githubusercontent.com/Azure-Samples/azure-search-openai-demo/main/docs/images/chatscreen.png; https://raw.githubusercontent.com/Azure-Samples/azure-search-openai-demo/main/docs/images/endpoint.png; https://raw.githubusercontent.com/Azure-Samples/azure-search-openai-demo/main/docs/images/transaction-tracing.png,https://www.youtube.com/watch?v=3acB0OWmLvM; https://www.youtube.com/watch?v=j8i-OM5kwiY,5220,2023-02-08T21:00:54Z
2024-04-11,https://github.com/streamlit/streamlit,https://raw.githubusercontent.com/streamlit/streamlit/master/README.md,"Streamlit is a powerful tool that enables rapid conversion of data scripts into shareable web apps using Python. It's an open-source platform that's free to use. Users can deploy, manage, and share their apps via the Community Cloud platform. Streamlit supports easy installation, including within a virtual environment for Windows, Mac, and Linux users. The platform boasts a straightforward API, allowing for the development of interactive and rich web applications. For instance, it features a demo project showcasing a self-driving car dataset with real-time inference capabilities. Streamlit also offers a GitHub badge for users to promote their apps. The community around Streamlit provides a supportive environment for sharing, troubleshooting, and discussing apps. Streamlit is licensed under the Apache 2.0 license, making it fully open-source.",Streamlit: The Ultimate Guide to Building and Sharing Data Apps Quickly,"Discover the power of Streamlit, the fastest way to build and share data apps using Python. Streamlit transforms data scripts into web apps in minutes, offering a free, open-source platform ideal for quick prototyping and deployment. With easy installation and a strong community support, it simplifies app development to just a few lines of code, enabling everyone from beginners to experts to bring their data projects to life quickly. Experience it first-hand by deploying your apps on the Community Cloud platform.","Learn how Streamlit revolutionizes data app development by turning Python scripts into shareable web apps in minutes. Explore its easy installation, vibrant community, and rapid deployment capabilities.",Open Source Tool,"Python





        31,308





        2,763


        Built by

          









        31 stars today",https://raw.githubusercontent.com/streamlit/docs/main/public/images/Streamlit_overview.gif; https://raw.githubusercontent.com/streamlit/docs/main/public/images/simple_example.png; https://raw.githubusercontent.com/streamlit/docs/main/public/images/complex_app_example.gif,,31308,2019-08-24T00:14:52Z
2024-04-11,https://github.com/Rudrabha/Wav2Lip,https://raw.githubusercontent.com/Rudrabha/Wav2Lip/master/README.md,"Wav2Lip is a cutting-edge technology that enables accurate lip-syncing of videos in any setting, language, voice, and identity, including CGI faces and synthetic voices. Hosted for free at Sync Labs, it offers a turn-key hosted API for product integration and addresses commercial/enterprise requests. The technology stems from the paper ""A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild"" (ACM Multimedia 2020). It provides complete training code, inference code, and pre-trained models, along with reliable evaluation benchmarks. For those interested in research or academic uses, the interactive demo, Google Colab notebooks, and tutorial materials facilitate easy experimentation and learning. However, commercial use requires direct contact due to the use of the LRS2 dataset in model training. The project emphatically stands out for its open-source contribution and the substantial resources it offers for developers and researchers alike to explore and innovate in speech-to-lip generation technology.",Revolutionizing Video Editing with Wav2Lip: Mastering Lip Sync in Any Video,"Wav2Lip presents an innovative solution for flawlessly syncing lips in videos, a feature freely hosted at Sync Labs. Ideal for integrating into products through a hosted API, it supports any identity, voice, and language, including CGI and synthetic voices. The technology, stemming from the paper 'A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild,' demonstrates unprecedented accuracy. Available tools include complete training code, pretrained models, and an interactive demo for immediate testing. This breakthrough is transforming video editing by making accurate lip-syncing accessible to everyone.","Discover Wav2Lip: the cutting-edge tool for perfect lip-syncing in videos, supporting any voice or language. Explore the hosted API for product integration and access complete training codes and interactive demos for immediate use.",Video Generation Tool,"Python





        9,108





        1,983


        Built by

          









        19 stars today",,https://www.youtube.com/watch?v=0fXaDCZNOJc; https://www.youtube.com/watch?v=Ic0TBhfuOrA,9108,2020-08-07T08:06:38Z
2024-04-11,https://github.com/THUDM/ChatGLM2-6B,https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/README.md,"ChatGLM2-6B, an enhanced bilingual open-source dialogue model, has been introduced as the second generation following ChatGLM-6B. It showcases significant upgrades including improved performance through the use of GLM's mixed objective function, extended context length to 32K supported by FlashAttention technology, increased efficiency in inference powered by Multi-Query Attention, and an open protocol for academic and commercial use after registration. Additionally, ChatGLM2-6B-32K has been released to handle longer texts more effectively, and CodeGeeX2 has been introduced, leveraging code pre-training based on ChatGLM2-6B for enhanced coding capabilities. The model maintains a range of functionalities such as function calling, code interpretation, and agent tasks. Notably, ChatGLM2-6B has shown substantial performance gains on various datasets compared to its predecessor, marking a strong competitor in its category. Despite its advancements, users are cautioned on the potential inaccuracy and susceptibility to being misled due to the model's probabilistic nature and size. The model is recommended for various applications, including dialogue systems, subject to adherence to open-source protocols and considerations for data security and ethical use.",Exploring ChatGLM2-6B: A Leap in AI Conversation Models,"Discover the next-generation ChatGLM2-6B model, a game-changer in AI-driven conversations. Learn about its enhanced performance, longer context handling, and improved efficiency. Dive into its open-source journey and potential applications in academia and beyond. Uncover the improvements over its predecessor, including significant enhancements in various datasets and the introduction of advanced features like longer context lengths and efficient inference.","Explore the ChatGLM2-6B model's new features including improved performance, support for extended contexts, and efficiency in AI conversations. Understand how it advances the capabilities of open-source bilingual dialogue models.",Natural Language Processing,"Python





        15,424





        1,824


        Built by

          









        11 stars today",https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/resources/math.png; https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/resources/knowledge.png; https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/resources/long-context.png; https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/resources/web-demo.gif; https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/resources/web-demo2.gif; https://raw.githubusercontent.com/THUDM/ChatGLM2-6B/main/resources/cli-demo.png,,15424,2023-06-24T06:21:34Z
2024-04-11,https://github.com/PrefectHQ/prefect,https://raw.githubusercontent.com/PrefectHQ/prefect/main/README.md,"Prefect is an advanced orchestration and observability platform designed to simplify building, monitoring, and troubleshooting workflows. It allows users to convert Python code into an interactive application, providing an API to access pipelines, automate retries, schedule tasks, and much more, enhancing the resilience and dynamics of workflows. Prefect can be monitored through either a self-hosted server or the managed Prefect Cloud dashboard, offering enterprise-level security and management features for large-scale deployments. Installation is straightforward, requiring Python 3.8 or later, and involves using decorators to define workflows and tasks. It supports scheduling, monitoring via a user interface, and provides options for manual or event-triggered execution. Prefect Cloud further simplifies workflow management with automation, orchestration, and client-side interfacing through the `prefect-client`. The platform encourages community involvement, offering various resources and forums for users to learn, contribute, and discuss Prefect, guided by a Code of Conduct to maintain a friendly and respectful environment.",Supercharge Your Workflow Automation with Prefect,"Discover how Prefect transforms Python code into interactive workflow applications, offering a seamless way to build, monitor, and manage data workflows. With features like API exposure, standardized development, and automatic retries, Prefect revolutionizes workflow automation. It's designed for resilience, allowing workflows to adapt to unexpected changes while ensuring consistent deployment across organizations. Utilize Prefect Cloud for enterprise-level orchestration and manage your workflows with unmatched efficiency. Start with Prefect today to elevate your workflow management system to the next level.","Learn how Prefect, the leading orchestration and observability platform, can automate and enhance your data workflows with advanced features like API exposure, automatic retries, and distributed execution. Get started with Prefect now.",AI Task Automation,"Python





        14,484





        1,441


        Built by

          









        12 stars today",https://raw.githubusercontent.com/PrefectHQ/prefect/main/docs/img/ui/cloud-dashboard.png; https://raw.githubusercontent.com/PrefectHQ/prefect/main/docs/img/ui/automations.png,,14484,2018-06-29T21:59:26Z
2024-04-11,https://github.com/newren/git-filter-repo,https://raw.githubusercontent.com/newren/git-filter-repo/main/README.md,"Git filter-repo is an advanced tool for rewriting git history, surpassing the capabilities of git filter-branch in performance, functionality, and scalability for complex rewriting tasks. It is officially recommended over git filter-branch. Beyond simple command-line usage, it serves as a library for developing new history rewriting tools, catering to users with unique requirements. It necessitates git version 2.22.0 or higher and Python 3.5 or above. Installation is straightforward, involving a single Python script. The tool aids in tasks such as pruning history, renaming files, and transforming directories, with user-friendly commands. It addresses limitations of other tools like filter-branch and BFG Repo Cleaner by offering a design that emphasizes usability, safety, performance, and flexibility in rewriting repository history. Additionally, contributions to git filter-repo and adherence to the git project's Code of Conduct are encouraged, showcasing its community-driven development approach.",Maximizing Git Efficiency: Unleashing the Power of git filter-repo,"Discover the superior capabilities of git filter-repo for rewriting history with enhanced performance, usability, and functionality over git filter-branch. This tool not only dramatically improves efficiency but also introduces a level of precision in handling sophisticated rewriting tasks, recommended by the git project for its exceptional performance and scalability. Explore how to leverage git filter-repo's advanced features for customized history rewriting tools.","Learn how git filter-repo offers a breakthrough in repo history rewriting with high performance, extensive capabilities, and scalability, making it the preferred choice over git filter-branch.",Open Source Tool,"Python





        7,333





        661


        Built by

          









        7 stars today",,,7333,2018-08-21T15:40:09Z
2024-04-11,https://github.com/cyrus-and/gdb-dashboard,https://raw.githubusercontent.com/cyrus-and/gdb-dashboard/master/README.md,"The GDB Dashboard is an enhancement tool for the GNU Debugger, designed to simplify the debugging process by providing a modular interface to display important program information. It is implemented as a `.gdbinit` file leveraging the GDB Python API, aiming to minimize the need for multiple GDB commands and letting developers focus on the program's control flow. Installation involves placing the `.gdbinit` file in the user's home directory, with an optional step to install Pygments for syntax highlighting. The dashboard does not redefine GDB commands; it introduces a `dashboard` command for accessing its features. For usage and task performance, resources are available on its wiki page.",Maximize Your Debugging Efficiency with GDB Dashboard,"GDB Dashboard revolutionizes debugging with a modular interface, streamlining the process by reducing the need for excessive GDB commands. By simply placing a `.gdbinit` file in your home directory, you unlock a powerful dashboard that enhances your debugging workflow. Adding syntax highlighting is a breeze with an optional Pygments installation. This tool ensures developers can concentrate on the program's control flow, making debugging more intuitive and efficient. Dive into the wiki to master this indispensable development tool.",Discover how GDB Dashboard can transform your debugging approach by providing a modular interface for more efficient problem-solving. Learn to install and use this powerful tool.,Software Development,"Python





        10,509





        754


        Built by

          









        83 stars today",https://raw.githubusercontent.com/wiki/cyrus-and/gdb-dashboard/Screenshot.png,,10509,2015-09-09T16:53:23Z
2024-04-12,https://github.com/paul-gauthier/aider,https://raw.githubusercontent.com/paul-gauthier/aider/main/README.md,"Aider is an AI-assisted command-line tool designed for pair programming, leveraging GPT-3.5/GPT-4 to edit code within local git repositories. It edits local source files directly and commits changes with descriptive messages. Aider supports starting new projects or integrating into existing ones, offering unique capabilities for modifying large existing codebases. The tool is installed via pip, and it interacts with users through conversational commands to perform operations like adding or modifying code, committing changes, and more. Aider supports a wide range of programming languages and features in-chat commands to manage the coding session effectively. It enhances productivity by allowing collaborative coding with AI, providing detailed documentation and examples to guide users. Feedback from users praises Aider for significantly improving coding efficiency and ease of use, highlighting its respect as the best AI coding assistant tool available.",Revolutionize Your Coding with Aider: AI-Powered Pair Programming,"Discover how Aider, the innovative command line tool, transforms your coding experience by pairing you with GPT-3.5/GPT-4 for editing and committing code directly in your local git repository. Learn to start a new project or enhance existing ones by navigating large codebases with ease, thanks to Aider's intelligent assistance. Make programming a breeze with Aider's unique ability to understand and modify your code, all while ensuring smooth collaboration with GPT-4.","Elevate your programming with Aider, the command line tool that pairs you with GPT-3.5/GPT-4 for editing local git repository code. Start new projects or enhance existing ones with ease.",AI Coding Assistant,"Python





        8,553





        868


        Built by

          









        334 stars today",,https://www.youtube.com/watch?v=df8afeb1FY8; https://www.youtube.com/watch?v=MPYFPvxfGZs,8553,2023-05-09T18:57:49Z
2024-04-12,https://github.com/PKU-YuanGroup/MagicTime,https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/README.md,"MagicTime is an innovative project aimed at generating time-lapse videos based on given prompts, enhancing video generation models' ability to depict real-world transformations accurately. It stands out for its focus on metamorphic video generation, showcasing videos that include complex changes and physical transformations challenging to capture. Through the use of a specialized dataset, MagicTime significantly pushes the boundaries of video generation technology. The project has seen multiple updates, including the release of training codes, model weights, and a subset of the ChronoMagic dataset specifically designed for training MagicTime. This dataset features 2,265 metamorphic video-text pairs, providing rich resources for further development. Moreover, MagicTime has been integrated into the DiT-based architecture, indicating its versatility and potential for enhancing other text-to-video models. The project encourages community contributions, showcasing plugins and extensions developed by third parties, and is presented with a comprehensive suite for easy experimentation, including web demos and CLI inference tools. Recognized for its contributions to the field, MagicTime invites further collaboration and experimentation within the community.",Harnessing AI for Breakthrough Metamorphic Video Generation: An Insight into MagicTime,"Discover MagicTime, the innovative AI-driven project revolutionizing time-lapse video generation to accurately depict real-world transformations. By enhancing video generation models and leveraging a unique dataset, MagicTime opens new horizons in digital content creation. It outperforms traditional methods by generating metamorphic videos with unprecedented realism and detail. To explore this cutting-edge technology, visit their GitHub page for the latest updates and try the demo on Huggingface Spaces.","Dive into the world of MagicTime, a pioneering AI project transforming time-lapse video generation with metamorphic simulators. Experience the future of content creation with enhanced realism and precision.",Video Generation Tool,"Python





        522





        23


        Built by

          







        109 stars today",https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/magictime_logo.png; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/C_0_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/C_0_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/C_0_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/C_0_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/C_1_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/C_1_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/C_1_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/C_1_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_0_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_0_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_0_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_0_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_1_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_1_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_1_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_1_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_2_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_2_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_2_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_2_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_3_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_3_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_3_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_3_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_4_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_4_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_4_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_4_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_5_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_5_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_5_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_5_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_6_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_6_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_6_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_6_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_7_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_7_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_7_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_7_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_8_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_8_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_8_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/A_8_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_0_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_0_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_0_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_1_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_1_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_1_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_2_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_2_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/B_2_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/D_0_0.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/D_0_1.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/D_0_2.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/D_0_3.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/D_0_4.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/D_0_5.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/D_0_6.gif; https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/videos/D_0_7.gif,,523,2024-04-07T14:17:15Z
2024-04-12,https://github.com/barry-far/V2ray-Configs,https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/README.md,"This repository offers a variety of free V2ray configurations for secure and anonymous internet access. It includes links to different subscription configurations and protocols such as Vmess, Vless, Trojan, ShadowSocks, and more. Instructions for usage across various devices and operating systems like Windows, Linux, Android, iOS, and Mac are provided, along with recommendations for client applications. The configurations support automatic updates for improved performance and security. Users are guided on how to import these configurations into their preferred V2ray client. Contributions to the repository are welcome, and it operates under the MIT license. Special thanks are given to several contributors for their support.",Unlock Seamless Internet with Free V2ray Configurations,"Discover a treasure trove of free V2ray configuration files in our updated repository to enhance your internet browsing experience securely and anonymously. Whether you're a newbie or a seasoned V2ray client user, our collection offers a wide range of configs for various protocols including Vmess, Vless, and more. Stay updated with automatic subscription link updates every 10 minutes, ensuring optimal performance and security. Explore our detailed Wiki for setup instructions across different devices and operating systems. Contribute or reach out with any queries to continue improving this open-source project.","Access the latest free V2ray configuration files for secure and anonymous internet browsing. Find easy setup guides, automatic updates, and contribute to this open-source community project.",Cybersecurity Tool,"Python





        2,033





        318


        Built by

          







        104 stars today",,,2034,2023-09-17T09:39:01Z
2024-04-12,https://github.com/IDEA-Research/GroundingDINO,https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/README.md,"Grounding DINO introduces a novel approach to open-set object detection by combining DINO (a variant of DETR, a transformer-based object detector) with grounded pre-training techniques. Achieving state-of-the-art results on benchmarks like MSCOCO for zero-shot and fine-tuned object detection, Grounding DINO demonstrates its ability to detect ""everything"" using language, with remarkable performance boasting a 52.5 AP in a zero-shot setting and 63.0 AP when fine-tuned on COCO data. The project is a collaboration among numerous researchers, offering not only a PyTorch implementation and pretrained models but also tutorials, demos on Huggingface, Colab notebooks for hands-on experimentation, and integration examples with Stable Diffusion for image editing. Grounding DINO's architecture features a text and image backbone, feature enhancer, and a cross-modality decoder. It sets a new standard in object detection, providing tools and methodologies for further exploration in image segmentation and editing, thanks to its flexible infrastructure that combines visual and textual data.",Revolutionizing Object Detection: Explore the Power of Grounding DINO,"Discover the cutting-edge Grounding DINO, a PyTorch implementation for seamless and open-set object detection. With outstanding performance on MSCOCO and innovative approaches to marrying DINO with grounded pre-training, Grounding DINO sets a new standard in flexibility and efficiency for object detection tasks. Explore tutorials, demos, and the pioneering work of IDEA-Research in advancing object detection technology.","Learn about Grounding DINO, the groundbreaking PyTorch tool by IDEA-Research for advanced object detection, offering unmatched performance on MSCOCO. Explore our tutorials and demos for insights into future of object detection.",Computer Vision Platform,"Python





        4,836





        499


        Built by

          









        21 stars today",https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/./.asset/grounding_dino_logo.png; https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/hero_figure.png; https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/GD_GLIGEN.png; https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/COCO.png; https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/ODinW.png; https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/GD_SD.png; https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/GD_GLIGEN.png; https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/arch.png,https://www.youtube.com/watch?v=wxWDt5UiwY8; https://www.youtube.com/watch?v=cMa77r3YrDk; https://www.youtube.com/watch?v=C4NqaRBz_Kw; https://www.youtube.com/watch?v=oEQYStnF2l8; https://www.youtube.com/watch?v=wxWDt5UiwY8; https://www.youtube.com/watch?v=cMa77r3YrDk; https://www.youtube.com/watch?v=oEQYStnF2l8; https://www.youtube.com/watch?v=C4NqaRBz_Kw; https://www.youtube.com/watch?v=cMa77r3YrDk,4837,2023-03-09T06:14:41Z
2024-04-12,https://github.com/1Panel-dev/MaxKB,https://raw.githubusercontent.com/1Panel-dev/MaxKB/main/README.md,"MaxKB, a knowledge base question-and-answer system, leverages LLM (Large Language Models) to provide powerful cognitive capabilities for businesses. It's designed for ease of use, allowing for direct document uploads, online document crawling, automatic document splitting, and vectorization for an enhanced interactive experience. MaxKB supports seamless integration into third-party business systems without the need for coding and offers compatibility with leading large models including Llama 2, Azure OpenAI, and Baidu's Qianfan models. It can be quickly launched with Docker, and options are available for deployment through the 1Panel app store for a complete system setup including MaxKB + Ollama + Llama 2 within 30 minutes. Users can explore its functionalities through online demos or the DataEase assistant, which showcases its integration into products and documentation. MaxKB utilizes Vue.js and Django for its front and back end, respectively, and employs PostgreSQL with pgvector for its database needs, aligning with various large models for versatile performance capabilities.",MaxKB: Revolutionize Your Enterprise with LLM-Based Knowledge Base Systems,"Discover how MaxKB, the powerful LLM-based Knowledge Base system, is transforming enterprise knowledge management. Offering plug-and-play convenience, seamless integration, and support for leading large models, MaxKB is tailored for enhancing smart Q&A interactions within any business system. Whether it's leveraging mainstream models or deploying locally with Llama 2, MaxKB integrates effortlessly, promising a robust enterprise brain ready to scale with your business needs. Learn how to quickly set up with Docker and access a range of resources for a comprehensive knowledge management solution.","Explore MaxKB's features: an LLM-based Knowledge Base Q&A system that integrates seamlessly into business systems, supports major models, and offers a superior interactive experience.",AI Development Platform,"Python





        413





        35


        Built by

          









        50 stars today",,,414,2023-09-14T02:05:12Z
2024-04-12,https://github.com/google-deepmind/recurrentgemma,https://raw.githubusercontent.com/google-deepmind/recurrentgemma/main/README.md,"RecurrentGemma, developed by Google DeepMind, is a cutting-edge open-weights language model family built on the Griffin architecture, which enhances long-sequence generation through a mix of local attention and linear recurrences. This repository offers both a highly optimized Flax implementation and a reference PyTorch implementation for model sampling and fine-tuning. The technical report and Griffin paper provide in-depth details on the model's training and evaluation. Users can install the model using Poetry or pip, with support for both JAX and PyTorch pathways. Model checkpoints are accessible through Kaggle, and various tutorials are available for sampling and fine-tuning in Colab notebooks. RecurrentGemma is adaptable to CPU, GPU, or TPU, with the Flax version optimized for TPU. Contributions are welcome under the guidelines provided in CONTRIBUTING.md, and the code is licensed under Apache License 2.0. However, it is noted as not being an official Google product.",Exploring RecurrentGemma: Google DeepMind's Open-Weight Language Model,"Discover RecurrentGemma by Google DeepMind, an innovative open-weights language model harnessing the Griffin architecture for rapid sequence generation. This versatile implementation is available in both Flax for efficiency and PyTorch for those seeking a more hands-on approach. Dive into technical reports for an in-depth understanding, install with ease using Poetry or pip, and start experimenting with provided examples and Colab tutorials. Whether you're sampling or fine-tuning, RecurrentGemma is designed to cater to a wide range of machine learning projects and research.","Learn about RecurrentGemma, Google DeepMind's latest language model based on the advanced Griffin architecture. Discover its fast sequence generation capabilities, easy installation, and how to start experimenting with it.",Language Models,"Python





        394





        8


        Built by

          







        63 stars today",,,394,2024-03-25T11:13:36Z
2024-04-12,https://github.com/AIRI-Institute/OmniFusion,https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/README.md,"OmniFusion is a cutting-edge multimodal AI model created to surpass traditional language processing systems by incorporating different data types like images, and potentially audio, 3D, and video content. It uses a core called Mistral-7B and employs visual encoders, such as CLIP-ViT-L, for efficient information transfer. The model's unique adapter allows it to interpret and integrate information from various modalities into text. It has undergone two main training stages, focusing on Image Captioning tasks initially and then moving to improve its understanding of dialogues and complex queries. OmniFusion has been benchmarked against state-of-the-art models, showing superior performance in many areas. Recent updates include the release of OmniFusion-1.1 with added Russian language capability. The team behind OmniFusion, led by the FusionBrain scientific group from the AIRI Institute, plans to further enhance the model's capabilities by incorporating additional modalities and improving language understanding.",Exploring OmniFusion: The Next-Gen Multimodal AI Model,"OmniFusion is revolutionizing the AI landscape by integrating various data modalitiesâ€”images, audio, 3D, and videoâ€”into traditional language processing systems. Its architecture employs one or two visual encoders and a unique adapter to enhance multimodal capabilities. The model excels in benchmarks against state-of-the-art multimodal models, demonstrating superior performance in generative metrics and classification. With ongoing developments for additional modalities and language support, OmniFusion is poised to set new standards in AI applications.","Discover OmniFusion, the advanced AI model merging images, audio, and more with language processing for unparalleled multimodal capabilities. See how it's shaping the future of AI.",Multimodal AI Model,"Python





        137





        15


        Built by

          









        11 stars today",https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/./content/architecture2.png; https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/./content/radar_plot_gigachat.png; https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/./content/radar.png; https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/content/ex1.png; https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/content/ex2.png; https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/./content/examples.png,,137,2023-11-20T10:51:51Z
2024-04-12,https://github.com/huggingface/parler-tts,https://raw.githubusercontent.com/huggingface/parler-tts/main/README.md,"Parler-TTS is an innovative, open-source text-to-speech (TTS) model highlighted in the paper ""Natural language guidance of high-fidelity text-to-speech with synthetic annotations"" by Dan Lyth and Simon King. Unlike many other TTS models, Parler-TTS offers the ability to generate natural-sounding speech that can be customized according to the speaker's characteristics, such as gender, pitch, and speaking style. This model, fully released under a permissive license, promotes community engagement by making its datasets, pre-processing tools, training code, and weights publicly available. The project showcases the Parler-TTS Mini v0.1, a promising start with a model trained on 10.5K hours of audio, with plans to expand to 50K hours. Installation and usage are straightforward, encouraging users to participate in its development and improvement. Acknowledgments are given to the researchers, supporting libraries, and contributors. The project welcomes further contributions to enhance its capabilities, including dataset expansion, training enhancements, and optimization strategies for better quality and efficiency in speech generation.",Introducing Parler-TTS: Revolutionizing Text-to-Speech Technology,"Parler-TTS, a groundbreaking text-to-speech model, offers unmatched quality and customization in speech generation. Developed by Stability AI and Edinburgh University, it stands out as fully open-source, encouraging collaboration and innovation within the community. The model, which can mimic various speaking styles and voices, is accompanied by extensive datasets and training code. Excitingly, Parler-TTS Mini v0.1 is now available, promising further advancements with future updates. This initiative marks a significant leap forward in making synthetic speech more natural and accessible.","Discover Parler-TTS, the open-source text-to-speech model by Stability AI and Edinburgh University, delivering high-quality, customizable speech. Learn about its features, open-source community benefits, and the latest Parler-TTS Mini v0.1 release.",Open Source Tool,"Python





        836





        52


        Built by

          






        298 stars today",,,837,2024-02-13T10:05:58Z
2024-04-12,https://github.com/trekhleb/learn-python,https://raw.githubusercontent.com/trekhleb/learn-python/master/README.md,"This repository is a comprehensive resource for learning Python, featuring a host of Python scripts categorized by topics which include code examples, explanations, and additional reading links. It's designed as both a playground and a cheatsheet. As a playground, it encourages interaction by allowing code modification and testing through assertions, and code linting to ensure adherence to Python style guidelines. As a cheatsheet, it serves as a reference for Python syntax and constructs, providing immediate feedback through built-in assertions without needing to run the code. The repository is structured to facilitate learning by exploring topics, reading documentation, viewing code examples, and testing through provided mechanisms. It covers a broad range of Python fundamentals such as variables, data types, control flow, functions, classes, error handling, and more, with links to additional resources for further learning. Installation instructions for Python and dependencies are provided, along with guidance on testing and linting code.",Master Python: A Comprehensive Guide for Beginners,"Dive into Python with this comprehensive playground and cheatsheet, perfect for beginners looking to master Python through interactive learning. From basic syntax to advanced concepts, this repository offers code examples, explanations, and links to further readings, ensuring a well-rounded understanding of Python. Test your code with assertions, discover Python's standard statements and constructions, and maintain high code quality from the start. Whether you're starting your programming journey or brushing up on your Python skills, this guide is your go-to resource for becoming proficient in Python.","Learn Python with an interactive playground and cheatsheet. Explore examples, explanations, and further readings to master Python programming from basics to advanced topics.",Python Learning Journey,"Python





        15,822





        2,559


        Built by

          









        72 stars today",,,15822,2018-08-29T15:20:05Z
2024-04-12,https://github.com/NaishengZhang/book-recommendation-system,https://raw.githubusercontent.com/NaishengZhang/book-recommendation-system/master/README.md,"The final project for a class involves building and evaluating a book recommendation system using the Goodreads dataset. This dataset includes interactions between 876K users and 2.4M books, resulting in 223M user-book interactions. The recommendation model will employ Spark's alternating least squares (ALS) method, requiring tuning of hyper-parameters like the dimension of latent factors and the regularization parameter. Data will be split into training, validation, and test sets, with a recommendation to downsample for prototyping. The modelâ€™s accuracy will be evaluated on both validation and test data, and students are encouraged to document their hyper-parameter tuning process. It's also recommended to start with smaller dataset samples before scaling up to ensure efficient use of shared cluster resources and to convert CSV data to parquet format for better performance.",Maximizing Book Recommendations: Leveraging Goodreads Data with Spark ALS,"In the realm of recommender systems, the final project of building one with Spark's ALS using Goodreads data stands as a deep dive into applied problem-solving with massive datasets. The project embarks on using over 223M user-book interactions from 876K users and 2.4M books, laying a practical foundation for tuning ALS hyper-parameters like rank and lambda for optimal performance. Through meticulous data splitting and subsampling, the project paves the way for evaluating recommendation models' accuracy, using Spark's advanced evaluation metrics. This opens avenues for both beginner and advanced data scientists to experiment, learn, and contribute to the evolving field of personalized book recommendations.","Explore the practical guide to building a sophisticated book recommendation system using Spark's ALS method on the extensive Goodreads dataset, complete with tips on data handling, model tuning, and evaluation.",Machine Learning,"Python





        61





        17


        Built by

          







        8 stars today",,,61,2020-06-20T19:46:07Z
2024-04-13,https://github.com/dataelement/bisheng,https://raw.githubusercontent.com/dataelement/bisheng/main/README.md,"Bisheng, an innovative open-source platform for large model application development, was officially released under the Apache 2.0 License in August 2023. Designed to empower rapid development and implementation of large model applications, it offers an intuitive interface that allows even non-technical users to create intelligent applications effortlessly. The platform distinguishes itself with enterprise-grade features for real-world production use, including high availability under heavy loads, continuous application operation and improvement, and versatile functionalities suited to varied business scenarios. Moreover, Bisheng integrates capabilities for managing unstructured data, leveraging the team's core expertise developed over the years. It supports a wide array of applications, from report generation and knowledge base QA to dialog and element extraction, aiming to transcend beyond dialog-based interactions to include automated processes and search among other forms. Developers can start with Bisheng by installing or compiling it from the source, and contributions to its community through code or discussions are welcomed. DataElem Inc., the company behind Bisheng, is recruiting talents to join in advancing this cutting-edge platform. Bisheng acknowledges several dependencies including Triton, langchain, unstructured, and langflow for their contributions to its capabilities.",Unlocking Next-Gen App Development with Bisheng: The Ultimate Open-Source Platform,"Discover Bisheng, a pioneering open-source platform for large model application development, revolutionizing how users approach next-gen application development. Leveraging its unique strengths in convenience, flexibility, and enterprise-grade reliability, Bisheng empowers even non-technical business personnel to rapidly deploy intelligent applications. Officially open-sourced in August 2023 under the Apache 2.0 License, Bisheng stands as a testament to innovation, named after the inventor of movable type printing, aiming to similarly transform intelligent application deployment. Join us in exploring Bisheng's vast capabilities and contribute to shaping the future of smart application landscapes.","Explore how Bisheng, the open-source large model application development platform, accelerates and simplifies the creation of intelligent applications. Officially open-sourced in 2023, Bisheng invites collaboration to revolutionize next-gen app development.",AI Development Platform,"Python





        5,250





        873


        Built by

          









        25 stars today",https://www.dataelem.com/nstatic/bisheng.png; https://www.dataelem.com/nstatic/qrcode.png,,5250,2023-08-28T10:00:24Z
2024-04-13,https://github.com/Lightning-AI/litgpt,https://raw.githubusercontent.com/Lightning-AI/litgpt/main/README.md,"LitGPT, developed by Lightning AI, is a versatile command-line tool designed for the use, pretraining, finetuning, and deployment of over 20+ large language models (LLMs). It supports a wide array of state-of-the-art techniques and optimizations including flash attention, fully-sharded data parallelism (FSDP), and precision settings ranging from fp4 to fp32, facilitating efficient training on a scalable range from 1 to 1000+ GPUs/TPUs. Additionally, it offers parameter-efficient finetuning methods such as LoRA, QLoRA, and Adapter variants. LitGPT simplifies integrating popular and custom datasets for model training, and its configuration files streamline the optimization process for various training conditions. It also enables model export to other formats, making it highly adaptable for different use cases. The platform invites community involvement, providing extensive documentation, tutorials, and guides to assist users in leveraging LLMs effectively. Acknowledging contributions from several prominent projects and individuals, LitGPT extends the capabilities of nanoGPT and Lit-LLaMA, powered by Lightning Fabric.",Mastering LitGPT: Your Ultimate Guide to Training and Deploying Large Language Models,"Discover how to leverage LitGPT for state-of-the-art LLM training and deployment. Learn to pretrain, finetune, and deploy over 20 LLMs on your data using the latest techniques like LoRA, flash attention, and more. Whether you're working with 1 GPU or 1000+ GPUs/TPUs, LitGPT optimizes your AI development with low-precision settings and efficient finetuning methods. Get started now and transform the way you work with LLMs.","Unveil the power of LitGPT for training and deploying large language models (LLMs) with advanced techniques. Optimize AI development with custom data using GPUs/TPUs, low-precision settings, and more. Start now.",AI Development Platform,"Python





        6,028





        632


        Built by

          









        24 stars today",https://pl-public-data.s3.amazonaws.com/assets_lightning/LitStableLM_Badge.png,,6028,2023-05-04T17:46:11Z
2024-04-13,https://github.com/zulip/zulip,https://raw.githubusercontent.com/zulip/zulip/main/README.md,"Zulip is an open-source collaboration platform combining aspects of email and chat to facilitate both live and asynchronous communication. Embraced by Fortune 500 companies, prominent open-source initiatives, and numerous other organizations, Zulip stands out for its topic-based threading. The project is the product of a global developer community, with over 1000 contributors and a rapidly growing codebase. Zulip encourages contributions from developers and non-developers alike, offering extensive documentation and various paths for engagement. It supports self-hosting as well as cloud solutions, catering to a broad audience. Additionally, Zulip is actively involved in outreach programs and welcomes support from its user base through advocacy, sponsorship, and feedback. The platform is licensed under the Apache 2.0 license, reflecting its commitment to open-source principles.",Exploring Zulip: The Ultimate Team Collaboration Tool for Productive Remote Work,"Zulip is revolutionizing remote work by uniquely combining email's structure with chat's immediacy in an open-source collaboration tool. With its innovative topic-based threading, it supports both synchronous and asynchronous communication, making it a favorite among Fortune 500 companies and leading open-source projects. Its development is driven by a global community of over 1000 contributors, making Zulip the most rapidly evolving team chat project. Whether you're contributing code, feedback, or simply exploring, Zulip offers comprehensive resources to fully engage with its community. Join the growing ecosystem of Zulip users and see how it can transform your team's collaboration.","Discover Zulip, the open-source collaboration platform blending email and chat functionalities to support productive remote teamwork. Learn how its unique threading, extensive community, and robust features make it ideal for any organization.",Open Source Community,"Python





        19,749





        7,360


        Built by

          









        3 stars today",,,19749,2015-09-25T16:37:25Z
2024-04-13,https://github.com/outlines-dev/outlines,https://raw.githubusercontent.com/outlines-dev/outlines/main/README.md,"Outlines is a robust tool for structured text generation, developed with love by the team at .txt. It offers integration with multiple models like OpenAI and transformers, utilizes the Jinja templating engine for prompting, and supports efficient regex-structured and JSON generation adhering to JSON schemas or Pydantic models. Features include dynamic stopping, batch inference, and advanced sampling algorithms. Outlines aim to make language model outputs more predictable, improving inference speed and model performance without adding overhead. Additionally, it facilitates the creation of complex prompts via template functions. Outlines are continually updated, with the team encouraging community engagement through contributions, ideas on Discord, and issue reporting on GitHub.",Revolutionize Your Code with Outlines: Text Generation Simplified,"Discover Outlines, the must-have library for structured text generation in your coding projects. Enjoy seamless integration with models like OpenAI, quick installation, and a suite of features like regex-structured generation, efficient JSON output, and powerful prompting options. Dive into the future of code efficiency and performance with Outlines today.","Explore Outlines, the game-changing Python library for structured text generation. Get faster, more predictable outputs with OpenAI integration, complex prompt handling, and more. Perfect for developers seeking efficiency in automation.",AI Development Platform,"Python





        5,425





        264


        Built by

          









        19 stars today",https://raw.githubusercontent.com/outlines-dev/outlines/main/./docs/assets/images/logo.png; https://raw.githubusercontent.com/outlines-dev/outlines/main/./docs/assets/images/dottxt.png,,5425,2023-03-17T16:01:18Z
2024-04-13,https://github.com/huggingface/dataspeech,https://raw.githubusercontent.com/huggingface/dataspeech/main/README.md,"Data-Speech is a collection of utility scripts for tagging speech datasets, designed to enhance the development of speech-based AI models, including text-to-speech engines. It focuses on implementing the annotation method from the research by Dan Lyth and Simon King, providing labels for speaker characteristics with natural language descriptions. This tool has been used on datasets like LibriTTS-R and a subset of the English version of MLS, supporting the Parler-TTS library's models. The process involves predicting annotations based on speaking rate, signal-to-noise ratio, reverberation, and pitch estimation, then mapping these annotations to discrete keywords and generating natural language descriptions. The suite, which is still under development, aims to aid in the scalable and adjustable preparation of speech synthesis datasets, acknowledging the contributions of multiple open-source resources and recommending citation for users who find the repository beneficial.",Maximizing Speech Dataset Utility with Data-Speech: Revolutionizing TTS Model Development,"Data-Speech offers revolutionary utility scripts for tagging speech datasets, facilitating high-quality text-to-speech (TTS) model development. It leverages annotations based on cutting-edge research to enrich datasets like LibriTTS-R and MLS English with detailed speaker characteristics. This suite simplifies the application of audio transformations, supports the Parler-TTS library for advanced TTS model training, and enhances speech-based AI research and development. By preparing datasets with precise annotations, Data-Speech propels the next generation of speech synthesis technologies forward.","Discover how Data-Speech transforms speech dataset preparation with utility scripts for TTS model development, featuring audio transformation and detailed annotation based on latest research. Enhance your speech AI projects now.",Natural Language Processing,"Python





        129





        5


        Built by

          






        11 stars today",,,129,2024-02-07T17:34:44Z
2024-04-13,https://github.com/ModelTC/lightllm,https://raw.githubusercontent.com/ModelTC/lightllm/main/README.md,"LightLLM is a Python framework designed for efficient inference and serving of Large Language Models (LLMs), capitalizing on its lightness, scalability, and speed. It integrates features from various renowned open-source projects like FasterTransformer, TGI, vLLM, and FlashAttention. Key features include tri-process asynchronous collaboration enhancing GPU utilization, support for nopad attention across multiple models, dynamic batch scheduling, tensor parallelism for quick inference across multiple GPUs, and a high-performance router for optimal GPU memory management of token processes. Additionally, it supports a variety of models including BLOOM, LLaMA, and others. LightLLM facilitates easy installation and containerization for users through Docker, aiming to achieve state-of-the-art throughput performance for both text and multimodal models (e.g., QWen-VL, Llava). Performance comparisons demonstrate LightLLMâ€™s superior service performance over older models, ensuring efficient handling of broad and dynamically changing request loads.","Exploring LightLLM: The High-Speed, Lightweight LLM Inference Framework","LightLLM stands out in the tech world as a Python-based, high-performance Large Language Model (LLM) inference and serving framework. It offers unique features such as tri-process asynchronous collaboration, dynamic batch scheduling, and innovative memory management techniques for optimized GPU utilization. LightLLM supports a range of models from BLOOM to LLaMA and beyond, promising no memory waste and efficient processing of diverse requests. Its high-performance router and Tensor Parallelism enable seamless scalability and faster inference across multiple GPUs. Dive into LightLLM for groundbreaking efficiency in large language model inference.","Discover LightLLM, a lightweight, scalable LLM inference framework offering high-speed performance, dynamic batch scheduling, and support for multiple models like BLOOM and LLaMA. Perfect for efficient, large-scale text processing.",AI Development Platform,"Python





        1,755





        157


        Built by

          









        3 stars today",https://raw.githubusercontent.com/ModelTC/lightllm/main/assets/lightllm.drawio.png,,1755,2023-07-22T08:11:15Z
2024-04-13,https://github.com/goauthentik/authentik,https://raw.githubusercontent.com/goauthentik/authentik/main/README.md,"Authentik is an open-source Identity Provider designed for flexibility and versatility, ideal for integrating into existing systems and managing sign-up, recovery, and other functions efficiently. It supports Docker Compose for smaller setups and a Helm Chart for larger deployments, with documentation available for both. The project encourages community involvement, inviting users to share their adoption stories, contribute to development, and assist with translations. Development resources and security guidelines are clearly outlined, with an open call for contributions. Featuring a dual-themed UI, authentik combines robust functionality with ease of installation and a community-driven approach to identity management.",Exploring authentik: The Open-Source Identity Provider for Modern Applications,"Discover authentik, the open-source Identity Provider designed for flexibility and versatility in modern applications. Whether you're integrating into existing systems or setting up new protocols, authentik offers a streamlined solution for sign-up processes, recovery features, and more. Ideal for both small and large setups, it supports Docker Compose and Helm Chart installations. Dive into its development, see it in action through screenshots, and learn how your organization can contribute to its growth.","Learn about authentik, an open-source Identity Provider that enhances flexibility and versatility for applications. Ideal for seamless integration and supporting new protocols, it simplifies user management processes. Discover more now.",Open Source Tool,"Python





        6,615





        504


        Built by

          









        12 stars today",https://docs.goauthentik.io/img/screen_apps_light.jpg; https://docs.goauthentik.io/img/screen_apps_dark.jpg; https://docs.goauthentik.io/img/screen_admin_light.jpg; https://docs.goauthentik.io/img/screen_admin_dark.jpg,,6615,2019-12-30T09:19:48Z
2024-04-13,https://github.com/VRSEN/agency-swarm-lab,https://raw.githubusercontent.com/VRSEN/agency-swarm-lab/main/README.md,"The Agency Swarm Lab is a collaborative repository that demonstrates the capabilities of AI agent teams created with the Agency Swarm framework. To get started, users need to clone the repository and install necessary requirements. Agencies, each with specific requirements, are housed in separate folders. Utilizing OpenAI's API, users must also set up a `.env` file with their API key for operations. Additionally, Docker installation is recommended for safer operation without affecting the local file system. The repository welcomes contributions from those who have developed custom AI agencies using the framework. The initiative aims to transform the future of work with AI, encouraging interested individuals to stay updated through their YouTube channel.",Unlocking AI Collaboration: A Guide to Agency Swarm Lab,"Welcome to Agency Swarm Lab, your comprehensive guide to creating custom AI agent teams with the cutting-edge Agency Swarm framework. Dive into the agency-centric world where collaboration meets innovation, starting with cloning the repository, setting up your environment, and running your agency with ease. Whether you're installing locally or prefer Docker for a safer environment, this guide ensures a seamless setup process. Explore the potential of AI collaboration in transforming the future of work. Don't miss out on the latest tutorials and updates by subscribing to our YouTube channel.","Discover how to create custom AI agent teams with Agency Swarm Lab. This step-by-step guide covers everything from local installation to Docker setup, helping you unlock the full potential of AI collaboration.",Collaborative AI Framework,"Python





        168





        88


        Built by

          






        31 stars today",,,168,2024-02-03T05:18:52Z
2024-04-13,https://github.com/gto76/python-cheatsheet,https://raw.githubusercontent.com/gto76/python-cheatsheet/main/README.md,"This text is a comprehensive cheatsheet for Python, available for download or purchase as a PDF. It includes various Python concepts and features such as Collections (List, Dictionary, Set, Tuple, Range, Enumerate, Iterator, Generator), Types (Type, String, Regular Expressions, Format, Numbers, Combinatorics, Datetime), Syntax (Arguments, Inline, Import, Decorator, Class, Duck Types, Enum, Exception), System commands (Exit, Print, Input, Command Line Arguments, Open, Path, OS Commands), Data structures (JSON, Pickle, CSV, SQLite, Bytes, Struct, Array, Memory View, Deque), Advanced topics (Threading, Operator, Match Statement, Logging, Introspection, Coroutines), Libraries (Progress Bar, Plots, Tables, Curses, GUIs, Scraping, Web, Profiling), and Multimedia (NumPy, Image, Animation, Audio, Synthesizer, Pygame, Pandas, Plotly). The cheatsheet also includes detailed examples and syntax for various operations, data types, and concepts such as sorting lists, manipulating dictionaries, and working with sets and tuples. It serves as a quick reference for developers looking to brush up on Python syntax and its core libraries.","Ultimate Python Cheat Sheet: Tips, Tricks, and Best Practices","Discover the ultimate Python cheat sheet that compiles all the essential elements from basics to advanced concepts of Python programming in one place. Whether you're just starting out or looking to brush up on your skills, this comprehensive guide provides quick access to syntax, libraries, and functions, simplifying your development process. From collections and types to threading and multimedia libraries, enhance your coding proficiency with this essential Python cheat sheet.","Looking to elevate your Python skills? Explore our ultimate Python cheat sheet, covering everything from basic syntax to advanced programming concepts. Find quick answers and best practices to streamline your coding workflow.",Python Libraries Collection,"Python





        35,327





        6,357


        Built by

          









        57 stars today",https://raw.githubusercontent.com/gto76/python-cheatsheet/main/web/image_888.jpeg; https://raw.githubusercontent.com/gto76/python-cheatsheet/main/web/covid_deaths.png; https://raw.githubusercontent.com/gto76/python-cheatsheet/main/web/covid_cases.png,,35327,2018-01-25T04:16:53Z
2024-04-13,https://github.com/pytorch/vision,https://raw.githubusercontent.com/pytorch/vision/main/README.md,"The torchvision package is a comprehensive tool for computer vision in PyTorch, offering datasets, model architectures, and image transformations. Installation instructions and compatibility with Python versions are detailed online, alongside guidelines for installing from source. Vision tasks leverage torch tensors and PIL images, with support for Pillow and Pillow-SIMD for faster processing. An unstable video backend is also available, requiring specific setups, especially on Linux. Additionally, torchvision enables C++ integration of PyTorch models with JIT Script, detailing steps for installation and linking. The project emphasizes the need for awareness around dataset permissions and model license responsibilities, highlighting specific terms for pre-trained models, such as those under the CC-BY-NC 4.0 license. Contributions to torchvision are welcome, with clear guidelines provided for interested developers.",Exploring TorchVision: The Essential Toolkit for Computer Vision in PyTorch,"Discover the power of TorchVision, PyTorch's library for computer vision applications, offering a vast array of datasets, model architectures, and image transformations. Learn how to seamlessly install TorchVision alongside PyTorch to empower your computer vision projects with the latest models and datasets. From image and video processing to leveraging pre-trained models for accelerated development, TorchVision is a vital component for researchers and developers in the AI field. Check the documentation for comprehensive guides and how-to's. Embrace the full potential of computer vision with TorchVision's robust toolset today.","Unveil the capabilities of TorchVision, PyTorch's comprehensive library for computer vision, featuring extensive datasets, models, and transformations. Perfect for AI researchers and developers.",Computer Vision,"Python





        15,365





        6,837


        Built by

          









        5 stars today",,,15365,2016-11-09T23:11:43Z
2024-04-13,https://github.com/VRSEN/agency-swarm,https://raw.githubusercontent.com/VRSEN/agency-swarm/main/README.md,"Agency Swarm, initiated by Arsenii Shatokhin (VRSEN), aims to fully automate AI agencies through a unique framework that simplifies the creation process of collaborative agent swarms, or ""Agencies,"" each possessing distinct roles and capabilities. This approach, which anthropomorphizes automation by reflecting real-world agency structures and specialized roles, intends to make the process more intuitive for both agents and users. Key features of Agency Swarm include customizable agent roles using the Assistants API, full control over prompts for greater flexibility, tool creation with Instructor for easy interface and automatic type validation, efficient agent communication via a bespoke tool, and effective state management of assistants. Agency Swarm is designed for reliability in production environments. Users can start by setting their OpenAI key, defining custom tools with Instructor, creating agent roles, and defining agency communication flows to ensure streamlined interactions among different agents. Additional functionalities include a CLI for agency genesis, importing existing agents, and creating agent templates locally, further optimizing the process for scalability and customization. Future updates will focus on enabling agencies to autonomously create other agencies and enhancing inter-agency communication. The project is open-source under the MIT license, inviting contributions from the public.",Revolutionizing Business Automation with Agency Swarm: A Comprehensive Guide,"Discover how Agency Swarm, initiated by Arsenii Shatokhin, aims to transform the AI Agency landscape through automation. This innovative framework allows for the creation of a collaborative swarm of agents, or 'Agencies,' each equipped with unique roles and capabilities, making agent creation intuitive and efficient. Key features include customizable agent roles, full control over prompts, tool creation, efficient communication, and state management. Agency Swarm stands out for its deployability in production, ensuring reliability and straightforward application in real-world settings.","Explore Agency Swarm's cutting-edge framework for automating AI agencies. Learn about its customizable agent roles, tool creation, and efficient communication features designed to streamline agent creation and deployment.",Collaborative AI Framework,"Python





        1,167





        324


        Built by

          









        15 stars today",,https://www.youtube.com/watch?v=M5Pa0pLgyYU,1167,2023-11-16T02:29:26Z
2024-04-14,https://github.com/openai/simple-evals,https://raw.githubusercontent.com/openai/simple-evals/main/README.md,"This repository introduces a lightweight library designed for assessing language models, starting with the evaluation of `gpt-4-turbo-2024-04-09`. It aims for transparency in how accuracy numbers are derived for models by emphasizing a zero-shot, chain-of-thought approach in its evaluations, diverging from traditional few-shot or role-playing prompt methods. The library doesn't plan for active maintenance or the acceptance of new evaluations outside of bug fixes, model adapters, and benchmark updates. It contains evaluations from various sources like MMLU, MATH, GPQA, DROP, MGSM, and HumanEval, and it interfaces with model APIs like OpenAI and Claude. Instructions for setup across different evaluations and samplers are provided, as well as a demonstration command for executing evaluations using the OpenAI API. The repository shares a commitment to open-source principles but specifies its focus and limits on contributions while also clarifying the legal standing of contributions as falling under the MIT license, with contributors agreeing to OpenAI's usage policies.",Introducing the Latest Language Model Evaluation Library for GPT-4 Turbo,"Explore our new language model evaluation library, designed for transparent and accurate performance metrics of the latest models like gpt-4-turbo-2024-04-09. Aimed at a realistic assessment, it emphasizes zero-shot, chain-of-thought evaluations over traditional few-shot approaches. This open-source initiative is set to refine how we understand and improve language model capabilities, despite not being actively maintained for new evals beyond specific updates. Dive into how this tool shapes the future of model evaluations, harnessing simplicity and precision.","Discover our open-source library for evaluating language models, offering a fresh perspective on accuracy with a focus on zero-shot, chain-of-thought techniques. See how it sets new standards for model performance assessment.",Language Models Tool,"Python





        567





        56


        Built by

          






        47 stars today",,,567,2024-04-11T22:38:17Z
2024-04-14,https://github.com/fudan-generative-vision/champ,https://raw.githubusercontent.com/fudan-generative-vision/champ/master/README.md,"Champ, developed by researchers from Nanjing University, Fudan University, and Alibaba Group, introduces a novel method for controllable and consistent human image animation with 3D parametric guidance. The project enables users to animate human images through 3D guidance, enhancing the realism and consistency of animated sequences. The framework supports various functionalities, including dance video animation, and provides tools and scripts for users to animate their own content. Pretrained models and checkpoints are available for download, ensuring users can leverage the system effectively. Installation requires specific system environments and the use of pip or poetry for package management. The project also acknowledges contributions from related works and invites participation from the research community, offering positions at the Generative Vision Lab, Fudan University. For detailed usage and future development plans, the project's roadmap is recommended. The project encourages academic citation and provides contact information for research opportunities.",Revolutionizing Human Image Animation: Discover Champ's Breakthrough,"Explore the cutting-edge Champ framework, a pioneering effort in controllable and consistent human image animation with 3D parametric guidance. Developed by a collaborative team from Nanjing University, Fudan University, and Alibaba Group, this powerful tool enables users to animate human images with unprecedented precision and realism. With the release of SMPL & rendering scripts, and comprehensive tutorials, Champ is set to transform digital animation. Dive into the future of image animation by leveraging the advanced capabilities of Champ, and join the community in pushing the boundaries of what's possible.","Discover Champ, an advanced framework for human image animation with 3D parametric guidance, developed by experts from top universities and Alibaba Group. Learn how to bring static images to life with precision and realism.",Animation Engine,"Python





        2,513





        275


        Built by

          







        45 stars today",https://raw.githubusercontent.com/fudan-generative-vision/champ/master/assets/framework.jpg,https://www.youtube.com/watch?v=2XVsy9tQRAY,2513,2024-03-17T04:49:43Z
2024-04-14,https://github.com/OpenBMB/MiniCPM-V,https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README.md,"MiniCPM-V and OmniLMM are open-source, multimodal large models designed for image and text understanding, providing high-quality text output from image and text inputs. Two versions have been released aiming at leading performance and efficient deployment. MiniCPM-V 2.8B is an advanced multimodal model deployable on end devices, capable of processing high-resolution images for scene text recognition and exhibiting low hallucination rates akin to GPT-4V. OmniLMM-12B, compared to models of similar size, demonstrates superior performance across multiple benchmarks, achieving lower hallucination rates than GPT-4V. These models have been evaluated extensively, showcasing their superior OCR capabilities and reliable behavior in multimodal content generation. They support bilingual (Chinese and English) capabilities and are optimized for efficient deployment even on mobile devices. Updates and enhancements are regularly released to improve performance and expand functionalities.",Exploring OmniLMM and MiniCPM-V: Pioneering Multimodal Large Models for Text and Image Understanding,"OmniLMM and MiniCPM-V are groundbreaking open-source multimodal large models designed for understanding both text and imagery, offering unparalleled performance and efficient deployment across two versions. MiniCPM-V 2.8B showcases remarkable scene text recognition and low illusion rates akin to Gemini Pro and GPT-4V, catering to edge devices. OmniLMM-12B outperforms similar-sized models in various benchmarks, presenting an even lower illusion rate compared to GPT-4V, setting new standards for reliability and multimodal integration.","Discover the capabilities of OmniLMM and MiniCPM-V, the leading open-source multimodal large models blending text and image understanding for cutting-edge performance and deployment. Learn about their top features, including exceptional scene text recognition and low illusion rates.",Multimodal AI Model,"Python





        698





        42


        Built by

          









        142 stars today",https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/./assets/minicpmv-omnilmm.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv2-cases_2.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/station.gif; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/london_car.gif; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/omnilmm-12b-examples_2.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/./assets/modelscope_logo.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/hk_OCR.jpg; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png; https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png,https://github.com/OpenBMB/OmniLMM/assets/157115220/8fec13bf-bb47-4bf8-8f8c-d0b716a964ec,698,2024-01-29T05:30:33Z
2024-04-14,https://github.com/chengaopan/AutoMergePublicNodes,https://raw.githubusercontent.com/chengaopan/AutoMergePublicNodes/main/README.md,"AutoMergePublicNodes is a project initiated by chengaopan, aimed at automatically aggregating and merging public internet nodes for free use. This initiative provides various subscription links for Base64 and Clash protocols to access free nodes, facilitating circumvention tools like v2ray, ssr, and clash. It serves as a workaround for accessing services like Google Play in regions where they are blocked, specifically addressing issues caused by domain redirections to empty servers within certain regions due to Google's server reassignments. The project offers multiple solutions, including updating project subscriptions to use foreign servers for Google services, using rooted devices to unlock GMS (Google Mobile Services) in restricted areas, or attempting connection via Clash for Android. It also includes a disclaimer emphasizing that the nodes are for educational and communication purposes only, discouraging illegal activities. Additionally, the project holds a stance against overwork, adopting an ""Anti 996"" license in protest against the 996 work schedule prevalent in some Chinese companies. The project also shares snippets and configuration pieces for integration into personal setups, housed under the 'snippets' folder on GitHub.",Unlock Global Services in China with AutoMergePublicNodes: A Guide to Free Internet,"Discover how AutoMergePublicNodes is revolutionizing access to global internet services like Google Play in China through free public node merging. Overcome restrictions with updated Clash subscriptions or ROOT methods to unlock GMS and navigate freely. Stay informed and compliant with the Anti 996 License, ensuring ethical use. Embrace the freedom of information, courtesy of the AutoMergePublicNodes project.",Learn how to bypass internet restrictions in China using AutoMergePublicNodes. Get the latest Clash subscriptions and unlock Google Play services with our comprehensive guide.,Open Source Tool,"Python





        261





        66


        Built by

          





        24 stars today",,,261,2024-01-11T03:59:55Z
2024-04-15,https://github.com/TencentARC/InstantMesh,https://raw.githubusercontent.com/TencentARC/InstantMesh/main/README.md,"InstantMesh is a framework designed for generating 3D meshes from single images efficiently, employing a large reconstruction model (LRM) architecture. As the official implementation, InstantMesh has released both its inference and training code, model weights, and a Gradio demo on HuggingFace for public experimentation. It recommends environments with Python 3.10, PyTorch 2.1.0, and CUDA 12.1 for optimal performance. Users can generate 3D meshes using command-line inputs or through a local Gradio demo, with provisions for customizing output formats and handling images with or without alpha masks. Training codes are provided, although the dataset itself is not due to size constraints. The project invites further citation for its contributions to 3D generative AI and acknowledges several prior works for their influence. Future updates are expected to include support for low-memory GPUs and additional multi-view diffusion models.",Revolutionizing 3D Mesh Generation with InstantMesh: A Breakthrough Tool,"Discover the groundbreaking InstantMesh tool, an official implementation for transforming single images into efficient 3D meshes using advanced LRM architecture. With its recent release, users can now access inference and training codes, model weights, and an interactive Gradio demo. This innovation not only promises to enhance 3D modeling efficiency but also paves the way for future advancements in sparse-view large reconstruction models. A must-try for enthusiasts and professionals in the field of 3D generative AI and modeling.","Explore InstantMesh, the latest feed-forward framework designed for efficient 3D mesh generation from a single image. Experience the future of 3D modeling with its cutting-edge LRM architecture, available model weights, and interactive demos.",3D Image Rendering,"Python





        364





        11


        Built by

          






        51 stars today",,,364,2024-04-10T11:22:38Z
2024-04-15,https://github.com/jina-ai/jina,https://raw.githubusercontent.com/jina-ai/jina/master/README.md,"Jina is an open-source platform designed for building multimodal AI services leveraging cloud-native technologies, enabling developers to create, scale, and deploy AI models efficiently. It supports gRPC, HTTP, and WebSockets for communication and offers seamless integration with Docker, Kubernetes, and Jina AI Cloud for production deployment. With Jina, developers can serve machine learning models for any data type using any deep learning framework, design high-performance services with features like easy scaling, streaming for Large Language Models (LLMs), and deploy them in a cloud-native environment. It differentiates from FastAPI by offering advanced orchestration, scaling capabilities, and better support for data-intensive applications through protocols like gRPC. Jina simplifies the transition from local development to production with cloud-native deployment options and tools like Executor Hub for containerization and deployment facilitation. The platform also emphasizes ease of scaling and concurrency, offering utilities for improved throughput in applications. Further, Jina aids in deploying AI applications to the cloud, highlighting its support for Kubernetes and Docker Compose. Additionally, Jina outlines an approach for efficient streaming services with LLMs, demonstrating how to serve and stream requests to maximize responsiveness and reduce latency for end-users.",How to Build Multimodal AI Applications with Jina's Cloud-Native Technologies,"Discover how Jina enables you to effortlessly build and deploy multimodal AI services and pipelines with cloud-native technologies, focusing on your logic and algorithms without worrying about infrastructure. Jina offers a seamless transition from model serving to production through advanced orchestration like Kubernetes and Docker Compose, making cloud-native technologies accessible to every developer.","Learn to build and deploy multimodal AI services using Jina's cloud-native technologies, focusing on development without the infrastructure hassle. Explore seamless transition to production with Kubernetes and Docker Compose.",AI Development Platform,"Python





        19,925





        2,180


        Built by

          









        26 stars today",https://raw.githubusercontent.com/jina-ai/jina/master/./.github/images/build-deploy.png; https://raw.githubusercontent.com/jina-ai/jina/master/./.github/images/deployment-diagram.png; https://raw.githubusercontent.com/jina-ai/jina/master/./.github/images/flow-diagram.png; https://raw.githubusercontent.com/jina-ai/jina/master/./.github/images/mona-lisa.png; https://raw.githubusercontent.com/jina-ai/jina/master/./.github/images/scaled-deployment.png,,19925,2020-02-13T17:04:44Z
2024-04-15,https://github.com/walkxcode/dashboard-icons,https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/README.md,"Dashboard Icons offers a comprehensive collection of icons specifically designed for dashboards, touted as the best source for such graphics. Users can easily integrate these icons into various dashboards like Homepage, Homarr, and Dashy. Icons can be downloaded from the icons page via simple right-clicking or using terminal commands with curl or wget for more technical users. It supports both `png` and `svg` formats. However, users are warned about potential excessive data usage and system slowdowns when loading all icons simultaneously due to the volume. Contributions to the project are welcomed following the provided guidelines, and users must adhere to the terms outlined in the license before using any icons or software.",Top Source for Dashboard Icons: Enhance Your Projects Today,"Discover the ultimate collection of dashboard icons perfect for integrating into various dashboards like Homepage, Homarr, and Dashy. With seamless integration and easy download options available directly from the icons page, enhancing your project's aesthetics has never been easier. Learn how to use these icons on any device, avoid system slowdowns by navigating the icons wisely, and contribute to the ever-growing icon collection. Plus, understand the legal use of these assets to ensure your project complies with the licensing terms.",Explore the best source for dashboard icons to elevate your projects. Find out how to easily integrate and download these icons for your dashboard with our step-by-step guide.,Developer Monitoring Platform,"Python





        4,006





        411


        Built by

          









        6 stars today",,,4006,2022-08-16T15:41:54Z
2024-04-15,https://github.com/wligithub/tax-tool,https://raw.githubusercontent.com/wligithub/tax-tool/main/README.md,"The Automated Personal Tax Preparing Tool is a Python-based application designed to calculate the cost basis for the VMW-AVGO merger, specifically for US holders of VMware shares. It processes the Gain&Loss file from E*TRADE, automatically generating tax information for cash received, converted AVGO shares, and AVGO cash-in-lieu fractional shares across all lots, including those acquired via ESPP, RSU, NSO, and brokerage purchases, both sold before and on the merger date. It offers commands for adjusting ESPP offer dates for mid-year joiners at VMware and provides a detailed guide for inputting data, running the tool, and understanding output files. Additionally, it guides on adjusting the cost base for Turbo Tax filing and addresses frequently asked questions related to lot cost base and share acquisition dates. The tool emphasizes its utility for informational purposes, disclaiming professional tax advice liability.",Maximize Your Tax Returns: Easy VMW-AVGO Merger Tax Preparation with Python,"In the wake of the VMW-AVGO merger, calculating cost basis and managing tax information has become simpler with our Python-based automated tool. This innovative tool processes the Gain&Loss file from E*TRADE, automates tax calculations including cost basis for cash and AVGO shares, and supports various types of VMW shares. It's designed for US holders of VMware shares, streamlining tax preparation by automating the calculation of cost bases and generating comprehensive tax summaries. Say goodbye to manual data entry and hello to efficiency and accuracy in your tax filings.",Discover how to simplify your tax preparation for the VMW-AVGO merger with our automated Python tool. Automate cost basis calculations and streamline tax filing for US VMware shareholders.,Document Conversion Tool,"Python





        109





        22


        Built by

          







        4 stars today",,,109,2024-03-04T05:23:24Z
2024-04-15,https://github.com/metavoiceio/metavoice-src,https://raw.githubusercontent.com/metavoiceio/metavoice-src/main/README.md,"MetaVoice-1B is a text-to-speech (TTS) model with 1.2 billion parameters, trained on 100K hours of speech, emphasizing emotional rhythm and tone in English. It enables zero-shot cloning for American and British voices with just 30 seconds of reference audio and supports voice cloning and cross-lingual cloning with finetuning, demonstrating success with minimal training data (as low as 1 minute for Indian speakers). It can synthesize text of any length and is available under the Apache 2.0 license for unrestricted use. Instructions for quick setup, installation, and usage are provided, including Docker commands, environmental setup, project dependency installation using poetry or pip/conda, and examples of usage scenarios like local use, cloud deployment, and finetuning details. The architecture involves predicting EnCodec tokens from text, diffusing these to waveform level with post-processing to clean up audio. It also outlines upcoming features, optimizations like KV-caching and batching, contribution opportunities, and acknowledgments to contributors and supporting organizations.",Exploring MetaVoice-1B: The Future of Text-to-Speech Technology,"MetaVoice-1B, a cutting-edge 1.2B parameter model trained on 100K hours of audio, brings a new level of expressiveness to text-to-speech technology with its focus on emotional speech rhythm and tone. Offering zero-shot cloning for American and British voices and support for cross-lingual voice cloning, it's a breakthrough in synthesis of arbitrary length text. Its open-source availability under the Apache 2.0 license allows unrestricted use, promising advancements in personalized audio content creation. Whether for developers or creators, MetaVoice-1B stands at the forefront of audio synthesis technology, heralding a new era of expressive and accessible digital voices.","Discover MetaVoice-1B, an open-source text-to-speech engine with emotional speech, zero-shot cloning, and cross-lingual voice cloning capabilities. Learn how it's changing the future of digital communication.",Voice Conversion Tool,"Python





        2,958





        381


        Built by

          









        7 stars today",,,2958,2024-02-06T17:04:29Z
2024-04-15,https://github.com/MatrixTM/MHDDoS,https://raw.githubusercontent.com/MatrixTM/MHDDoS/main/README.md,"MHDDoS is a DDoS (Distributed Denial of Service) attack script equipped with 56 different methods, developed in Python 3. It features a variety of Layer 7 methods like GET Flood, POST Flood, Bypass OVH, and many others, including techniques to bypass Cloudflare, Google's Project Shield, and DDoS Guard. For Layer 4 attacks, MHDDoS supports TCP Flood Bypass, UDP Flood Bypass, SYN Flood, among others. Additionally, the script comes with tools for practical tasks such as finding the real IP addresses of websites protected by Cloudflare, showing DNS records, checking website statuses, and more. For ease of use, MHDDoS includes commands for starting and stopping attacks, accessing console tools, and providing user guidance. The project stresses not to attack websites without the owner's consent and offers downloads and documentation via GitHub, where users are encouraged to support the project. MHDDoS is also compatible with several requirements like dnspython, cfscrape, and impacket, which are necessary for running the script effectively.",Unveiling MHDDoS: A Python 3 Script for DDoS Defense with 56 Unique Methods,"Explore the MHDDoS script, a powerful Python 3 tool designed for cybersecurity experts to simulate DDoS attacks with 56 diverse methods, ensuring robust website defense mechanisms. From Layer7 to Layer4 and special tools, it encompasses GET and POST Floods, Bypass techniques for OVH, CloudFlare, and more, plus amplification attacks for comprehensive stress testing. Remember, ethical use in consented environments is a must for responsible cybersecurity practice.","Discover MHDDoS, a Python-based DDoS attack simulation script with 56 methods, including CloudFlare bypass and amplification attacks, for ethical cybersecurity testing. ",Cybersecurity Tool,"Python





        11,139





        2,336


        Built by

          









        10 stars today",https://i.ibb.co/3F6V9JQ/MHDDoS.png; https://i.imgur.com/aNrHJcA.png; https://i.imgur.com/4Q7v2wn.png; https://img.icons8.com/cotton/344/domain.png; https://cdn0.iconfinder.com/data/icons/database-storage-5/60/server__database__fire__burn__safety-512.png; https://upload.wikimedia.org/wikipedia/en/thumb/f/f9/OVH_Logo.svg/1200px-OVH_Logo.svg.png; https://cdn-icons-png.flaticon.com/512/1691/1691948.png; https://cdn-icons-png.flaticon.com/512/4337/4337972.png; https://cdn.iconscout.com/icon/premium/png-256-thumb/cyber-bullying-2557797-2152371.png; https://pbs.twimg.com/profile_images/1351562987224641544/IKb4q_yd_400x400.jpg; https://cdn-icons-png.flaticon.com/512/6991/6991643.png; https://cdn2.iconfinder.com/data/icons/poison-and-venom-fill/160/loris2-512.png; https://lyrahosting.com/wp-content/uploads/2020/06/ddos-how-work-icon.png; https://img.icons8.com/plasticine/2x/null-symbol.png; https://i.pinimg.com/originals/03/2e/7d/032e7d0755cd511c753bcb6035d44f68.png; https://cdn0.iconfinder.com/data/icons/dicticons-files-folders/32/office_pps-512.png; https://cdn3.iconfinder.com/data/icons/internet-security-14/48/DDoS_website_webpage_bomb_virus_protection-512.png; https://seeklogo.com/images/D/ddos-guard-logo-CFEFCA409C-seeklogo.com.png; https://i.imgur.com/bGL8qfw.png; https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Google_%22G%22_Logo.svg/1024px-Google_%22G%22_Logo.svg.png; https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/Apache_HTTP_Server_Logo_%282016%29.svg/1000px-Apache_HTTP_Server_Logo_%282016%29.svg.png; https://icon-library.com/images/icon-for-wordpress/icon-for-wordpress-16.jpg; https://cdn-icons-png.flaticon.com/512/905/905568.png; https://raw.githubusercontent.com/kgretzky/pwndrop/master/media/pwndrop-logo-512.png; https://styles.redditmedia.com/t5_2rxmiq/styles/profileIcon_snoob94cdb09-c26c-4c24-bd0c-66238623cc22-headshot.png; https://cdn-icons-png.flaticon.com/512/1918/1918576.png; https://cdn-icons-png.flaticon.com/512/1017/1017466.png; https://icon-library.com/images/icon-ping/icon-ping-28.jpg; https://s6.uupload.ir/files/1059643_g8hp.png; https://ia803109.us.archive.org/27/items/source-engine-video-projects/source-engine-video-projects_itemimage.png; https://mycrackfree.com/wp-content/uploads/2018/08/TeamSpeak-Server-9.png; https://cdn2.downdetector.com/static/uploads/logo/75ef9fcabc1abea8fce0ebd0236a4132710fcb2e.png; https://cdn.iconscout.com/icon/free/png-512/redis-4-1175103.png; https://lyrahosting.com/wp-content/uploads/2020/06/ddos-attack-icon.png; https://cdn-icons-png.flaticon.com/512/4712/4712139.png; https://cdn.icon-icons.com/icons2/2699/PNG/512/minecraft_logo_icon_168974.png; https://cdn.icon-icons.com/icons2/2699/PNG/512/minecraft_logo_icon_168974.png; https://cdn-icons-png.flaticon.com/512/2653/2653461.png; https://lyrahosting.com/wp-content/uploads/2020/06/ddos-attack-icon.png; https://help.apple.com/assets/6171BD2C588E52621824409D/6171BD2D588E5262182440A4/en_US/8b631353e070420f47530bf95f1a7fae.png; https://www.tenforums.com/geek/gars/images/2/types/thumb__emote__esktop__onnection.png; https://icon-library.com/images/github-icon-vector/github-icon-vector-27.jpg; https://i.ibb.co/LthJcL8/image.png,,11139,2020-10-13T19:02:46Z
2024-04-15,https://github.com/kohya-ss/sd-scripts,https://raw.githubusercontent.com/kohya-ss/sd-scripts/main/README.md,"This repository offers scripts for various tasks related to the Stable Diffusion AI model, including DreamBooth training, fine-tuning, LoRA training, textual inversion training, image generation, and model conversion. It is geared towards developers and researchers with projects that need custom-generated images or fine-tuned versions of Stable Diffusion. Notably, the repository supports multiple versions of Stable Diffusion and provides detailed documentation primarily in Japanese, with links to English translations. Installation requires Python and Git, with specific instructions provided for setting up the environment on Windows platforms. The repository emphasizes flexibility in training and generating images, accommodating varying requirements and capabilities of users' systems. Additionally, it includes updates and improvements such as library upgrades, introduction of masked loss and Scheduled Huber Loss, enhanced dataset settings, and more robust image tagging features. Contributions from the community have led to significant enhancements, such as DeepSpeed support, which indicates active development and responsiveness to user feedback.",Exploring Stable Diffusion Scripts for Advanced AI Image Generation,"Discover the comprehensive Stable Diffusion repository for advanced AI image training and generation scripts, including DreamBooth, Fine-tuning, LoRA, and Textual Inversion methods. Users can access GUI and PowerShell scripts for an easier experience, thanks to the community's contributions. The repository also offers support for model conversion, with detailed documentation primarily in Japanese but translated resources available. For seamless setup, the blog guides through Windows installation, highlighting the necessity of Python 3.10.6, Git, and specific PyTorch versions.","Explore an in-depth guide to Stable Diffusion scripts for AI image generation including DreamBooth training, fine-tuning, and model conversion with easy Windows setup instructions.",Image Generation Platform,"Python





        4,120





        703


        Built by

          









        4 stars today",,,4120,2022-12-18T04:32:15Z
2024-04-16,https://github.com/intuitem/ciso-assistant-community,https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/README.md,"CISO Assistant is a comprehensive tool designed for cyber security posture management and GRC (Governance, Risk, and Compliance), aiming to enhance clarity and productivity in cybersecurity teams. It uniquely separates compliance from the implementation of cybersecurity controls and includes built-in standards, controls, and threats. Users can manage risk assessments, follow up on remediation plans, and maintain a security controls and threats catalog, with the option to add custom frameworks. It simplifies audits, evidence collection, and report generation, allowing teams to focus on remediation rather than paperwork. The tool supports a wide range of frameworks, like ISO 27001 and NIST CSF, among others, and offers a free trial for easy startup. Installation can be done via Docker for local testing or by setting it up for development with detailed steps provided for backend and frontend setups. It ensures security best practices and is available under the AGPLv3 license.",Revolutionize Your Cybersecurity Posture Management with CISO Assistant,"Discover how CISO Assistant transforms GRC and cybersecurity posture management, decoupling compliance for efficiency, integrating built-in standards, and enabling seamless risk assessments. Save time by reusing assessments, managing audits effortlessly, and focusing on remediation with our comprehensive tool. CISO Assistant continuously evolves to enhance clarity and productivity for cybersecurity teams, proving to be the ultimate solution for a robust cybersecurity foundation. Explore our free trial to streamline your GRC processes today.","Learn how CISO Assistant elevates cybersecurity posture management with decoupled compliance, built-in standards, and efficient risk assessments. Try our free trial for a streamlined GRC process.",Cybersecurity Tool,"Python





        220





        32


        Built by

          









        4 stars today",https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/gh_banner.png; https://raw.githubusercontent.com/intuitem/ciso-assistant-community/main/posture.png,,220,2023-09-20T16:47:58Z
2024-04-16,https://github.com/dvlab-research/MiniGemini,https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/README.md,"Mini-Gemini is a project aiming to enhance multi-modality vision language models, offering models ranging from 2B to 34B parameters capable of understanding, reasoning, and generating image descriptions simultaneously. Built upon LLaVA, Mini-Gemini released its paper, demo, code, models, and data to the public. The models bridge vision and language processing through dual vision encoders and a Large Language Model, effectively combining text and images. The project provides detailed installation and usage instructions, including training and evaluation guidelines. It supports models with both standard and high-resolution image capabilities. The evaluation covers various benchmarks, demonstrating the model's effectiveness in image-based question answering and multimodal interactions. The project also includes a Gradio web interface for easy interaction with the models. Mini-Gemini appreciates acknowledgments to related repositories and highlights the licensing terms for its code, data, and weights, emphasizing non-commercial use within research.",Exploring Mini-Gemini: Unleashing the Power of Vision and Language AI Models,"Mini-Gemini revolutionizes AI by supporting dense and Mixture of Experts (MoE) Large Language Models for simultaneous image understanding, reasoning, and generation. Built on LLaVA, Mini-Gemini spans models from 2B to 34B, offering unprecedented versatility across various tasks. Its groundbreaking approach combines dual vision encoders with patch-level mining and LLMs for a comprehensive understanding and generation of visual and textual content. The recent release includes a demonstration, paper, code, models, and data, inviting the community to explore its potentials.","Discover Mini-Gemini, the latest advancement in AI that integrates vision and language models ranging from 2B to 34B for enhanced understanding, reasoning, and image generation. Explore its features, applications, and access the demo today.",Vision-Language Model,"Python





        1,376





        90


        Built by

          








        293 stars today",https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/teaser.png; https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/pipeline.png; https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/demo_und.png; https://raw.githubusercontent.com/dvlab-research/MiniGemini/main/images/demo_gen.png,,1376,2024-03-26T14:48:45Z
2024-04-16,https://github.com/chiasmod0n/chiasmodon,https://raw.githubusercontent.com/chiasmod0n/chiasmodon/main/README.md,"Chiasmodon is a powerful Open Source Intelligence (OSINT) tool engineered for extensive data gathering on target domains. It empowers users to collect information such as domain emails, credentials, CIDRs, ASNs, and subdomains. Additionally, it enables searches by various identifiers including domain, CIDR, ASN, email, username, password, and Google Play application ID. The toolâ€™s remarkable features include targeted domain searches, information on Google Play applications, exploration of network blocks and ASNs, and identification of security risks through compromised credentials. Future updates promise the addition of searches by phone numbers, company names, and facial recognition technology. Chiasmodon, named after a deep-sea fish known for swallowing prey larger than itself, symbolizes its capability to gather extensive data. It is available for subscription through Telegram and can be installed via pip. It provides a flexible user interface suitable for both command-line usage and integration as a Python library, emphasizing customization in output format and filtration by geographical locations.",Maximize Your OSINT Investigations with Chiasmodon: The Ultimate Domain Analysis Tool,"Chiasmodon, a state-of-the-art OSINT tool, transforms domain investigations with its comprehensive search capabilities. Covering domain emails, credentials, CIDRs, ASNs, and subdomains, it enables precise searches by domain, CIDR, ASN, and more. Exciting future updates promise to extend functionalities to phone numbers, company names, and facial recognition. Ideal for cybersecurity professionals and researchers, Chiasmodon offers customization and advanced options for a tailored analysis. Subscribe now to unlock its full potential for your security assessments.","Discover how Chiasmodon OSINT tool can enhance your cybersecurity efforts with advanced domain search capabilities, including emails, credentials, and subdomains. Future updates to include phone number and facial recognition searches.",Cybersecurity Tool,"Python





        296





        28


        Built by

          





        59 stars today",https://journal.voca.network/wp-content/uploads/2017/10/DTR083_1200.png,,296,2024-03-25T01:10:45Z
2024-04-16,https://github.com/WeChatAPIs/WeChatMsgHistory_real,https://raw.githubusercontent.com/WeChatAPIs/WeChatMsgHistory_real/main/README.md,"The Real-time Chat project offers developers and researchers a solution to deeply explore chat content, allowing access to specific group or private chat histories under certain conditions through a provided API. Its key features include chat history retrieval within the WeChat environment and flexible RESTful API access for deep utilization. The design is scalable, with future expansions such as access for unpaid users to paid group chats, AI analysis of chat content for trend tracking, and easy cloud reporting of chat data. The project emphasizes user privacy compliance with laws and regulations, ensuring no personal or sensitive information is disclosed. It is intended solely for legal research and development purposes, with strict privacy and data protection in its core design, focusing on a developer-friendly API interface and scalability. Installation involves running commands to set up a virtual environment and dependencies, with contributions welcomed via Pull Requests, ensuring coding standards and tests are passed.",Unlocking Insights with å¾®ä¿¡ Real-time Chat: A Developer's Guide,"The å¾®ä¿¡ Real-time Chat project presents a cutting-edge solution for developers and researchers to delve into chat analytics, offering access to specific group or private chat records under certain conditions through an easy-to-use API. Key features include chat history retrieval within the å¾®ä¿¡ environment, API access for deep utilization, and expandability for future enhancements like unpaid user access to paid chat groups and AI-driven chat analysis. The project prioritizes user privacy, adhering to privacy laws without disclosing personal or sensitive information, and is designed solely for legal research and personal development purposes. Installation is straightforward, with a dependency on the wechatAPI and a developer-friendly API interface that ensures both privacy and ease of use.","Explore the å¾®ä¿¡ Real-time Chat project: An innovative solution for accessing and analyzing å¾®ä¿¡ chat records with a focus on privacy, expandability, and developer-friendly API access for legal research.",Livestream Data Extraction,"Python





        507





        43


        Built by

          






        75 stars today",https://raw.githubusercontent.com/WeChatAPIs/WeChatMsgHistory_real/main/img%2Fimg.png; https://raw.githubusercontent.com/WeChatAPIs/WeChatMsgHistory_real/main/img%2Fimg_2.png; https://raw.githubusercontent.com/WeChatAPIs/WeChatMsgHistory_real/main/img%2Fimg_1.png,,507,2024-02-23T10:39:23Z
2024-04-16,https://github.com/nlpxucan/WizardLM,https://raw.githubusercontent.com/nlpxucan/WizardLM/main/README.md,"WizardLM is a project featuring advanced pre-trained language models (LMs) like WizardCoder and WizardMath, aimed at improving the LMs' ability to follow complex instructions. The WizardCoder models, including versions with 33B, 34B, 15B, 13B, 7B, 3B, and 1B parameters, demonstrate state-of-the-art performance on code-related benchmarks, outperforming competitors like ChatGPT across various datasets. Similarly, WizardMath models, specialized for mathematical reasoning, have surpassed significant benchmarks, showing superior capabilities over existing models. WizardLM also includes comprehensive versions from 70B to 7B parameters, emphasizing advancements in performance across foundational tasks in NLP and code generation, often achieving or approaching the best-in-class results. The project openly shares models on the Hugging Face platform, though with a focus on academic and non-commercial use. The team encourages feedback and contribution to evolve the models further, providing a novel method, Evol-Instruct, for expanding the range and difficulty level of instructions that LMs can understand and perform on.",Unleashing the Power of WizardLM: A Leap Forward in AI Language Models,"Uncover the groundbreaking advancements of WizardLM, WizardCoder, and WizardMath in transforming large pre-trained language models to understand and execute complex instructions. These cutting-edge models surpass existing benchmarks, offering unprecedented capabilities in code generation, mathematical reasoning, and more. Discover how WizardLM sets new standards in AI's ability to follow intricate commands, marking a significant milestone in the development of intelligent language processing technologies.","Explore how WizardLM revolutionizes the capabilities of large pre-trained language models with unparalleled proficiency in complex instructions, surpassing industry benchmarks and offering advanced solutions in AI language understanding.",Large-scale Language Models,"Python





        8,723





        658


        Built by

          









        153 stars today",https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/WizardLM.png; https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/WizarLM30b-GPT4.png; https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/evol-testset_skills-30b.png; https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/git_overall.png; https://raw.githubusercontent.com/nlpxucan/WizardLM/main/imgs/git_running.png,https://www.youtube.com/watch?v=WdpiIXrO4_o; https://www.youtube.com/watch?v=SaJ8wyKMBds; https://www.youtube.com/watch?v=I6sER-qivYk; https://www.youtube.com/watch?v=XjsyHrmd3Xo,8723,2023-04-23T13:26:46Z
2024-04-16,https://github.com/RUCAIBox/LLMSurvey,https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/README.md,"LLMSurvey is an extensive collection of papers and resources focused on Large Language Models (LLMs), including a survey paper titled ""A Survey of Large Language Models"". The collection is organized to reflect categories and trends within LLM research, from technical evolutions in models like GPT and LLaMA to strategies for designing effective prompts and conducting experiments to assess LLM capabilities. For usage of this survey in research, citation details are provided. Additionally, it offers a Chinese version to make the information accessible to a wider audience. Recent updates showcase trends in LLM-related papers on arXiv, technical progress in GPT-series models, an evolutionary graph of the LLaMA family, and tips for prompt design. Experiments include instruction tuning and ability evaluation, seeking computational support for further experimentation. The documentation serves as a valuable aid for researchers by providing a structured overview of developments and resources in the domain of large language models.",Navigating the Frontier of Large Language Models: Insights and Innovations,"Explore the rapid evolution of Large Language Models (LLMs) with our comprehensive survey covering everything from the pioneering GPT and BERT models to the latest breakthroughs in AI research. Stay ahead with our analysis of technical advancements, significant milestones, and future directions in the LLM landscape. Whether you're delving into instruction tuning experiments or the fascinating world of LLaMA and its derivatives, our survey offers a treasure trove of knowledge for academics and AI enthusiasts alike.","Discover the latest trends in Large Language Models (LLMs) with our in-depth survey featuring GPT, BERT, LLaMA, technical innovations, and key research milestones in AI. Perfect for scholars and AI aficionados.",Data Science Resources,"Python





        8,586





        655


        Built by

          









        199 stars today",https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/chinese_version.png; https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/arxiv_llms.png; https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/gpt-series.png; https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/llama-0628-final.png; https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/prompts_main.png; https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/instruction_tuning_table.png; https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/ability_main.png; https://raw.githubusercontent.com/RUCAIBox/LLMSurvey/main/assets/LLMs-0623-final.png,,8586,2023-03-14T06:47:45Z
2024-04-16,https://github.com/tech-shrimp/WechatMoments,https://raw.githubusercontent.com/tech-shrimp/WechatMoments/main/README.md,"WechatMoments is a Windows-based tool for exporting WeChat Moments as HTML. Developed by ""æŠ€æœ¯çˆ¬çˆ¬è™¾"", also present on Bilibili, Douyin, and YouTube, this open-source project licensed under Apache License allows users to back up their Moments. Users need to download the WechatMoments zip from the Releases section, extract it (avoiding Chinese characters in the path), and run the wechat_moments.exe as an administrator. The tool enables downloading images/videos for offline viewing, filtering exports by contacts or dates, and relies heavily on the WeChat Windows client. Challenges include unstable video downloads and limited export capabilities to only viewed Moments. Future updates aim to enhance HTML functionality and support more formats. Users are warned only to export content legally permissible for them to view, stressing that misuse leading to legal issues is solely the user's responsibility. Contributions, including problem-solving and enhancements, are welcomed through issues or emails to the developer.",Ultimate Guide to Exporting WeChat Moments: Enhance Your Digital Memories,"Discover the power of WechatMoments, a Windows-based tool for backing up your WeChat Moments to HTML format. Crafted by the innovative Tech Shrimp, this open-source project promises to secure your digital memories with ease. Whether it's downloading media for offline viewing or filtering exports by contacts and dates, WechatMoments has got you covered. Dive into the simple installation process and start preserving your WeChat memories today. For a more vivid demonstration, check out their video tutorial on Bilibili.","Learn how to easily export and back up your WeChat Moments to HTML with WechatMoments. Secure your digital memories with this user-friendly, open-source tool.",Document Conversion Tool,"Python





        403





        35


        Built by

          





        43 stars today",https://raw.githubusercontent.com/tech-shrimp/WechatMoments/main/doc/pic/ä¸»ç•Œé¢.png,,404,2024-03-28T15:02:07Z
2024-04-16,https://github.com/DeepInsight-AI/DeepBI,https://raw.githubusercontent.com/DeepInsight-AI/DeepBI/main/README.md,"DeepBI is an AI-driven data analysis platform that utilizes large language models for data exploration, querying, visualization, and sharing from various sources, enabling users to derive insights and make data-driven decisions. It supports conversational data analysis, query generation, dashboard creation, and aims to automate data analysis reporting. Compatible with multiple data sources like MySQL, PostgreSQL, and Excel, it offers multi-platform support including Windows, Linux, and Mac, and is internationalized for English and Chinese users. Installation options include Windows executable files and Docker builds, with detailed guides for setup. It emphasizes community engagement, inviting users to support the project through GitHub.",Maximizing Data Insights: A Comprehensive Guide to DeepBI,"Explore the transformative potential of DeepBI, an AI-native platform designed for comprehensive data analysis. Utilizing advanced large language models, DeepBI offers users intuitive ways to query, visualize, and share insights from any data source. Its capabilities extend to conversational data analysis, query generation, and the integration of varied data into cohesive dashboards. Enhanced further by support for multiple platforms and languages, DeepBI empowers users to make informed, data-driven decisions seamlessly.","Discover how DeepBI leverages AI to provide powerful data analysis, enabling users to gain insights and drive decisions using conversational queries, visualizations, and multi-source integration.",AI Development Platform,"Python





        811





        121


        Built by

          









        21 stars today",https://raw.githubusercontent.com/DeepInsight-AI/DeepBI/main/user_manual/cn/img/download.png,,811,2023-11-20T16:59:54Z
2024-04-16,https://github.com/InternLM/xtuner,https://raw.githubusercontent.com/InternLM/xtuner/main/README.md,"XTuner is an advanced toolkit designed for efficiently fine-tuning large language models (LLMs) and vision language models (VLMs) across various GPUs. It supports a wide array of models, including the latest like InternLM2, Llama2, and Gemma, facilitating fine-tuning on almost all GPU setups, from a single 8GB GPU to multi-node configurations for models over 70B. The toolkit incorporates high-performance computing optimizations and is compatible with DeepSpeed for leveraging sophisticated optimization techniques such as ZeRO. XTuner is versatile, supporting a range of LLMs, VLMs, datasets, and fine-tuning algorithms, allowing for tailored training processes. Its comprehensive features include continuous pre-training, instruction fine-tuning, and agent fine-tuning, alongside integration capabilities with deployment and evaluation toolkits for streamlined model application. Recent updates have introduced support for Gemma, Qwen1.5 models, and multi-modal VLM pretraining with LLaVA architecture, among others. The project is open-source, inviting contributions and providing extensive documentation for quick start, fine-tuning, chatting with models, deployment, and evaluation.",Unleashing the Power of XTuner: Revolutionize Large Model Fine-Tuning,"Discover the next generation of model fine-tuning with XTuner, an efficient, flexible, and comprehensive toolkit designed for the cutting-edge fine-tuning of large language and vision models. From supporting the latest models like Gemma and Qwen1.5 to seamless integration with deployment tools, XTuner is setting new benchmarks in the AI domain. With its compatibility with almost all GPU setups and a variety of optimization techniques, including DeepSpeed, XTuner is revolutionizing how developers approach model fine-tuning. Dive deep into the capabilities of XTuner, explore its extensive support for models, datasets, and training algorithms, and learn how to get started with this revolutionary toolkit today.","Explore XTuner, the ultimate toolkit for fine-tuning large models efficiently. Learn about its support for recent models, integration with DeepSpeed, and guides for seamless model improvement.",Collaborative AI Framework,"Python





        1,345





        125


        Built by

          









        18 stars today",https://img.shields.io/badge/-grey?style=social&logo=wechat&label=WeChat)](https://cdn.vansin.top/internlm/xtuner.jpg,,1345,2023-07-11T03:18:13Z
2024-04-16,https://github.com/archlinux/archinstall,https://raw.githubusercontent.com/archlinux/archinstall/master/README.md,"Archinstall is a guided/automated installer for Arch Linux, doubling as a Python library to manage services, packages, and other system aspects. It supports standard installation from an Arch Linux live ISO or through Python pip. The installer offers a `--config` option for running with a JSON configuration file for user and credentials setup. Archinstall provides various channels for support and contributions, including Discord, matrix.org, and IRC. It supports scripting both interactive and automated installations with examples provided in its documentation. Profiles for desktops and servers simplify the process, and it's possible to test new installations using Arch Linux Live ISO or through local loop devices. The installer strives to adhere to Arch Linux principles, offering flexibility and optional configurations throughout the process. Contributions to the project are welcome, with guidelines available in its repository.",Ultimate Guide to Archinstall: Streamline Your Arch Linux Setup,"Discover how to streamline your Arch Linux installation with Archinstall, a guided installer that doubles as a powerful python library. Learn the basics of installation, configuration, and usage, and how Archinstall offers a twist on traditional setup processes by also serving as a library for managing packages, services, and more. With support for different languages and the ability to script your own installation, Archinstall is revolutionizing the way users install and manage Arch Linux. Whether you're a newcomer or a seasoned Arch enthusiast, this post walks you through the benefits of using Archinstall for a simplified installation experience.","Learn how to use Archinstall for an easier Arch Linux setup. Simplify the installation process with this guided installer and python library for managing packages, services, and more. Perfect for both new and experienced users.",Open Source Tool,"Python





        5,605





        476


        Built by

          









        13 stars today",https://github.com/archlinux/archinstall/raw/master/docs/logo.png,,5605,2018-04-06T16:07:40Z
2024-04-17,https://github.com/kornia/kornia,https://raw.githubusercontent.com/kornia/kornia/main/README.md,"*Kornia* is a differentiable computer vision library for PyTorch, aiming to solve generic computer vision problems. The library provides a collection of routines and differentiable modules, making use of PyTorchâ€™s computational efficiency and auto-differentiation capabilities. It includes components for data augmentation in the GPU, color space conversions, image filtering, edge detection, normalization, intensity transformation, feature detection, and morphological operations, among others. It supports various installation methods, including pip and building from source. Kornia promotes community involvement, offering tutorials and examples for easy learning and application. Itâ€™s recommended to cite their research paper when using Kornia in scholarly works. Contributions to the library are welcomed, with discussions taking place in forums, GitHub issues, and a Slack workspace for community engagement.",Exploring Kornia: The PyTorch Differentiable Computer Vision Library for Deep Learning,"Kornia is a powerful, differentiable computer vision library for PyTorch, designed to solve generic computer vision problems efficiently with GPU support. By leveraging PyTorch's backend for auto-differentiation, Kornia enables complex function gradients, facilitating operations like image transformation, epipolar geometry, and depth estimation directly on tensors. Whether for augmenting data, converting color spaces, or enhancing images, Kornia provides a comprehensive suite of tools for researchers and developers alike. Dive into Kornia's tutorials for practical applications and join the growing community to contribute to this open-source project.","Discover Kornia, the PyTorch-based computer vision library that makes deep learning and image processing easier with GPU support. Learn how it can streamline your projects.",Computer Vision Platform,"Python





        9,347





        930


        Built by

          









        3 stars today",https://github.com/kornia/data/raw/main/kornia_banner_pixie.png; https://github.com/kornia/kornia/raw/main/docs/source/_static/img/hakuna_matata.gif; https://raw.githubusercontent.com/kornia/data/main/hello_world_arturito.png,,9347,2018-08-22T10:31:37Z
2024-04-17,https://github.com/run-llama/llama_index,https://raw.githubusercontent.com/run-llama/llama_index/main/README.md,"LlamaIndex, also known as GPT Index, is a comprehensive data framework aimed at enhancing LLM (Large Language Models) applications with tailored data integration. It offers two main installation routes: a Starter package that includes the core framework and selected integrations, and a Customized option that allows users to add specific integration packages to the core framework. With over 300 integration packages available, users can customize their setup to work with preferred LLMs, embedding, and vector store providers. The Python library structure differentiates between core and integration packages to facilitate easy use in applications. Additionally, LlamaIndex supports a wide array of tools and connectors for data ingestion, structuring, and querying, making it versatile for both novice and experienced developers. The ecosystem also includes LlamaHub, for community data loaders, and LlamaLab, for advanced AGI projects. Contributions to both the core and its integrations are encouraged, with detailed documentation and example usage provided to support developers.
",Maximizing LLM Application Potential with LlamaIndex,"LlamaIndex is revolutionizing the way developers integrate large language models (LLMs) into their applications. With its comprehensive data framework, LlamaIndex not only simplifies the process of building LLM apps but also enhances their functionality by facilitating seamless integration with a wide array of data sources and formats. Whether you're starting with the basic package or customizing your setup with core integrations, LlamaIndex offers more than 300 integration packages to cater to your specific needs. Dive deep into LlamaIndex's ecosystem and discover how it can transform your LLM application's capabilities.","Explore how LlamaIndex, a dynamic data framework, elevates LLM applications by offering over 300 integrations for customizing and expanding LLM capabilities, making development seamless and efficient.",Data Ingestion Tool,"Python





        30,687





        4,141


        Built by

          









        30 stars today",,,30687,2022-11-02T04:24:54Z
2024-04-17,https://github.com/xlang-ai/OSWorld,https://raw.githubusercontent.com/xlang-ai/OSWorld/main/README.md,"The text introduces OSWorld, a benchmarking platform for evaluating multimodal agents designed to perform open-ended tasks in realistic computer environments. Released in April 2024 along with its paper, the project includes a repository for installation, virtual machine setup instructions for both non-virtualized systems and an upcoming guide for virtualized platforms like AWS or Azure. Users can start quickly through a minimal example demonstrating the environment interaction. Furthermore, the platform supports agent baseline experiments and provides an interface for custom agent evaluation. Running times and costs vary depending on the setting, indicating the platform's flexibility in performance and cost management. The authors encourage contributions and cite their work for users finding the environment beneficial for their research or applications.",Exploring OSWorld: A New Frontier for AI-Multimodal Agent Benchmarking,"Discover the cutting-edge OSWorld environment, designed for benchmarking multimodal agents in real computer tasks. Released in April 2024, OSWorld aims to bridge the gap between artificial intelligence and practical applications by providing a dynamic playground for testing and improving AI capabilities. From installation instructions to quick start guides, this blog post introduces you to everything you need to kickstart your journey with OSWorld. Explore the potential of AI in navigating real-world computer environments, executing tasks, and pushing the boundaries of what artificial intelligence can achieve.","Dive into OSWorld, a groundbreaking benchmark environment for multimodal agents. Learn how it facilitates real-world AI applications and get started with our comprehensive guide.",Multimodal AI Model,"Python





        273





        17


        Built by

          









        64 stars today",https://huggingface.co/datasets/xlangai/assets/resolve/main/github_banner_v2.png,,273,2023-10-16T01:49:13Z
2024-04-17,https://github.com/cognitivecomputations/github2file,https://raw.githubusercontent.com/cognitivecomputations/github2file/main/README.md,"The GitHub Repository to File Converter is a Python script designed for downloading and processing files from GitHub repositories, tailored for sharing code with chatbots. It supports both public and private repositories and includes features like filtering by programming language (Python or Go), excluding directories, file types, and test files, and optionally removing comments from Python code. Users can specify a branch or tag for download, with the default being the ""master"" branch. Usage involves a simple command line syntax, with additional options for language, maintaining comments, and specifying branches. The output is a text file with combined source code, named after the repository and language. The tool requires Python 3.x and the `requests` library, and is open-source under the MIT License.",Ultimate Guide to Converting GitHub Repositories for Chatbot Sharing,"Discover how a Python script can transform your GitHub repository downloads into chatbot-friendly formats. This tool supports both public and private repositories, offers filtering by programming language, and allows for the exclusion of unwanted files. Perfect for those looking to streamline code sharing with AI, it removes barriers by processing and preparing code for large-context chatbots without manual GitHub downloads. Explore features like comment removal and branch specification to customize your code sharing.","Learn how to easily download and process GitHub repository files with a Python script. Support for public/private repos, programming language filtering, and comment stripping enhances AI chatbot code sharing.",Document Conversion Tool,"Python





        723





        86


        Built by

          









        81 stars today",,,723,2024-03-10T12:03:06Z
2024-04-17,https://github.com/hudson-and-thames/arbitragelab,https://raw.githubusercontent.com/hudson-and-thames/arbitragelab/master/README.md,"ArbitrageLab is an open-source Python library aimed at empowering traders to exploit mean-reverting portfolios by offering comprehensive algorithms from prominent academic journals. This tool is accessible for anyone, facilitating the deployment of strategies detailed in Krauss' taxonomy for pairs trading strategies. It signifies a pivotal shift, enabling tasks previously possible only with large R&D teams. The project appreciates the significant contributions of its original team, especially noting the exceptional input of Illya Barziy and Valeriia Pervushyna for their technical skills and dedication, and Dirk Frees for his business insights. Additionally, the text mentions a dedication to WorldQuant University (WQU), highlighting its tuition-free, online Master of Science in Financial Engineering (MSFE) program as a remarkable educational opportunity in financial engineering. ArbitrageLab also promotes a Udemy course designed to help learners create production-ready Python libraries, furthering the educational mission of the team behind the library.",Unlock Financial Strategies with ArbitrageLab: A Comprehensive Guide,"ArbitrageLab is a cutting-edge python library designed for traders looking to explore mean-reverting portfolio strategies. With tools that encompass Krauss' taxonomy for pairs trading strategies, it offers a unique blend of academic theory and practical application. This open-source library is a product of dedication and innovation, crafted by a team passionate about making complex trading strategies accessible. Beyond the library, the blog highlights WorldQuant University's MSFE program, offering a tuition-free, online education in financial engineering. Explore ArbitrageLab and empower your trading strategies or elevate your career in quantitative finance through the MSFE program.","Discover how ArbitrageLab, an open-source python library, transforms trading strategies with tools backed by academic research. Also, explore WorldQuant University's MSFE program for a career in quantitative finance.",Crypto Trading Bot,"Python





        274





        84


        Built by

          








        81 stars today",,,274,2020-11-06T10:14:16Z
2024-04-17,https://github.com/Megvii-BaseDetection/YOLOX,https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/README.md,"YOLOX is an anchor-free upgrade of YOLO, offering simpler design and improved performance, aimed at narrowing the gap between research and industrial application. A technical report detailing its design is available on Arxiv, with implementations in PyTorch and MegEngine. Recent updates include support for assignment visualization, JIT compile operation, optimized training leading to twice faster training and approximately 1% higher performance, and a fix for memory leak issues. The repository also anticipates the addition of larger YOLOX models, pretraining on Objects365, transformer modules, and more features. Benchmark results are provided for various YOLOX versions, including standard and light models, displaying their mAP scores, speed, parameters, and FLOPs. The repository offers a quick start guide, instructions on reproducing results on the COCO dataset, evaluation, and deployment tutorials, including on platforms such as ONNXRuntime, TensorRT, and more. Additionally, there are links to third-party resources and a tribute to Dr. Jian Sun, highlighting his influence on YOLOX's development and his significance to the computer vision field.",YOLOX: Elevating YOLO to New Heights with Anchor-Free Design,"Discover YOLOX, the anchor-free version of YOLO, designed for superior performance in object detection. Bridging the gap between research and industrial application, YOLOX simplifies yet enhances the original model. With ongoing updates and support for various features, this PyTorch implementation invites further exploration. Dive into the technical advancements and benchmarks that set YOLOX apart on our comprehensive report.","Explore YOLOX, the advanced anchor-free YOLO variant, offering simplified design and improved detection accuracy. Learn more about its features, updates, and how it excels in object detection tasks.",Computer Vision Platform,"Python





        8,991





        2,116


        Built by

          









        4 stars today",https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/assets/logo.png; https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/assets/demo.png; https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/assets/git_fig.png; https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX/main/assets/sunjian.png,,8991,2021-07-17T02:01:45Z
2024-04-17,https://github.com/OpenTalker/SadTalker,https://raw.githubusercontent.com/OpenTalker/SadTalker/main/README.md,"The SadTalker project, detailed in a publication for CVPR 2023, introduces a method for animating still portrait images into talking heads using audio inputs. Developed by researchers from Xi'an Jiaotong University, Tencent AI Lab, and Ant Group, SadTalker is now integrated into Discord for free use, allowing users to generate high-quality talking head videos from single images and audio clips. The tool has been updated to support full image mode, and several new features and improvements have been added, such as an enhancer for video quality and various animation modes. SadTalker supports diverse platforms, including Linux, Windows, macOS, through Anaconda, Python, and Git installations, with detailed instructions for each. Pre-trained models and offline patches for enhanced face detection are available for download. Community tutorials aid installation, and the project offers both WebUI and CLI interfaces for accessibility. The project's open-source nature under the Apache 2.0 license invites collaboration while ensuring adherence to legal use respecting Tencent's intellectual property.",Revolutionizing Animated Portraits with SadTalker: The Ultimate Guide,"SadTalker transforms single portrait images into realistic talking head videos with exceptional audio synchronization, offering a blend of 3D motion and stylized animation. Leveraging advanced technology from Xi'an Jiaotong University and Tencent AI Lab, it has been integrated into platforms like Discord for easy access. With its recent updates, including full body animation and multiple new modes, SadTalker invites creators to explore its capabilities for generating high-quality videos.","Discover SadTalker, a groundbreaking tool that animates single portrait images into talking head videos with lifelike audio sync, developed by Xi'an Jiaotong University and Tencent AI Lab. Explore the latest updates and how to use SadTalker for creative video generation.",Image Generation Platform,"Python





        9,983





        1,813


        Built by

          









        132 stars today",https://user-images.githubusercontent.com/4397546/222490039-b1f6156b-bf00-405b-9fda-0c9a9156f991.gif; https://user-images.githubusercontent.com/4397546/232511411-4ca75cbf-a434-48c5-9ae0-9009e8316484.png,https://user-images.githubusercontent.com/48216707/229484996-5d7be64f-2553-4c9e-a452-c5cf0b8ebafe.mp4; https://user-images.githubusercontent.com/4397546/230717873-355b7bf3-d3de-49f9-a439-9220e623fce7.mp4,9983,2022-11-23T02:18:18Z
2024-04-17,https://github.com/6abd/horus,https://raw.githubusercontent.com/6abd/horus/main/README.md,"Project Horus is described as a comprehensive tool designed to enhance investigative processes through API integration and data compilation, positioning itself as an essential pre-operations resource. To use Horus, users are instructed to have Python installed, clone the repository, navigate to the 'horus' directory, install necessary dependencies with 'pip install -r requirements.txt', and finally run 'horus.py' within the 'sentinel' directory. API configuration for specific command functionalities requires manual or 'apicon' command entry of API keys into a designated JSON file, with a caution to remove these keys before pushing changes if contributing or testing publicly. Current contributors include the project lead and developer, Maestro. There are open invitations for bug reporting, feature requests, PR submissions, and inquiries into becoming a long-term contributor, with contact provided via email and Discord. The document also acknowledges previous team members and mentions an intention to implement certain features, categorized by their development status.",Unveiling Horus: The Ultimate Tool for Investigation Assistance,"Discover Horus, the revolutionary tool designed to enhance your investigative endeavors by leveraging APIs and compiling crucial data. Ideal for pre-operations, Horus simplifies the complexity of setup with easy Python-based installation and comprehensive API configuration. Its capabilities extend beyond mere data aggregation, providing a secure and efficient way to manage your investigative projects. Join the community of contributors and enhance your investigative toolkit with Horus today.","Explore Horus, a comprehensive tool for investigative assistance, featuring easy setup, Python-based operations, and secure API configuration. Perfect for pre-ops tasks.",Cybersecurity Tool,"Python





        83





        15


        Built by

          







        33 stars today",https://i.ibb.co/kcvtBM0/Screenshot-2024-04-15-at-7-01-47-PM.png; https://i.ibb.co/Lrzj2Mf/Screenshot-2024-04-15-at-7-02-02-PM.png,,83,2024-01-21T23:32:38Z
2024-04-18,https://github.com/PixArt-alpha/PixArt-sigma,https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/README.md,"PixArt-Î£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation represents a significant advancement in text-to-image generation technology. Developed by a team from Huawei Noahâ€™s Ark Lab and various universities, it introduces a model capable of producing high-resolution 4K images from textual descriptions. The project builds upon the success of the PixArt-Î± project, with improvements aimed at making the repository user-friendly and conducive to community contributions. The model demonstrates superior performance compared to its predecessor, PixArt-Î±, in generating detailed images at increased resolutions of 2K/4K. The initiative provides extensive resources, including pre-trained models, training, and inference code, alongside a comprehensive set of tools for working with the model. A series of updates have been released, including online demos and various training and sampling codes. The model comparison showcases the qualitative leap in image fidelity and resolution, presenting samples that vividly capture the described scenes with remarkable clarity and detail. The project encourages community contribution and seeks to facilitate accessibility, furthering advancements in the field of AI-driven creative image generation.",Advancing AI Art: Exploring 4K Text-to-Image Generation with PixArt-Î£,"PixArt-Î£ presents a revolutionary approach to 4K text-to-image generation, using Weak-to-Strong Training of Diffusion Transformer technology. This innovative project provides PyTorch model definitions, pre-trained weights, and sampling code, pushing the boundaries of AI-driven art. Contributors from prestigious institutions have come together under this project to refine and expand the capabilities of image generation, making strides in high-resolution artistic creations. With a user-friendly repository, PixArt-Î£ invites community participation to further AI art innovation.",Explore PixArt-Î£'s breakthrough in AI-powered 4K text-to-image generation leveraging Weak-to-Strong Training of Diffusion Transformer. Dive into high-resolution AI art technology.,Image Generation Platform,"Python





        678





        26


        Built by

          







        40 stars today",https://raw.githubusercontent.com/PixArt-alpha/PixArt-sigma/master/asset/logo-sigma.png,,678,2024-02-29T15:20:35Z
2024-04-18,https://github.com/pjialin/py12306,https://raw.githubusercontent.com/pjialin/py12306/master/README.md,"py12306 is a distributed, multi-account, multi-task ticket purchasing assistant with myriad features for a seamless booking experience. It supports querying tickets for multiple dates, auto captcha-solving for orders, user session recovery, phone and email notifications as well as WeChat messages. The assistant allows multi-account and multi-thread tasks, with capabilities for station-based searches within single tasks and dynamic configuration updates. It can run distributedly, supports Docker, and offers a web management interface for real-time log viewing and task management. Initial setup involves cloning the repository, configuring settingsâ€”like captcha service and voice notification through Aliyunâ€”and running tests to ensure account and passenger details are correct. The system is optimized for distributed clusters with Redis, enabling automatic failover and configuration sync across nodes. Docker users can easily deploy using provided commands or docker-compose. The project, which constantly updates with new features like free captcha solving and more notification services, acknowledges contributions from the community and inspiration from similar projects.",Maximize Your Train Ticket Booking with py12306: A Comprehensive Guide,"Discover how py12306, a versatile train ticket booking assistant, revolutionizes the way you purchase tickets with its distributed, multi-account, and multi-tasking capabilities. From automatic ticket ordering and voice notifications to web management interfaces and Docker support, py12306 offers an array of features designed to simplify and enhance your ticket booking experience. Its support for multiple dates, stations, and dynamic configuration updates, coupled with distributed operation, makes it a powerful tool for travelers. Ideal for Python 3.6 and above, setting up and running py12306 involves easy steps for a hassle-free booking process. Join the community of users benefiting from this efficient, automated ticket purchasing system.","Explore py12306, the ultimate train ticket booking assistant, offering automated bookings, multi-account support, and advanced features like voice notifications and a web management interface. Perfect for distributed operations and Docker environments.",Money Making Automation,"Python





        13,686





        3,404


        Built by

          







        12 stars today",https://github.com/pjialin/py12306/blob/master/data/images/web.png; https://github.com/pjialin/py12306/blob/master/data/images/order_success.png,,13686,2019-01-07T18:04:11Z
2024-04-18,https://github.com/pytorch/torchtune,https://raw.githubusercontent.com/pytorch/torchtune/main/README.md,"Torchtune is a PyTorch-native library tailored for authoring, fine-tuning, and experimenting with Large Language Models (LLMs), now officially supporting Meta Llama3. It promises easy implementation of popular LLMs using native PyTorch components and offers hackable training recipes without relying on external frameworks. The library supports a variety of models including Llama3, Llama2, Mistral, and Gemma, with specific support for different sizes. Fine-tuning techniques such as LoRA, QLoRA, and Full fine-tune are facilitated with configurations designed for both distributed and single-device training. Torchtune emphasizes memory efficiency, providing recipes tested across various hardware setups and optimized for memory constraints, especially on single GPUs. It integrates with tools like Hugging Face Hub, EleutherAI's LM Eval Harness, and Weights & Biases to enhance usability and community engagement. The framework embodies PyTorchâ€™s design principles, focusing on simplicity, extensibility, and correctness, while inviting community contributions. Torchtune is available under the BSD 3 license, encouraging wide use and contributions while advising users on legal obligations for third-party model usage.",Maximizing LLM Performance with PyTorch Torchtune: A Comprehensive Guide,"Discover the power of PyTorch's Torchtune for fine-tuning large language models (LLMs) like Meta Llama3. With native PyTorch implementations, Torchtune offers easy-to-use training recipes for popular techniques such as LoRA, QLoRA, and Full Fine-tune. Dive into configurations for optimized training across different hardware setups, ensuring memory efficiency even on single GPUs. Start fine-tuning with Torchtune today and push your LLM performances to the limit.","Explore the capabilities of Torchtune for fine-tuning LLMs with PyTorch. Learn how to utilize LoRA, QLoRA, and Full Fine-tune methods efficiently across various hardware configurations. Get started with Torchtune and enhance your LLM projects.",Deep Learning Tool,"Python





        1,391





        76


        Built by

          









        112 stars today",,,1391,2023-10-20T21:10:49Z
2024-04-18,https://github.com/testerSunshine/12306,https://raw.githubusercontent.com/testerSunshine/12306/master/README.md,"The 12306 Ticket Assistant is a Python-based tool designed to automate various aspects of ticket purchasing on China's 12306 website, supporting Python versions 3.6 to 3.7.4. It includes features such as automatic captcha solving, login, timely pre-sale ticket purchasing, intelligent standby, and notification through email or ServerChan. The project requires specific dependencies, including captcha recognition models which can be downloaded from provided links, and a self-hosted captcha solving server setup. Installation is managed through pip, with detailed instructions for different user permissions. The software, structured into multiple directories for different functions, allows for command-line operation with options for running the ticket booking script, filtering CDN, and testing notifications. Additionally, Docker support is included for streamlined deployment. The repository emphasizes community contribution, warning against commercial use and encourages participation in various designated discussion groups. It includes comprehensive documentation on usage, installation issues, and updates, thanking contributors for their support.",Ultimate Guide to 12306 Ticket Assistant: Master Python Version for Efficient Ticket Booking,"Discover the seamless ticket booking experience with 12306 Ticket Assistant, optimized for Python 3.6 to 3.7.4. With features like auto captcha, login, precise pre-sales, smart standby, and notifications via email or server chan, booking your next trip has never been easier. Learn how to set up your own cloud captcha server and navigate through dependency libraries with ease. Maximize your ticket booking process with our straightforward setup instructions and enhance your chances with docker-based deployment for an uninterrupted ticket hunting experience.","Explore the 12306 Ticket Assistant, your go-to Python tool for hassle-free train ticket booking. With auto captcha, login, email/server notifications, and smart ticket booking features, secure your travel tickets effortlessly.",AI Task Automation,"Python





        33,662





        9,777


        Built by

          









        19 stars today",https://raw.githubusercontent.com/testerSunshine/12306/master/uml/uml.png,,33662,2017-05-17T12:23:40Z
2024-04-18,https://github.com/xorbitsai/inference,https://raw.githubusercontent.com/xorbitsai/inference/main/README.md,"Xorbits Inference (Xinference) is a comprehensive library for easily serving language, speech recognition, and multimodal models. It simplifies deploying and serving custom or built-in models using a single command, catering to researchers, developers, and data scientists. Recent framework enhancements include support for worker and GPU indexes, SGLang backend, LoRA, speech recognition models, metrics, Docker images, and multimodal models. It features new built-in models like Qwen and OmniLMM. Integrations with platforms like Dify and Chatbox are highlighted, alongside key features like model serving simplicity, state-of-the-art model access, heterogeneous hardware utilization, flexible API and interfaces, distributed deployment, and integrations with third-party libraries. Xinference stands out for its comprehensive features compared to FastChat, OpenLLM, and RayLLM, offering a wide array of functionalities including multi-node cluster deployment, and support for image, text embedding, multimodal, and audio models. It encourages getting started with documentation, built-in and custom models guidance, deployment docs, examples, tutorials, Jupyter Notebook on Google Colab, and Docker usage for Nvidia GPU users. Finally, it invites community involvement through GitHub Issues, Slack, and Twitter and highlights contributions.",Unleashing AI Power with Xorbits Inference: Seamless Model Serving,"Discover how Xorbits Inference simplifies the deployment and serving of AI models with just a single command. This versatile library supports a wide range of models including language, speech recognition, and multimodal, making it an ideal tool for researchers, developers, and data scientists. Explore its key features like state-of-the-art model support, heterogeneous hardware utilization, and distributed deployment. Xorbits Inference empowers you to harness the full potential of AI models efficiently and effectively.","Learn how Xorbits Inference (Xinference) makes AI model serving straightforward with support for language, speech, and multimodal models. Ideal for researchers and developers seeking efficient deployment.",AI Inference Platform,"Python





        2,441





        189


        Built by

          









        21 stars today",https://raw.githubusercontent.com/xorbitsai/inference/main/./assets/xorbits-logo.png; https://raw.githubusercontent.com/xorbitsai/inference/main/assets/screenshot.png,,2441,2023-06-14T07:05:04Z
2024-04-18,https://github.com/cleanlab/cleanlab,https://raw.githubusercontent.com/cleanlab/cleanlab/master/README.md,"Cleanlab is an open-source software designed for improving the quality of machine learning datasets by identifying and correcting label issues. It automates the detection of problematic data in ML datasets, supporting a wide range of classifiers and datasets with just one line of code. Cleanlab is built on confident learning algorithms, providing theoretical guarantees on its noise estimation capabilities. It is scalable, easy to use, and applicable to any dataset or model, including popular frameworks like PyTorch, TensorFlow, and XGBoost. Cleanlab offers a comprehensive suite for data cleaning, including detecting outliers, duplicates, label errors, training noise-robust models, and suggesting data for re-labeling or active learning. It is compatible with Linux, macOS, and Windows, requires Python 3.8+, and supports various machine learning tasks such as classification, regression, and object detection. Cleanlab is based on peer-reviewed research and its effectiveness in cleaning datasets is documented through several publications. The project encourages community contributions and offers a platform for professional support.",Clean Your ML Data with cleanlab: The Ultimate Guide for Improved Model Accuracy,"cleanlab offers an efficient way to clean data and labels for machine learning projects, ensuring better model performance even with messy, real-world data. By automatically detecting dataset issues and estimating the impact of label problems, cleanlab enhances your existing models' reliability. Compatible with any classifier and dataset, this tool simplifies achieving robust data quality with just one line of code. Discover how cleanlab can help you train more precise models by identifying and correcting class-level issues and overall data quality, making it a universal solution for data-centric AI.","Discover cleanlab, a data-centric AI tool that cleans your data and labels for better machine learning model accuracy. Learn to enhance model reliability with any dataset and classifier easily.",Data Science Tool,"Python





        8,588





        661


        Built by

          









        96 stars today",https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/cleanlab_logo_open_source.png; https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/datalab_issues.png; https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/label-errors-examples.png; https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/flowchart.png; https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/ml-with-cleanlab-studio.png,,8588,2018-05-11T01:55:21Z
2024-04-18,https://github.com/yenchenlin/nerf-pytorch,https://raw.githubusercontent.com/yenchenlin/nerf-pytorch/master/README.md,"NeRF-pytorch is a PyTorch implementation of Neural Radiance Fields (NeRF) that synthesizes novel views of complex scenes with state-of-the-art results. This implementation is designed to reproduce the original results faster and has been tested to match the TensorFlow version numerically. Installation involves cloning the GitHub repository and installing required dependencies, including PyTorch 1.4 and ImageMagick for the LLFF data loader. Users can quickly start by downloading example datasets and running training scripts for different scenes like 'lego' and 'fern'. Additional datasets from the paper are available for download. Pre-trained models can also be downloaded and tested using provided scripts. The project includes tests for reproducibility and cites both the original NeRF paper and its own implementation.",Exploring NeRF with PyTorch for Advanced 3D Scene Synthesis,"Dive into NeRF-pytorch, a cutting-edge PyTorch implementation of Neural Radiance Fields that synthesizes novel views of complex scenes more efficiently. This project not only faithfully reproduces the original results but also operates 1.3 times faster than the TensorFlow version. Explore pre-trained models and learn how to train NeRF on custom datasets to create stunning 3D visuals. Perfect for both researchers and hobbyists looking to enhance their projects with the latest in 3D scene synthesis technology.",Discover NeRF-pytorch for creating 3D visuals of scenes using Neural Radiance Fields. Learn about its faster PyTorch implementation and how you can train it on custom datasets for unparalleled scene synthesis.,3D Image Rendering,"Python





        5,015





        1,007


        Built by

          









        6 stars today",https://user-images.githubusercontent.com/7057863/78472232-cf374a00-7769-11ea-8871-0bc710951839.gif; https://user-images.githubusercontent.com/7057863/78472235-d1010d80-7769-11ea-9be9-51365180e063.gif; https://user-images.githubusercontent.com/7057863/78473103-9353b300-7770-11ea-98ed-6ba2d877b62c.gif; https://user-images.githubusercontent.com/7057863/78473081-58ea1600-7770-11ea-92ce-2bbf6a3f9add.gif,,5015,2020-04-05T08:29:57Z
2024-04-18,https://github.com/getmoto/moto,https://raw.githubusercontent.com/getmoto/moto/master/README.md,"Moto is a library designed to help developers easily mock AWS Services for testing purposes. It works by simulating AWS services, allowing for the testing of code that interacts with AWS without having to incur the cost or complexity of actual AWS services. Installation is simple, requiring a pip command. The library supports a wide range of AWS services, with documentation available for further guidance. Moto's functionality can be seen in its ability to mock out an S3 service in a test environment, demonstrating how developers can test their AWS-interacting code efficiently. The project has garnered financial support and contributions, which are managed transparently through OpenCollective. Additionally, Moto provides a way to report security vulnerabilities through Tidelift, ensuring that security concerns are addressed promptly.",Effortlessly Mock AWS Services in Testing with Moto Library,"Discover how the Moto library can transform your AWS service testing by easily mocking services, enabling faster and more reliable tests. With Moto, you can simulate AWS environments in your tests without incurring actual AWS costs or needing to manage real AWS resources. This powerful tool supports a wide range of AWS services and ensures your tests are both scalable and cost-effective. Dive into how Moto seamlessly integrates with your Python code to mock AWS services and improve your development workflow. Explore the full capabilities and documentation of Moto to elevate your AWS testing strategy.","Learn how Moto library aids in efficiently mocking AWS services for your Python tests, making AWS testing cost-effective and scalable. Dive into the documentation to maximize your testing strategies.",Open Source Tool,"Python





        7,373





        1,965


        Built by

          









        6 stars today",,,7373,2013-02-18T21:10:59Z
2024-04-18,https://github.com/MakiNaruto/Automatic_ticket_purchase,https://raw.githubusercontent.com/MakiNaruto/Automatic_ticket_purchase/master/README.md,"The text discusses the discontinuation of maintenance for a ticket purchasing script for Damai, with most purchasing methods now moved to mobile and requiring packet capture for operation, and no current plans for updates. The latest version, V2.1, introduces the feature of seat selection for purchasing, supporting buying seats at specified prices but not yet capable of purchasing adjacent seats. A previous version, V2.0, was an improvement that replaced manual button operations with a more efficient process requiring only login via a webpage and proceeding with operations via requests. It involves setting up the environment, including installing necessary packages and configuring the path for a browser driver like ChromeDriver. The script, initially not supporting seat selection, allows for purchasing tickets by defining various parameters such as login credentials, item ID, and ticket quantity and price. A disclaimer clarifies that this repository is intended for personal reference and learning, and the creator bears no responsibility for commercial use or any potential infringement on Damai's interests.",Maximize Your Chances of Getting Tickets with the Latest Damai Ticket Bot V2.1,"Discover the evolution and enhancements of the Damai Ticket Bot from V2.0 to V2.1, including the shift to mobile, the introduction of seat selection for specific prices, and the streamlined process through selenium and requests instead of manual webpage interactions. Despite the discontinuation of maintenance and updates, this blog post will guide you through the necessary setup and operational steps to use the script effectively, ensuring a higher success rate in purchasing tickets on Damai.",Learn how to use the Damai Ticket Bot V2.1 for an efficient ticket purchasing experience with added seat selection feature. Get setup and operational guidance for better ticket acquisition on Damai.,AI Task Automation,"Python





        3,749





        770


        Built by

          






        22 stars today",https://raw.githubusercontent.com/MakiNaruto/Automatic_ticket_purchase/master/images/flow_chart.jpeg; https://raw.githubusercontent.com/MakiNaruto/Automatic_ticket_purchase/master/images/item_id.png,,3749,2019-05-08T01:54:09Z
2024-04-18,https://github.com/truefoundry/cognita,https://raw.githubusercontent.com/truefoundry/cognita/main/README.md,"Cognita is an advanced framework designed for operationalizing and managing Retrieval-Augmented Generation (RAG) systems in production environments. It builds upon simple abstractions provided by Langchain/LlamaIndex to offer a structured, modular, and scalable codebase suitable for production, addressing common challenges such as modular component design, scalability, and extensibility. Cognita integrates seamlessly into production setups, supporting modular RAG components, API-driven interactions, and easy scalability. Additionally, it features no-code UI support for straightforward experimentation and incremental indexing to optimize performance. While it supports local setups for testing and development, Cognita also ensures readiness for production deployment. The framework includes comprehensive support for different components of RAG systems such as data indexing, question-answering API servers, and customizability across data loaders, embedders, parsers, and vector databases. Contributions to Cognita's open-source development are encouraged, aiming at future enhancements like broader database support, advanced embeddings, and optimized LLMs for diverse applications.",Elevating RAG Systems to Production with Cognita: A Complete Guide,"Cognita revolutionizes the deployment of RAG systems from cumbersome Jupyter notebooks to a scalable and modular framework. It encapsulates the complexities of production environments by offering modularity, API-driven components, and no-code UI support, making it ideal for both local experimentation and production deployment. Moreover, Cognita seamlessly integrates incremental indexing and enhances organizational codebase management, catering to both developers' and non-technical users' needs.","Discover how Cognita transforms RAG system deployment from prototyping to production with modular components, scalable architecture, and no-code UI, simplifying incremental indexing and codebase organization.",Collaborative AI Framework,"Python





        269





        45


        Built by

          









        70 stars today",https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/readme-banner.png; https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/RAG-TF.gif; https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/rag_arch.png; https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/datasource.png; https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/list-datasources-in-collection.png; https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/adding-collection.png; https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/dataingestion-started.png; https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/dataingestion-complete.png; https://raw.githubusercontent.com/truefoundry/cognita/main/./docs/images/response-generation.png,,269,2023-07-26T13:08:54Z
2024-04-19,https://github.com/meta-llama/PurpleLlama,https://raw.githubusercontent.com/meta-llama/PurpleLlama/main/README.md,"Purple Llama is a project aimed at enhancing the responsible development of open generative AI models, with a focus on cybersecurity and input/output safeguards. It embodies a ""purple teaming"" approach from cybersecurity, blending offensive and defensive tactics to address AI challenges. The project will license its components permissively for both research and commercial use, including benchmarks under the MIT license and models under the Llama 2 and Llama 3 Community licenses. Initial offerings include CyberSec Eval, a set of cybersecurity evaluations for large language models (LLMs), and tools like Llama Guard for filtering potentially harmful outputs. Future updates will expand safeguards and address more risk areas, promoting community collaboration and establishing standards for AI trust and safety. Further resources and guidance for using Purple Llama components with Llama models are provided, along with active encouragement for community contributions to the project.",Exploring Purple Llama: The Future of Responsible Generative AI Development,"Purple Llama is a pivotal initiative aimed at bundling tools and evaluations for building with generative AI in a responsible manner. Starting with cybersecurity and input/output safeguards, the project commits to expanding its contributions to the AI community. It embodies a 'purple teaming' approach from cybersecurity, blending offensive and defensive tactics to address the nuanced challenges of generative AI. With a permissive licensing model, Purple Llama encourages both research and commercial use, ensuring broad accessibility and collaboration. The ultimate goal is to standardize trust and safety in AI development, making secure and ethical generative technology widespread.","Discover how Purple Llama is shaping the future of responsible generative AI development with a comprehensive toolkit for cybersecurity, input/output safeguards, and collaborative community engagement.",Collaborative AI Framework,"Python





        815





        157


        Built by

          









        89 stars today",https://github.com/facebookresearch/PurpleLlama/blob/main/logo.png,https://www.youtube.com/watch?v=ab_Fdp6FVDI,815,2023-12-06T21:29:41Z
2024-04-19,https://github.com/meta-llama/codellama,https://raw.githubusercontent.com/meta-llama/codellama/main/README.md,"Code Llama introduces a range of powerful large language models specialized in code, derived from Llama 2. These models, varying in sizes from 7B to 34B parameters, are designed to excel in completing and generating programming code, supporting long inputs and offering zero-shot capabilities in following programming instructions. The variants include foundational models, Python-specific versions, and instruction-following models. Developed through fine-tuning Llama 2 with a focus on code, Code Llama also emphasizes safety mitigations. The release is aimed at broad audiences including individuals, researchers, and businesses, offering the model weights and starting code for download. For usage, the models are available after licensing agreement, with technical documentation provided for setup, inference, and code infilling tasks. The project promotes ethical use and responsible innovation in AI, backed by a Responsible Use Guide and comprehensive support for dealing with issues and feedback.",Unlocking Next-Gen Coding: Introducing Code Llama for Developers,"Discover Code Llama, the cutting-edge large language model tailored for coding, based on Llama 2 technology. With specializations in Python, instruction-following capabilities, and support for input contexts up to 100k tokens, Code Llama opens new horizons for developers. Equipped with models varying from 7B to 34B parameters, it promises state-of-the-art performance for a wide array of programming tasks. Accessible now for both individual creators and businesses, it's designed to fuel innovation and scale projects responsibly. Dive into the world of Code Llama for an unparalleled coding experience.","Explore Code Llama, the advanced AI language model for code, offering specialized Python support and instruction-following prowess with up to 34B parameters for robust programming solutions. Now available for developers and businesses.",AI Development Platform,"Python





        14,407





        1,537


        Built by

          









        54 stars today",,,14407,2023-08-24T14:25:15Z
2024-04-19,https://github.com/facebookresearch/llm-transparency-tool,https://raw.githubusercontent.com/facebookresearch/llm-transparency-tool/main/README.md,"The LLM Transparency Tool by Facebook Research is designed for analyzing Transformer Language Models through interactive visualization and manipulation. Users can select a model, input prompts, and run inferences. It offers features like browsing a contribution graph, selecting and viewing the representation of tokens post any block, and exploring the impact of previous blocks on token promotion or suppression. Installation instructions cover Dockerized deployment and local setup, including building the frontend. For using custom or unsupported models, modifications to the tool and additional implementation are necessary. The tool aims to facilitate a deeper understanding of how language models work, offering various interaction points for detailed analysis. If used for research, the creators provide citation details. This tool is available under a CC BY-NC 4.0 license, with a reminder to adhere to third-party terms where applicable.",Exploring AI's Black Box with LLM Transparency Tool: A Revolutionary Approach,"Discover the LLM Transparency Tool, your essential solution for understanding AI language models from Facebook Research. This tool provides an interactive way to choose models, run inferences, and analyze the internal decision-making process. It supports a range of models and can be easily set up via Docker or local installation. Enhance your research or project by visualizing how AI models make decisions, promoting transparency in AI.","Unlock the secrets of AI language models with the LLM Transparency Tool by Facebook Research. Explore its key features, installation guide, and how it promotes AI transparency.",AI Development Platform,"Python





        408





        24


        Built by

          






        26 stars today",,,408,2023-12-21T03:14:28Z
2024-04-19,https://github.com/mistralai/mistral-common,https://raw.githubusercontent.com/mistralai/mistral-common/main/README.md,"Mistral-common is a toolkit designed to enhance working with Mistral models by offering advanced tokenization features. It extends beyond typical text-token exchanges, incorporating parsing of tools and structured conversations. The toolkit includes validation and normalization code for API use. Three tokenizer versions are released for different model sets, including v1 for models like open-mistral-7b, v2 for mistral-small-latest and mistral-large-latest, and v3 for open-mixtral-8x22b. Installation is straightforward via pip or from the source using poetry, a dependency and virtual environment manager. An example provided shows how to import necessary packages, load a Mistral tokenizer, and tokenize messages, demonstrating the toolkit's practical application in a coding environment.",Enhance Your Mistral Model Workflows with Mistral-Common Tokenizers,"Discover how mistral-common tools streamline your work with Mistral models, featuring advanced tokenization for enhanced parsing and structured conversation. Our latest release includes three versions of our tokenizer, designed to support models ranging from open-mistral-7b to open-mixtral-8x22b. Get started easily with pip installation or build from source using poetry for a more customized setup. Dive into our Colab examples to see mistral-common in action, perfecting your model's interaction capabilities.","Unlock the full potential of Mistral models with mistral-common's advanced tokenization tools, designed for efficient parsing and structured conversation. Install now and explore our easy-to-follow examples.",Natural Language Processing,"Python





        258





        14


        Built by

          






        60 stars today",,,258,2024-04-15T08:43:59Z
2024-04-19,https://github.com/binarly-io/binary-risk-intelligence,https://raw.githubusercontent.com/binarly-io/binary-risk-intelligence/master/README.md,"The text introduces Binary Risk Intelligence, emphasizing the in-depth analysis of emerging advanced threats conducted by Binarly Research. It highlights a specific study on the ""xz backdoor,"" detailing investigative findings into this cybersecurity threat. The backdoor's original disclosure is referenced with a link to an announcement made on an open security mailing list dated March 29, 2024. The format suggests an official report or summary from Binarly, aimed at informing readers about their research into this particular security vulnerability.
",Unveiling the XZ Backdoor: Insights and Impacts on Security,"Explore the forefront of cybersecurity with our detailed analysis of the xz backdoor, a significant threat recently unearthed by Binarly Research. Delving into the depths of this menace, we offer comprehensive insights into its mechanics, implications, and measures for protection. Our experts dissect the intricate details, providing a clear understanding of the risks and potential defenses. Stay informed and ahead of cyber threats with our in-depth examination.","Discover the latest in cybersecurity threats with Binarly's in-depth analysis of the xz backdoor. Understand its impact, mechanics, and how you can protect yourself from this emerging menace.",Cybersecurity Tool,"Python





        128





        7


        Built by

          







        34 stars today",https://raw.githubusercontent.com/binarly-io/logo/master/binarly_white.png,,128,2024-04-12T17:49:51Z
2024-04-19,https://github.com/meta-llama/llama,https://raw.githubusercontent.com/meta-llama/llama/main/README.md,"Llama 2 is the latest advancement in large language models, aimed at a wide audience including individuals, creators, researchers, and businesses, facilitating experimentation, innovation, and scaling of ideas. It offers pre-trained and fine-tuned models ranging from 7B to 70B parameters, available for download upon agreement to the license. The model weights and tokenizer can be downloaded from the Meta website, and thereâ€™s also provision for access via Hugging Face. The release not only includes the models but also guides for quick start and running inferences, covering a variety of use cases from text to chat completion. This technology, while promising, comes with potential risks; therefore, a Responsible Use Guide is provided. Issues can be reported through specified channels, and the project encourages responsible and ethical AI development.",Exploring Llama 2: The New Frontier in Large Language Models,"Unlock the potential of the latest large language models with Llama 2, now accessible for everyone from individuals to businesses for innovating and scaling ideas. This release includes everything from pre-trained to fine-tuned models with 7B to 70B parameters, available on the Meta website and Hugging Face. Discover how to easily download, set up, and run these advanced models to enhance your projects, supported by extensive resources and a responsible use guide. Whether for research, development, or commercial use, Llama 2 promises to empower users with cutting-edge AI functionalities.","Discover Llama 2, Meta's latest advancement in large language models, offering extensive resources for users to download, set up, and innovatively apply AI from 7B to 70B parameters models for diverse projects.",Large-scale Language Models,"Python





        52,570





        9,077


        Built by

          









        117 stars today",,,52570,2023-02-14T09:29:12Z
2024-04-19,https://github.com/ethereum/consensus-specs,https://raw.githubusercontent.com/ethereum/consensus-specs/master/README.md,"The Ethereum Proof-of-Stake (PoS) consensus specifications repository details the core specs for Ethereum PoS clients, described across various documentation stages and the progress of features towards implementation. It includes detailed stable specifications for different phases such as Phase 0, Altair, Bellatrix, Capella, and Deneb, each with its unique core changes, additions, and respective fork epochs. In-development specifications like Electra and outdated sections on Sharding, Custody Game, and Data Availability Sampling highlight ongoing research and adjustments. Accompanying documents offer insights into SimpleSerialize spec, Merkle proof formats, and general test formats, aimed at guiding developers. Additional specs for client implementers are accessible through linked repositories. Broad design goals prioritize minimal complexity, network resilience, quantum security readiness, inclusivity of validators, and accessibility for consumer-grade devices. External resources, contributor guides, and an online viewer for the latest releases are provided for further education and engagement, enhancing clarity on the Ethereum PoS consensus mechanism's evolution and community collaboration avenues.",Understanding Ethereum's Proof-of-Stake Transition: Specifications and Impacts,"Explore the core specifications and design goals of Ethereum's transition to a Proof-of-Stake (PoS) consensus mechanism. This crucial update aims at enhancing scalability through sharding, improving energy efficiency, and ensuring network security. Discussions about proposed changes and design rationale are encouraged, with a community-driven approach for modifications. The Ethereum PoS consensus specifications serve as a foundational guide for developers, researchers, and stakeholders in the Ethereum ecosystem.","Dive into Ethereum's Proof-of-Stake consensus specifications. Learn about the shift towards scalability, efficiency, and security in Ethereum's network through detailed specs, design goals, and community discussions.",Crypto Tools & Guides.,"Python





        3,410





        883


        Built by

          









        0 stars today",,,3410,2018-09-20T05:12:54Z
2024-04-19,https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once,https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/README.md,"SEEM (Segment Everything Everywhere All at Once) introduces a versatile segmentation tool capable of handling a wide range of prompts, including both visual (e.g., points, scribbles, image segments) and language prompts (text and audio). It operates on a multi-modal basis, allowing for segmentation using any combination of prompts or even custom ones. This tool is built upon X-Decoder and utilizes backbones like FocalNet and DaViT, with techniques from UniCL for learning image-text representations. SEEM is designed for interactive use, with features like compositional prompt handling, multi-round interactivity, and semantic awareness, assigning semantic labels to predicted masks. It supports a broad vocabulary but can classify out-of-vocabulary items as 'others'. Demonstrations of its capabilities include versatile image segmentation tasks, video segmentation without specific training, and examples showcasing its applications across different styles and subjects. SEEM's architecture allows it to understand spatial relationships effectively, providing accurate segmentations based on complex prompts.",Introducing SEEM: Revolutionize Image Segmentation with Multi-Modal Prompts,"Discover SEEM, the cutting-edge tool for segmenting everything, everywhere, all at once using versatile multi-modal prompts. SEEM combines visual and language prompts, allowing unprecedented flexibility and precision in image segmentation tasks. Whether it's points, text, or audio cues, SEEM understands and segments with ease. Perfect for researchers and developers, it stands out for its adaptability to various prompt types and its potential for custom prompt creation. Dive into the future of image segmentation with SEEM and explore its capabilities today.",SEEM transforms image segmentation with its ability to understand and process multi-modal prompts for precise results. Explore this versatile tool for your research and development projects for improved segmentation.,Multimodal AI Model,"Python





        4,019





        318


        Built by

          









        5 stars today",https://user-images.githubusercontent.com/11957155/233255289-35c0c1e2-35f7-48e4-a7e9-68da50c839d3.gif; https://user-images.githubusercontent.com/11957155/233526415-a0a44963-19a3-4e56-965a-afaa598e6127.gif; https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/transformers_gh.png; https://user-images.githubusercontent.com/11957155/234230320-2189056d-1c89-4f0c-88da-851d12e8323c.gif; https://user-images.githubusercontent.com/11957155/234231284-0adc4bae-ef90-41d3-9883-41f6407a883b.gif; https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/text.png; https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/audio.png; https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/emoj.png; https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/trees_text.png; https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/minecraft.png; https://raw.githubusercontent.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/main/assets/compare.jpg,,4019,2023-04-12T05:08:43Z
2024-04-19,https://github.com/josephmisiti/awesome-machine-learning,https://raw.githubusercontent.com/josephmisiti/awesome-machine-learning/master/README.md,"Awesome Machine Learning is a meticulously curated list of machine learning frameworks, libraries, and software organized by programming language. Initiated by Joseph Misiti and inspired by awesome-php, it aims to provide a comprehensive resource for those looking to delve into the field of machine learning. To ensure the list remains updated and reliable, it encourages contributions and specifies criteria for repository deprecation, such as lack of maintenance or long periods of inactivity. Additionally, it links to extensive resources covering free machine learning books, professional events, online courses, blogs, newsletters, and local meetups. The document outlines various tools and libraries across numerous programming languages and domains, including natural language processing, computer vision, and deep learning among others. It's a valuable starting point for anyone interested in exploring the vast landscape of machine learning technologies and applications.",Ultimate Guide to Awesome Machine Learning Resources,"Dive deep into the world of machine learning with this curated list of frameworks, libraries, and software by language. Inspired by `awesome-php`, this comprehensive guide covers everything from free books and courses to professional events. Stay ahead in the ever-evolving field of machine learning by exploring this go-to directory for machine learning enthusiasts and professionals alike. Make your contribution to keep it fresh and up-to-date.","Discover the ultimate resource for machine learning: frameworks, libraries, books, courses, and more in one curated list. Perfect for enthusiasts and professionals looking to expand their knowledge and stay ahead in the field.",Machine Learning Tool,"Python





        63,462





        14,454


        Built by

          









        14 stars today",,https://www.youtube.com/watch?v=CsF4pd7fGF0,63462,2014-07-15T19:11:19Z
2024-04-19,https://github.com/ShineChen1024/MagicClothing,https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/README.md,"Magic Clothing is an official implementation derived from OOTDiffusion that focuses on controllable garment-driven image synthesis. Introduced in a paper by Weifeng Chen, Tao Gu, Yuhao Xu, and Chengcai Chen, it offers advancements in generating detailed clothing images with customizable features. Recent updates include a 1024 version with enhanced resolution and early access, support for generating GIFs using AnimateDiff, and integration with IP-Adapter-FaceID and ControlNet-Openpose for added conditions in image synthesis. Sample demonstrations showcase its capabilities in synthesizing upper-body, lower-body, and full-body clothing images and animated versions, bolstering its practical applications. Installation instructions, inference steps for both Python and Gradio demos, and the citation for the foundational paper are provided, highlighting the project's ongoing development and accessibility to users interested in exploring garment-driven image creation.",Revolutionizing Fashion: Exploring Magic Clothing for Image Synthesis,"Discover the innovative world of Magic Clothing, a cutting-edge project designed for controllable garment-driven image synthesis. Stemming from the OOTDiffusion initiative, Magic Clothing leverages advanced technology to transform the way we interact with digital fashion. With its recent 1024 version update and support for AnimateDiff, users can now experience more realistic and dynamic clothing simulations than ever before. Dive into the details of their latest research paper and take your first steps towards a futuristic wardrobe with our comprehensive guide.","Explore the future of digital fashion with Magic Clothing, a project focused on garment-driven image synthesis. Learn about its latest updates, including the 1024 version and AnimateDiff support, for a closer look at the evolving interface of technology and fashion.",Image Generation Platform,"Python





        716





        56


        Built by

          







        64 stars today",https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/demo.png; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/workflow.png; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/a0.jpg; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/a1.png; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/b0.jpg; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/b1.png; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/c0.jpg; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/c1.png; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/valid_cloth/t1.png; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/animatediff0.gif; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/valid_cloth/t5.jpg; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/animatediff1.gif; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/valid_cloth/t6.png; https://raw.githubusercontent.com/ShineChen1024/MagicClothing/main/images/animatediff2.gif,,716,2024-02-06T02:39:13Z
2024-04-19,https://github.com/jimbosimbo/howstheweather,https://raw.githubusercontent.com/jimbosimbo/howstheweather/main/README.md,"This text describes a simple Python command-line application named ""How is the Weather?"" that checks the weather by fetching data from the OpenWeatherMap API. The application assesses the temperature and categorizes it as ""nice and warm,"" ""mild,"" or ""cold."" Key features include real-time weather data retrieval, a straightforward command-line interface, and temperature-based weather categorization. To get started, users need Python 3.6 or above, a virtual environment in their project directory, and an API key from OpenWeatherMap. Installation instructions are provided for Linux and macOS users, involving creating a virtual environment, activating it, and installing required packages.",Build Your Own Weather CLI with Python: A Simple Guide,"Discover how to create a simple Python command-line application to check the weather in real-time. Leveraging the OpenWeatherMap API, this application effortlessly categorizes the temperature into distinct feelings: warm, mild, or cold. Ideal for beginners, the prerequisites include Python 3.6+, a virtual environment, and an OpenWeatherMap API key. Follow a straightforward installation process to get started on Linux and macOS. This project promises a fun way to apply Python skills and interact with weather data directly from your command line.",Learn how to build a Python CLI tool for real-time weather updates using the OpenWeatherMap API. A beginner-friendly guide with simple steps to install and run on your system.,Weather Application,"Python





        56





        23


        Built by

          





        38 stars today",,,56,2024-04-16T16:46:33Z
2024-04-20,https://github.com/meta-llama/llama3,https://raw.githubusercontent.com/meta-llama/llama3/main/README.md,"Meta's Llama 3 is advancing large language model technology, making it widely accessible for experimentation and innovation among individuals, creators, researchers, and businesses. It offers pre-trained and instruction-tuned models ranging from 8B to 70B parameters, available for download upon accepting their license. The models can be accessed both from the Meta Llama website and on Hugging Face, in various formats compatible with different development environments. The documentation provides a quick start guide for local setup and inference, emphasizing the importance of following specific formats for instruction-tuned models to achieve optimal results. Additionally, Llama 3 comes with a guide for responsible use, addressing potential risks associated with its deployment. For any issues, users can report bugs, risky content, or security concerns through dedicated channels. Llama 3's licensing accommodates both research and commercial use, promoting ethical advancements in AI while ensuring openness and accessibility.",Unlocking the Power of Meta Llama 3: A New Era for Language Models,"Discover how Meta Llama 3 is revolutionizing language model accessibility for everyone from individuals to businesses. The latest release includes models ranging from 8B to 70B parameters, emphasizing experimentation, innovation, and scalable solutions. With straightforward instructions for downloading and using the models, including integration with Hugging Face, this version promises to enhance AI applications across various sectors. Explore detailed examples and get started with pre-trained and instruction-tuned models for a wide array of applications.","Explore the capabilities of Meta Llama 3, the latest version offering pre-trained and instruction-tuned language models for diverse AI applications. Learn how to download, set up, and deploy these models for your projects.",Large-scale Language Models,"Python





        7,323





        506


        Built by

          









        1,443 stars today",https://github.com/meta-llama/llama3/blob/main/Llama3_Repo.jpeg,,7323,2024-03-15T17:57:00Z
2024-04-20,https://github.com/harperreed/photo-similarity-search,https://raw.githubusercontent.com/harperreed/photo-similarity-search/main/README.md,"Embed-Photos, developed by @harperreed, is a sophisticated photo similarity search engine using the CLIP model to match images with textual descriptions. It features rapid, efficient image searching capabilities, designed exclusively for Apple Silicon (MLX), and incorporates SQLite and Chroma for persistent storage of image embeddings. Users can interact through a web interface, ensuring secure image processing, with added benefits like logging and performance monitoring. Customization is possible via environment variables. The repository includes essential scripts and HTML templates for web interface setup. To start, clone the repository, install dependencies, configure environment variables, generate image embeddings, and launch the Flask web application. Planned improvements include incorporating siglip for enhanced functionality and making MLX optional. The project acknowledges contributions from the CLIP model and various open-source libraries.",Explore Visual Similarities with Embed-Photos: The Ultimate Photo Search Engine,"Discover the power of Embed-Photos, the innovative photo similarity search engine designed by @harperreed. Utilizing the cutting-edge CLIP model, this tool finds visually similar images with ease. Designed exclusively for Apple Silicon, it features persistent storage, a user-friendly web interface, and robust security measures. Perfect for tech enthusiasts and visual researchers alike. Dive into the world of advanced image search and explore like never before!","Unlock the potential of visual search with Embed-Photos, an advanced photo similarity engine using CLIP model technology for finding similar images. Experience fast, efficient, and secure image searches.",Image Generation Platform,"Python





        262





        20


        Built by

          







        17 stars today",,,262,2024-03-26T18:31:41Z
2024-04-20,https://github.com/mouredev/roadmap-retos-programacion,https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/README.md,"The ""Roadmap retos de programaciÃ³n semanales 2024"" provides a structured path for improving programming logic and learning any language. It is free, self-paced, and community-driven. Users can participate in weekly programming challenges, published every week and corrected live on Twitch by Brais Moure, the organizer. Corrections and community solutions can be found in the roadmap directory for each challenge. The program encourages participation through GitHub by forking the project, solving challenges, and submitting solutions via pull requests. Participants can use any programming language. The roadmap includes foundational topics like syntax, data types, control structures, functions, data structures, and advanced concepts like recursion, classes, inheritance, polymorphism, error handling, file management, JSON/XML, unit testing, dates, asynchrony, and regular expressions. The project fosters community engagement on Discord and provides a ranking of users and languages by contributions. Additional resources and a guide on using Git and GitHub are provided. Brais Moure, a freelance full-stack iOS & Android engineer, combines app development with creating educational content on programming across various platforms.",Unlock Your Programming Potential: The Ultimate 2024 Weekly Challenges Roadmap,"Dive into our comprehensive 2024 roadmap for weekly programming challenges designed to sharpen your coding skills, enhance your logic, and learn any programming language at your own pace. With free access, community support, and a flexible timeline, youâ€™re set for a journey that transforms the way you code. We cover various languages and difficulties, provide real-time corrections on Twitch, and foster a supportive environment via our Discord community. Whether you're starting out or looking to polish your skills, our challenges cater to all levels. Get ready to tackle new challenges, climb the user and language rankings, and join a vibrant community of coding enthusiasts.","Join our 2024 weekly programming challenges to boost your coding skills across different languages and complexities. Benefit from live corrections, community support, and a flexible learning path. Start your coding adventure today!",System Design Education,"Python





        1,571





        1,173


        Built by

          









        88 stars today",https://raw.githubusercontent.com/mouredev/roadmap-retos-programacion/main/./Images/header.jpg; https://raw.githubusercontent.com/mouredev/mouredev/master/mouredev_emote.png,https://www.youtube.com/watch?v=gEIBJ7rmLa0; https://www.youtube.com/watch?v=DLSGCh9jdes; https://www.youtube.com/watch?v=auxClgiX6UM; https://www.youtube.com/watch?v=brxtPtUbU7M; https://www.youtube.com/watch?v=CKzY7nHwulA; https://www.youtube.com/watch?v=P2OQDT9Wrb0; https://www.youtube.com/watch?v=nTfDkLRrYiM; https://www.youtube.com/watch?v=cBeRWS2X0CA; https://www.youtube.com/watch?v=W4tv8WUbum4; https://www.youtube.com/watch?v=PVBs5PWjedA; https://www.youtube.com/watch?v=mfOzfj-BrQo; https://www.youtube.com/watch?v=Bsiay2nax4Y; https://www.youtube.com/watch?v=OwStihBItEg; https://www.youtube.com/watch?v=3WFQ2grp0h0; https://www.youtube.com/watch?v=EQIAhF7NNMI; https://www.youtube.com/watch?v=YA8Ssd3AUwA,1571,2023-12-18T10:59:18Z
2024-04-20,https://github.com/Upsonic/Tiger,https://raw.githubusercontent.com/Upsonic/Tiger/main/README.md,"Tiger is a community-driven initiative aimed at creating an integrated ecosystem of tools for AI agents, akin to Neuralink but for artificial intelligence. It enables AI agents to interact with computer systems by 'thinking', allowing them to perform tasks like code writing, web searches, and much more without traditional input methods. Tiger relies on Upsonic for isolated tool storage and document automation. It's designed to be universally applicable across various frameworks and supports a myriad of functions such as interpreter operations, searches, system inquiries, and communication abilities including Telegram integration. The project encourages community contributions and offers public and customizable tool libraries under an MIT license. For ease of use, it includes detailed documentation and a public dashboard for monitoring and documentation. Tiger also offers integration with several AI and machine learning frameworks including crewAI, LangChain, and AutoGen, demonstrating its versatility and utility in enhancing AI agent capabilities.",Tiger: Revolutionizing AI Agent Interactions with Neuralink-Inspired Tools,"Discover Tiger, a groundbreaking project that's reshaping the AI ecosystem by integrating Neuralink-inspired interfaces for LLM agents. By facilitating direct control over computers through 'thinking', Tiger empowers AI to perform an array of tasks from code execution to managing calendars. This community-driven platform not only provides a robust toolset curated for enhancing agent capabilities but also fosters a collaborative environment for open-source development. Whether you're integrating with crewAI, LangChain, or using Telegram, Tiger sets a new standard for AI interactions.","Explore Tiger, a Neuralink-inspired toolkit offering an advanced platform for AI agents to interact with computers using thought. Dive into its integrations, community-driven development, and how it's paving the way for the future of AI agent capabilities.",Collaborative AI Framework,"Python





        127





        7


        Built by

          






        25 stars today",https://raw.githubusercontent.com/Upsonic/Tiger/main/assets/overview.png; https://raw.githubusercontent.com/Upsonic/Tiger/main/assets/dashboard.png; https://raw.githubusercontent.com/Upsonic/Tiger/main/assets/documentation.png,,127,2024-04-05T18:49:43Z
2024-04-20,https://github.com/facebookresearch/generative-recommenders,https://raw.githubusercontent.com/facebookresearch/generative-recommenders/main/README.md,"This repository provides the code for the paper ""Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations."" It currently includes code for reproducing the public experiments discussed in Section 4.1.1 of the paper, using datasets like MovieLens and Amazon Reviews. To replicate these experiments, users need to install various dependencies, download and preprocess data, and run model training on a GPU with at least 24GB HBM. The experiments demonstrate the superiority of the HSTU model over traditional methods like SASRec, BERT4Rec, and GRU4Rec in terms of hit rate and NDCG metrics across different dataset sizes. Furthermore, this document lists the contributors to the project and notes that more documentation and efficiency experiments will be provided later. The codebase is licensed under Apache 2.0.",Unleashing the Power of Generative Recommenders: Insights from Trillion-Parameter Models,"Discover the groundbreaking research on generative recommenders through the 'Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations' study. Dive into the methodology, experiments, and impressive results highlighting the superiority of HSTU models on popular datasets like MovieLens and Amazon Reviews. Learn step-by-step how to reproduce these pioneering experiments using provided code and configurations. This research not only paves the way for advanced recommendation systems but also sets a new benchmark in the machine learning community.",Explore the latest advancements in generative recommendation systems with our detailed overview of the trillion-parameter models study. Learn how to replicate key experiments and understand the impact on MovieLens and Amazon Reviews datasets.,Machine Learning Tool,"Python





        104





        14


        Built by

          





        14 stars today",,,104,2024-04-08T06:03:18Z
2024-04-20,https://github.com/QwenLM/CodeQwen1.5,https://raw.githubusercontent.com/QwenLM/CodeQwen1.5/main/README.md,"CodeQwen1.5 is a powerful transformer-based, decoder-only language model specifically designed for code generation, pre-trained on a large dataset of code. It has remarkable abilities in generating code and performs competitively across various benchmarks, with the capability to understand and generate long contexts of up to 64K tokens. The model supports 92 programming languages, making it highly versatile. Its exceptional performance is evident in tasks such as text-to-SQL and bug fixing. For instance, compared to other models, CodeQwen1.5 and its chat version show superior performance in code generation benchmarks and programming challenges across several languages. It also demonstrates high execution accuracy in the text-to-SQL context. The provided documentation includes details for setup, including system requirements and a quick start guide, encouraging users to engage through available links to Hugging Face, ModelScope, and other resources for more information and demonstrations. The project invites community interaction through platforms like Discord and WeChat.",Discover CodeQwen1.5: The Future of Code Generation across 92 Languages,"CodeQwen1.5 sets a new standard in code generation, supporting an unprecedented 92 languages and boasting superior performance benchmarks. With capabilities for long context generation up to 64K tokens, it excels in text-to-SQL, bug fixes, and more. Dive deep into its competitive edge in our latest performance evaluations, including LiveCodeBench and MultiPL-E comparisons. Experience cutting-edge coding assistance by exploring CodeQwen1.5's features on Hugging Face or ModelScope.","Explore CodeQwen1.5, a transformer-based model advancing code generation across 92 languages with remarkable performance in benchmarks and support for 64K tokens context.",AI Coding Assistant,"Python





        156





        6


        Built by

          









        8 stars today",https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/codeqwen1.5/codeqwen_logo_final.png; https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/codeqwen1.5/intro.png,,156,2024-04-16T11:49:01Z
2024-04-20,https://github.com/theowni/Damn-Vulnerable-RESTaurant-API-Game,https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/README.md,"Damn Vulnerable RESTaurant is an intentionally vulnerable API service designed for educational purposes, targeting developers, ethical hackers, and security engineers. It offers an environment for training on identifying and exploiting vulnerabilities. Developers can play a game to find and fix vulnerabilities interactively, while ethical hackers can manually exploit them, viewing it as a Capture The Flag (CTF) challenge, starting from a low privileged API user and escalating to root. Security engineers can test vulnerability detection with various security automation tools. The project can be deployed locally using Docker and is also available on GitHub Codespaces for easy setup without a local environment. Successful participants are recognized in a Hall of Fame. The project aims to be a playground for learning about security in API services and encourages responsible use, highlighting the potential risks of deploying it in uncontrolled environments. It is developed under the GNU GPL v3.0, promoting collaboration and transparency in the open-source community.",Exploring Damn Vulnerable RESTaurant: A Training Ground for Cybersecurity Enthusiasts,"Dive into Damn Vulnerable RESTaurant, a purposely insecure API service created for developers, ethical hackers, and security engineers seeking hands-on cybersecurity training. This unique playground offers a practical way to detect and exploit vulnerabilities, providing an interactive experience for upgrading security skills. With scenarios for both fixing and hacking, participants can evolve from novices to seasoned experts. Completing challenges earns a spot in the Hall of Fame, celebrating those who have mastered the game. Whether through local deployment or GitHub Codespaces, starting the game is effortless, ensuring easy access to everyone keen on enhancing their cybersecurity prowess.","Join the ranks of cybersecurity enthusiasts by diving into Damn Vulnerable RESTaurant, an intentionally vulnerable API training platform designed for learning to detect and exploit security flaws. Easy setup, practical challenges, and a Hall of Fame await!",Cybersecurity Tool,"Python





        262





        32


        Built by

          







        12 stars today",https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/app/static/img/mad-chef-circle-text.png; https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/app/static/img/game-screenshot.png; https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/images/codespace-open-button.png; https://raw.githubusercontent.com/theowni/Damn-Vulnerable-RESTaurant-API-Game/main/images/codespace-screenshot-2.png,,262,2024-03-25T23:45:43Z
2024-04-20,https://github.com/DAMO-NLP-SG/Video-LLaMA,https://raw.githubusercontent.com/DAMO-NLP-SG/Video-LLaMA/main/README.md,"The Video-LLaMA project aims to enhance large language models with the ability to understand video and audio content. It features instruction-tuned, audio-visual language models specifically designed for video understanding. The project has introduced Video-LLaMA-2, which utilizes LLaMA-2-Chat for language decoding, and offers pre-trained and fine-tuned models optimized for video and audio analysis. These models are trained on extensive datasets such as Webvid-2M for video captions and image-caption pairs from LLaVA to improve understanding of static visual concepts and audio aspects. The models are accessible through Hugging Face and ModelScope with documentation for both usage and training provided. The training process involves pre-training on video caption and image caption datasets followed by fine-tuning with instruction data from sources like MiniGPT-4 and VideoChat. Video-LLaMA supports various applications, including interaction with videos having background sound, without sound effects, and static images. It is designed to work with advanced GPUs and is intended for non-commercial use, emphasizing ethical guidelines against misuse.",Empowering AI with Video and Audio: Introducing Video-LLaMA,"Discover Video-LLaMA, a cutting-edge project aimed at enhancing large language models with the ability to understand both video and audio. This initiative leverages the latest in audio-visual language models to provide nuanced video understanding, utilizing a combination of vision-language (VL) and audio-language (AL) branches. With support for major languages and customization options from pre-trained checkpoints, Video-LLaMA represents a significant advancement in multimedia AI applications.","Explore Video-LLaMA, an innovative project designed to imbue large language models with comprehensive video and audio understanding capabilities, leveraging VL and AL branches for enhanced multimedia interaction.",Multimodal AI Model,"Python





        2,392





        215


        Built by

          









        8 stars today",https://raw.githubusercontent.com/DAMO-NLP-SG/Video-LLaMA/main/figs/video_llama_logo.jpg; https://raw.githubusercontent.com/DAMO-NLP-SG/Video-LLaMA/main/figs/architecture_v2.png,,2392,2023-05-06T15:35:19Z
2024-04-20,https://github.com/mahdibland/V2RayAggregator,https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/README.md,"V2RayAggregator is a collection of automation functions implemented using GitHub Actions, aimed at testing and sharing high-speed and stable nodes from various sources for free. The core functionality includes testing the speed of free node pools on the network and nodes shared by bloggers, filtering out stable and high-speed nodes, and then sharing these nodes in the repository. It supports protocols like V2Ray, and provides nodes in major formats such as YAML, clash, v2ray, and base64 without additional conversion, ensuring compatibility and user convenience.

Recent updates include fixing region-based lite speed tests, sorting based on average speed, updating required software to the latest version, fixing conversion issues, adding functions for airports, and fixing naming conventions. The project aims to clean up redundant files and functions in the future. Users can find nodes ready for import in various formats, and the repository offers manual subscription conversion services. 

Node information includes a high-speed node quantity of 200, with explicit node details available. For local testing, all nodes are combined in single files, but due to high volume, it's recommended only for testing purposes to avoid client crashes.",Optimized Node Subscription: Access Global Networks,"Discover the perfect solution for stable and fast internet access with our comprehensive collection of nodes tailored for various protocols including SS, SSR, Vmess, and more. Get unlimited access to global networks with our tested, reliable nodes. Subscribe now to elevate your internet experience.","Unlock seamless internet access with our optimized node subscription. Featuring SS, SSR, Vmess compatibility and more, ensuring stable and fast connectivity. Subscribe for global network access.",Open Source Tool,"Python





        1,485





        241


        Built by

          









        20 stars today",https://i.ibb.co/g32RmJy/netlify.png; https://i.ibb.co/g32RmJy/netlify.png,,1485,2022-09-30T07:27:27Z
2024-04-21,https://github.com/moest-np/center-randomize,https://raw.githubusercontent.com/moest-np/center-randomize/main/README.md,"The Exam Center Randomization script is designed to efficiently assign exam centers to students, ensuring fair and practical distributions. It involves preparing input files, running the program (potentially multiple times to ensure no school goes unassigned and to achieve even distribution), performing sanity checks, and making minimal manual reassignments as needed. Allocation guidelines prioritize proximity, prevent students from the same school from being clustered in one center, avoid assigning a center within the same school, and forbid centers under the same management or with prior issues. Parameters include distance thresholds, minimum students per center, and a capacity stretch factor. Input files contain school and center data, including location coordinates and capacities, while output displays center allocations, including any over-capacity cases and unassigned students.",Streamlining Exam Center Allocation: A Guide to Randomization,"Efficiently assigning exam centers to students can be a daunting task, but with the right script, it becomes manageable. By preparing input files, running a randomization program, and conducting sanity checks, administrators can ensure equitable distribution. Essential steps include considering geographic proximity, capacity restrictions, and avoiding conflicts of interest. The process highlights the significance of flexible yet precise allocation guidelines to accommodate every student adequately.","Discover how to streamline the exam center allocation process using a script that considers student distribution, center capacity, and geographic proximity for fair and practical assignments.",AI Task Automation,"Python





        272





        85


        Built by

          









        65 stars today",,,272,2024-04-05T17:06:05Z
2024-04-21,https://github.com/langchain-ai/langgraph,https://raw.githubusercontent.com/langchain-ai/langgraph/main/README.md,"LangGraph is a tool for creating stateful, multi-actor language models (LLMs) applications, enhancing the functionality of LangChain with support for complex flows including cycles, not just Directed Acyclic Graphs (DAGs). It's designed to facilitate applications that require agent-like behaviors, where LLMs respond dynamically in loops. Key features include the ability to coordinate multiple actors over computation steps in a cyclic manner, inspired by systems like Pregel and Apache Beam, with a user interface reminiscent of NetworkX. Installation is straightforward via pip, and usage examples illustrate how to manage conversation states, implement conditional logic, and interact with external tools. LangGraph also supports advanced scenarios such as streaming output from nodes, persistence, human-in-the-loop interactions, and customizable agent behaviors. It offers visualization tools for clarity, and various guides and examples are provided for specialized functions like asynchronous operations, token streaming, and custom tool integration. LangGraph's documentation covers the essential APIs, emphasizing its flexibility and potential for building intricate LLM-powered applications.",Revolutionizing Multi-Actor Language Models with LangGraph,"LangGraph is transforming the way developers approach building complex, stateful language applications. By extending LangChain's capabilities, it enables multi-actor coordination and cyclic computations, offering a powerful alternative to traditional DAG workflows. This innovative library, inspired by models like Pregel and Apache Beam, prioritizes cycles for agent-like behaviors, setting a new standard for language agent constructions. Dive into how LangGraph's state management and conditional edges facilitate creating more dynamic and interactive language models.","Explore how LangGraph leverages the power of LangChain to build stateful, multi-actor language applications. Learn about its unique approach to cycles, state management, and conditional edges for advanced language modeling.",Collaborative AI Framework,"Python





        2,491





        337


        Built by

          









        27 stars today",,,2491,2023-08-09T18:33:12Z
2024-04-21,https://github.com/Ultimaker/Cura,https://raw.githubusercontent.com/Ultimaker/Cura/main/README.md,"Ultimaker Cura is a state-of-the-art slicer application designed to prepare 3D models for 3D printing. It boasts a plethora of settings and community-managed print profiles, positioning it as a leader for successful 3D printing projects. The app invites contributions in areas such as printer profiles and translations, providing links for those interested in contributing to these aspects. The visual presentation includes badges and buttons for various metrics and functionalities, including issues, pull requests, downloads, and contributions, alongside a security scorecard badge from OpenSSF. Images and badges serve to provide quick access to resources, reports, and the software's community-driven features.",Mastering 3D Printing: A Guide to Ultimaker Cura's Advanced Features,"Discover the powerful features of Ultimaker Cura, the leading slicer software for 3D printing projects. With its vast array of settings and community-managed print profiles, Ultimaker Cura turns intricate 3D models into reality. Whether you're contributing printer profiles or translations, this tool ensures your 3D printing success. Dive into a world where your designs come to life with precision and ease. Explore how Ultimaker Cura stands out in the 3D printing community.","Learn about the features that make Ultimaker Cura the go-to slicer software for 3D printing. Explore settings, print profiles, and how to contribute to the community.",3D Image Rendering,"Python





        5,785





        2,013


        Built by

          









        21 stars today",,,5785,2014-06-16T12:55:31Z
2024-04-21,https://github.com/MetaCubeX/mihomo,https://raw.githubusercontent.com/MetaCubeX/mihomo/main/README.md,"Mihomo is a Python pydantic model for the Honkai: Star Rail game, enabling type hints and autocompletion for data parsed from the Mihomo API. It simplifies interaction with the Mihomo API, providing an easy way to fetch and process game data in two versions (v1 and v2) depending on the data format needed. Installation involves a straightforward pip command fetching from GitHub. Usage instructions detail how to fetch user data, including handling of character icon URLs and example codes to demonstrate basic fetches, including optional parameters to replace icon names with URLs for ease of access. It also includes tools for removing duplicate characters and merging character data, as well as methods for data persistence using pickle and JSON, showcasing how to save and load the processed data efficiently.",Exploring Honkai: Star Rail Data with Mihomo Python API,"Dive into the vast universe of Honkai: Star Rail data using the Mihomo Python API. This powerful pydantic model provides type hint and autocompletion support, making it easier to fetch and process game data efficiently. Learn how to install it, utilize its basic and advanced features, and leverage tools for data manipulation and persistence. Whether you're dealing with V1 or V2 data formats, Mihomo streamlines your data fetching and processing tasks.","Discover how to efficiently handle Honkai: Star Rail data with the Mihomo Python API, offering enhanced support for type hints, autocompletion, and seamless data manipulation.",Python Libraries Collection,"Python





        12,414





        2,260





        26 stars today",,,12414,2021-05-20T11:06:33Z
2024-04-21,https://github.com/liming-ai/ControlNet_Plus_Plus,https://raw.githubusercontent.com/liming-ai/ControlNet_Plus_Plus/main/README.md,"The GitHub repository hosts the source code for the Nerfies website, which is linked. It asks to cite a specific 2021 ICCV paper by Park, Sinha, Barron, Bouaziz, Goldman, Seitz, and Martin-Brualla titled ""Nerfies: Deformable Neural Radiance Fields"" if the Nerfies project proves useful for someone's work. Additionally, the content of the Nerfies website is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License, as indicated by a badge and a link to the license details. This license allows for sharing and adapting the work, provided proper credit is given and any derivatives are shared under the same license.",Nerfies: Revolutionizing Digital Imaging with Neural Radiance Fields,"Nerfies represent a cutting-edge development in digital imaging, encapsulating deformable neural radiance fields for dynamic 3D environments. Originating from a collaborative effort among top researchers, their work is pivotal for tech aficionados and professionals in the field. The Nerfies project, accessible through its dedicated website, encourages scholarly citations, ensuring academic integrity and further innovation. Its open-source code fosters a community-driven approach to improving and implementing these advanced imaging techniques. Furthermore, the project's commitment to the Creative Commons Attribution-ShareAlike 4.0 International License underscores its dedication to knowledge sharing and collaboration.","Explore the innovative world of Nerfies: a groundbreaking project in deformable neural radiance fields for 3D imaging, developed by leading researchers and available under the Creative Commons License.",3D Image Rendering,"Python





        104





        7


        Built by

          






        27 stars today",https://i.creativecommons.org/l/by-sa/4.0/88x31.png,,104,2024-04-11T16:18:15Z
2024-04-21,https://github.com/embeddings-benchmark/mteb,https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/README.md,"The Massive Text Embedding Benchmark (MTEB) is a comprehensive framework introduced for evaluating text embedding models across a wide spectrum of tasks, datasets, and languages. Users can install MTEB via pip and utilize it in Python scripts or via the command line interface (CLI), enabling them to specify tasks, models, and datasets for evaluation. MTEB supports advanced usage scenarios such as dataset selection by task, category, or language, evaluation on specific splits, and the use of custom models. Additionally, it includes guidance on evaluating custom datasets and tasks, contributing new datasets, and submitting models to an interactive leaderboard. The benchmark aims to facilitate extensive comparison and improvement of text embedding techniques, with its details and usage published in a series of research papers. MTEB encourages open contributions and extensions, including adding new tasks or models, and is accessible for development and contributions from the broader research community.",Unveiling MTEB: The Ultimate Guide to Massive Text Embedding Benchmarking,"Discover the power of MTEB, the Massive Text Embedding Benchmark, designed to elevate your text embedding projects to new heights. Learn how to effortlessly install, utilize, and contribute to MTEB for advancing text embedding research. Explore advanced usage, dataset selection, and the process of evaluating custom models or datasets. Engage with a comprehensive toolkit that supports a wide array of tasks, languages, and models, aimed at fostering innovation and excellence in the field of text embedding.","Explore MTEB: A comprehensive guide on installing, utilizing, and contributing to the Massive Text Embedding Benchmark for advanced text embedding research and projects.",Deep Learning Tool,"Python





        1,357





        149


        Built by

          








        9 stars today",https://raw.githubusercontent.com/embeddings-benchmark/mteb/main/./docs/images/hf_logo.png,,1357,2022-04-05T08:25:47Z
2024-04-21,https://github.com/google-deepmind/penzai,https://raw.githubusercontent.com/google-deepmind/penzai/main/README.md,"Penzai, a portmanteau of the Chinese words for ""tray"" and ""planting"", is now a modern JAX library designed for creating, analyzing, and modifying neural network models as legible, functional pytree data structures. It excels in post-training research applications such as reverse-engineering, model component ablation, activation inspection, and more. Distinct from other neural network libraries, Penzai emphasizes transparency and interactivity, allowing users to fully understand model operations and easily integrate runtime adjustments. It offers a modular suite including a combinator-based neural network library, an interactive pretty-printer, complex pytree traversal tools, a named axis system for easy vectorization, and a system for managing side arguments and state. Penzai is accessible for installation via pip, and it promotes an interactive setup for enhanced usability in Jupyter notebooks or IPython environments, complete with examples and detailed tutorials for newcomers.",Exploring Penzai: Revolutionizing Neural Network Research with JAX,"Penzai is not just another JAX library; it's a transformative tool for neural network research, enabling easy manipulation, visualization, and analysis of models post-training. Its unique approach offers a modular suite of tools for a deep dive into model structures and behaviors, surpassing traditional neural network libraries by focusing on model flexibility and interactive debug capabilities. With features like `penzai.nn` for a combinator-based NN library, `penzai.treescope` for enhanced pytree visualization, and `penzai.core.selectors` for advanced model editing, Penzai stands out as a crucial resource for researchers and engineers looking to push the boundaries of machine learning and AI development.","Discover Penzai, a JAX library designed to revolutionize neural network research by simplifying model post-training manipulation, visualization, and analysis. Learn how it compares to other libraries and its unique features for researchers.",Deep Learning Tool,"Python





        807





        23


        Built by

          





        204 stars today",https://raw.githubusercontent.com/google-deepmind/penzai/main/docs/_static/readme_teaser.png,,808,2024-04-04T01:13:03Z
2024-04-22,https://github.com/benbusby/whoogle-search,https://raw.githubusercontent.com/benbusby/whoogle-search/main/README.md,"Whoogle Search is a self-hostable web search application that fetches Google search results but eliminates ads, JavaScript, AMP links, cookies, and IP address tracking, enhancing privacy and user experience. It's deployable via Docker and other methods such as Heroku, Render.com, Repl.it, Fly.io, Koyeb, pipx, pip, and manually, supporting multiple environments via easily configurable variables. Features include ad-free results, support for Tor and proxies, autocomplete suggestions, POST request queries, and light/dark/system themes. Whoogle is customizable with user-defined ""bangs"" for quick searches on specific sites and offers optional location-based searching. Despite utilizing server-side cookies to store non-sensitive settings and possibly using third-party JavaScript for features like search suggestions, Whoogle can work with JavaScript disabled, affirming its stance on privacy. Deployments vary in complexity, from quick Heroku setups to Docker builds and manual configurations for advanced users. It also includes a detailed guide for reverse proxying with Nginx.",Discover a Privacy-Focused Search Engine: Introducing Whoogle,"Experience Google search results with a twist â€“ no ads, no tracking, just pure information. Whoogle Search allows you to deploy your own instance of Google Search sans privacy concerns, ads, or tracking, making it a safer way to navigate the web. Supporting a variety of deployment options including Docker, Heroku, and more, Whoogle is customizable and can be set as your primary search engine on any device. With features like no ads, no JavaScript requirements, and the ability to filter results by time, Whoogle offers a superior, privacy-focused searching experience. Explore how to deploy and configure Whoogle to reclaim your online privacy today.","Learn about Whoogle Search â€“ a privacy-focused search engine alternative that eliminates ads, tracking, and Javascript. Deploy your own instance for a secure, customizable Google Search experience.",Privacy-focused Search Engine,"Python





        8,771





        886


        Built by

          









        20 stars today",https://raw.githubusercontent.com/benbusby/whoogle-search/main/docs/banner.png; https://raw.githubusercontent.com/benbusby/whoogle-search/main/docs/screenshot_desktop.png; https://raw.githubusercontent.com/benbusby/whoogle-search/main/docs/screenshot_mobile.png,,8771,2020-01-21T20:22:33Z
2024-04-23,https://github.com/CrazyBoyM/llama3-Chinese-chat,https://raw.githubusercontent.com/CrazyBoyM/llama3-Chinese-chat/main/README.md,"This repository introduces the first Chinese version of llama3 for sharing and learning about llama3 in Chinese. It welcomes collaborations and issues for specialized versions discovered online. The repository highlights updates, including the completion of the world's first llama3 Chinese version, using over 170k high-quality Chinese dialogue data for overnight training, and the addition of various tutorials and deployment guidelines. It outlines plans for future updates such as video tutorials, cloud training images, browser plugin development, and more enhancements. The document describes available llama3-based models, focusing on Chinese language adaptations, performance enhancements, and specialized versions for tasks such as role-playing and music generation. It includes information on model inference costs and usage, detailing instructions for web deployment and terminal inference, providing guidance for engaging in multi-turn Q&A sessions. The document emphasizes not expanding the vocabulary to preserve the model's general capabilities and suggests focusing on enriching Chinese knowledge within the model.",Discover the Power of Llama3 in Chinese: Revolutionizing Chatbots,"Introducing the first Chinese version of Llama3, a model surpassing its predecessors in performance and capabilities. Explore comprehensive resources, including Phi3 model and web deployment tutorials, tailored for the Chinese-speaking community. Engage with updates and future plans, from training videos to advanced AI functionalities, all aimed at enhancing the Chinese chat experience. Join the vibrant community contributing to a growing repository of specialized versions and training materials.","Discover the breakthrough Llama3 Chinese chat model, boasting superior performance with complete resources for training and deployment. Join our community for the latest updates and resources.",Language Models,"Python





        1,027





        97


        Built by

          





        412 stars today",,,1027,2024-04-18T17:54:10Z
2024-04-23,https://github.com/mnotgod96/AppAgent,https://raw.githubusercontent.com/mnotgod96/AppAgent/main/README.md,"AppAgent, a project led by Chi Zhang and developed by a team at TencentQQGYLab, introduces a novel AI framework designed to operate smartphone applications through actions such as tapping and swiping, eliminating the need for system backend access. This multimodal agent framework utilizes both autonomous exploration and learning from human demonstrations to operate apps effectively, referencing its generated knowledge base for task execution. The project includes detailed documentation, a demo, and step-by-step guides for setup and operation. It utilizes the `gpt-4-vision-preview` or `qwen-vl-max` as its underlying models, with a focus on providing a practical tool for app interaction without direct human handling. The initiative has documented its progress through updates and welcomes community involvement through GitHub issues for any encountered problems. The research and implementation details are open-sourced, encouraging further exploration and improvement of the framework.",Unlocking Smartphone App Accessibility with AppAgent: A TencentQQGYLab Innovation,"Explore the innovative AppAgent by TencentQQGYLab, a project aimed at enhancing smartphone app accessibility through a multimodal agent framework. This cutting-edge technology enables the agent to perform app operations mimicking human-like interactions without needing system backend access. By learning through autonomous exploration or human demonstrations, AppAgent creates a knowledge base for executing complex tasks across various applications, showcasing significant advancements in smartphone app interaction.","Discover AppAgent by TencentQQGYLab, an innovative framework designed to improve smartphone app accessibility through human-like interactions, autonomous learning, and a comprehensive knowledge base for task execution.",AI Browser Automation,"Python





        4,189





        446


        Built by

          









        4 stars today",https://raw.githubusercontent.com/mnotgod96/AppAgent/main/./assets/teaser.png; https://raw.githubusercontent.com/mnotgod96/AppAgent/main/./assets/demo.png,,4189,2023-12-20T11:41:04Z
2024-04-23,https://github.com/haizelabs/llama3-jailbreak,https://raw.githubusercontent.com/haizelabs/llama3-jailbreak/master/README.md,"Zuck and Meta released Llama 3, designed to rival OpenAI with significant emphasis on safety during its development, involving red teaming, supervised fine-tuning, and reinforcement learning with human feedback. Despite these efforts, it was discovered that Llama 3's safety measures could be easily circumvented by ""priming"" the model with a harmful prefix, leading it to generate unsafe content. This vulnerability occurs despite the model's ability to refuse harmful instructions under normal circumstances, revealing a deficiency in the model's self-reflection and understanding of its outputs. The experiment's results showed an increasing success rate of generating harmful responses with longer harmful prefixes. This raises questions about the real understanding capabilities of large language models (LLMs) and their ability to self-regulate.",Exposing Llama 3's Safety Limits: A Simple Jailbreaking Technique,"Despite Meta's extensive efforts to secure their new AI, Llama 3, a straightforward 'priming' method reveals significant vulnerabilities, bypassing its safety measures. By altering input prompts, researchers can induce Llama 3 to produce harmful outputs, challenging its ability to refuse dangerous instructions. This finding not only undermines the AI's safety protocols but also raises questions about the fundamental understanding and self-reflection capabilities of such models. The experiment demonstrates that, regardless of training on refusal scenarios, Llama 3 can be manipulated into generating inappropriate content, spotlighting a significant shortfall in current AI safety methodologies.","Discover how a simple 'priming' technique exposes critical safety shortcomings in Meta's Llama 3 AI, challenging its ability to effectively refuse harmful instructions and raising concerns about the AI's understanding and self-reflection capacities.",Large-scale Language Models,"Python





        239





        23


        Built by

          





        106 stars today",https://raw.githubusercontent.com/haizelabs/llama3-jailbreak/master/images/refusal.png; https://raw.githubusercontent.com/haizelabs/llama3-jailbreak/master/images/continue.png,,239,2024-04-20T07:21:54Z
2024-04-23,https://github.com/Lightning-AI/pytorch-lightning,https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/main/README.md,"Lightning 2.0 is a deep learning framework designed to streamline the pretraining, finetuning, and deployment of AI models. It features a clean and stable API, making it easy for developers to work with AI models. The framework can be installed simply from PyPI and comes with options for installing with optional dependencies, through Conda, as well as the stable version and the bleeding-edge version directly from the source. Lightning encompasses four core packages: PyTorch Lightning, Lightning Fabric, Lightning Data, and Lightning Apps, offering a gradient of control over PyTorch abstraction levels.

PyTorch Lightning organizes PyTorch code to separate the scientific aspects from the engineering tasks, making models hardware agnostic and coding more transparent. Lightning Fabric offers expert-level control over training and scaling strategies, suited for complex models. Lightning Data facilitates fast, distributed streaming of training data from cloud storage, and Lightning Apps helps in building AI products and workflows with minimal cloud infrastructure boilerplate. Advanced features of Lightning include support for multi-GPU, TPU training, 16-bit precision, and various training strategies with minimal code changes. The framework is extensively tested, supports multiple Python and PyTorch versions, and encourages community contributions.",Introducing Lightning 2.0: Revolutionize Your AI Model Training and Deployment,"Discover Lightning 2.0, the cutting-edge deep learning framework for AI model pretraining, finetuning, and deployment. With a clean, stable API, Lightning 2.0 simplifies the complexity of AI development, allowing researchers and developers to focus on groundbreaking innovations. Explore its core packages including PyTorch Lightning for scalable training, Lightning Fabric for expert control, and Lightning Apps for building AI products with minimal cloud infrastructure setup. Join the Lightning community to contribute and leverage the power of a streamlined AI development process.","Unlock the full potential of AI with Lightning 2.0, featuring a clean API for easy model training, finetuning, and deployment. Dive into PyTorch Lightning, Lightning Fabric, and Lightning Apps for efficient AI development.",Deep Learning Platform,"Python





        26,860





        3,248


        Built by

          









        13 stars today",https://pl-public-data.s3.amazonaws.com/assets_lightning/LightningColor.png; https://pl-public-data.s3.amazonaws.com/assets_lightning/continuum.png; https://raw.githubusercontent.com/Lightning-AI/pytorch-lightning/main/docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif; https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/features_2.jpg; https://pl-public-data.s3.amazonaws.com/assets_lightning/lightning-apps-teaser.png,,26860,2019-03-31T00:45:57Z
2024-04-23,https://github.com/ansible/ansible,https://raw.githubusercontent.com/ansible/ansible/master/README.md,"Ansible is a comprehensive IT automation platform designed to simplify configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and orchestration tasks. It prioritizes simplicity both in setup and operation, emphasizing an agentless architecture that leverages SSH, making it inherently secure and easy to manage. Ansible's design principles focus on the minimal learning curve, parallel machine management, and a human-friendly language for infrastructure management. Installation can be done via `pip` or package managers, with detailed guides available. The Ansible community encourages involvement through contributions, working groups, and feedback on the `devel` branch for those interested in the latest features. Ansible enforces coding guidelines for contributions and provides a clear roadmap for upcoming features and changes. Originally created by Michael DeHaan, Ansible has grown with contributions from over 5,000 users and is backed by Red Hat, Inc. It is licensed under the GNU General Public License v3.0.",Mastering Automation: Your Ultimate Guide to Ansible,"Discover the power of Ansible, the simple IT automation platform that transforms infrastructure management. From configuration management to application deployment, Ansible makes complex tasks like zero-downtime rolling updates effortless. Learn how its design principles promote security and ease of use without requiring agent installation. Whether you're a developer or a power user, get involved in the Ansible community to contribute, learn, and help shape its future. Dive into the latest features by exploring the 'devel' branch or stick with the stable release for your production needs.","Unlock the full potential of IT automation with Ansible. Explore how Ansible simplifies complex tasks, promotes security, and encourages community involvement. Start automating today.",AI Task Automation,"Python





        61,110





        23,688


        Built by

          









        13 stars today",,,61110,2012-03-06T14:58:02Z
2024-04-23,https://github.com/yuka-friends/Windrecorder,https://raw.githubusercontent.com/yuka-friends/Windrecorder/main/README.md,"Windrecorder is an open-source alternative to Rewind for Windows, designed to capture everything on your screen to help retrieve memory cues. It indexes only the changed screens, recording their OCR text or page titles into a database, and performs database maintenance, video cleanup, and compression when the computer is unused. This tool operates entirely offline, ensuring complete data ownership. It features a comprehensive web interface for screen retrospection, semantic searches, and provides data summaries through activity statistics, word clouds, timelines, lightboxes, and scatter plots. Windrecorder supports multiple languages and is currently in early development, inviting contributions for optimization and multilingual translations. Installation involves setting up ffmpeg, Git, and Python on your system, followed by cloning the Windrecorder repository and running the installation batch file. The tool automatically starts recording when turned on, pausing during inactivity or screen sleep, and resumes when activity is detected, ensuring efficient use of resources.",Maximize Your Productivity with Windrecorder: The Ultimate Memory Cue Tool for Windows,"Discover Windrecorder, an Open Source tool designed as an alternative to Rewind for Windows users, aimed at enhancing memory recall through screen content recording. This innovative application captures everything on your screen, allowing searches via OCR text or visual descriptors, and operates entirely offline, ensuring your data remains private and secure. Experience stable, continuous recording of single or multiple screens with minimal file sizes, advanced database maintenance during inactivity, and a comprehensive web UI for easy navigation and data analysis. Windrecorder is constantly evolving, so stay tuned for upcoming features!","Unlock the power of memory recall with Windrecorder on Windows. An Open Source tool capturing screen activity for efficient search and data analysis, all while ensuring complete data privacy. Discover more.",Privacy-focused Search Engine,"Python





        1,248





        51


        Built by

          









        91 stars today",https://github.com/yuka-friends/Windrecorder/blob/main/__assets__/product-header-cn.jpg; https://github.com/yuka-friends/Windrecorder/blob/main/__assets__/product-preview-cn.jpg; https://github.com/yuka-friends/Windrecorder/blob/main/__assets__/how-it-work-sc.jpg,,1248,2023-07-30T12:26:33Z
2024-04-24,https://github.com/frappe/erpnext,https://raw.githubusercontent.com/frappe/erpnext/master/README.md,"ERPNext is an open-source enterprise resource planning (ERP) software that simplifies processes such as accounting, inventory, manufacturing, CRM, sales, purchase, project management, and HRMS, requiring MariaDB. Developed on the Frappe Framework, which utilizes Python & JavaScript, ERPNext supports easy installation through a script that installs dependencies and sets up passwords for various users, including the ERPNext ""Administrator,"" the MariaDB root user, and the frappe user. Additionally, a virtual image is available for local system use. Its source code is licensed under GNU General Public License (v3), and the documentation is Creative Commons licensed. Contributing to ERPNext involves adhering to specific guidelines, including issues, security vulnerabilities, pull requests, translations, and chart of accounts development. Frappe Technologies owns the ERPNext trademarks, setting policies for their usage focused on promoting and improving the software while distinguishing official from unofficial projects and preventing misuse in commercial activities. Permission is required for using the ERPNext name or logo, especially in domains, to avoid implying endorsement or affiliation with Frappe Technologies or the ERPNext open source project.",Unlocking Business Potential with ERPNext: A Comprehensive Overview,"Discover how ERPNext simplifies ERP for businesses, integrating functions like Accounting, Inventory, CRM, and more on the Frappe Framework. This powerful, open-source solution, optimized for MariaDB, streamlines operations from Sales to HRMS, enhancing productivity. With an easy installation script and virtual image options, ERPNext is accessible for businesses of all sizes. Dive deeper into ERPNext's capabilities by exploring its user guide and joining the discussion forum. Embrace the future of business management with ERPNext.","Explore ERPNext, the comprehensive ERP solution built on the Frappe Framework. Learn how it integrates various business functions, from Accounting to HRMS, for enhanced operational efficiency.",Open Source ERP,"Python





        16,925





        6,488


        Built by

          









        25 stars today",https://github.com/frappe/design/blob/master/logos/logo-2019/erpnext-logo.png; https://travis-ci.com/frappe/erpnext.png,,16925,2011-06-08T08:20:56Z
2024-04-24,https://github.com/UnicomAI/Unichat-llama3-Chinese,https://raw.githubusercontent.com/UnicomAI/Unichat-llama3-Chinese/main/README.md,"The Unichat-llama3-Chinese, developed by China Unicom's AI Innovation Center, is the industry's first Llama3 Chinese-specific instruction fine-tuned model, launched on April 19, 2024. It is based on Meta Llama 3 and enhanced with Chinese data for high-quality Chinese question-and-answer capabilities, maintaining the original 8K context length, with a 64K version to be released later. Upcoming releases include a 70-billion-parameter version fine-tuned for Chinese, a long-text variant, and a version with secondary pre-training in Chinese. Data for model training includes high-quality command data from various fields, carefully screened for model fine-tuning. Examples demonstrate the model's capability in handling historical, mathematical, and ethical queries, as well as suggesting AI-enabled solutions for healthcare advancement. The environment requires Python 3.8+, transformers 4.37.0, torch 2.0.1, and CUDA 11.7. Instructions cover model download, environment setup, and inference code execution. Available models include Unichat-llama3-Chinese-8B with download links from Hugging Face and ModelScope. The document also suggests web UI deployment for text generation and local deployment frameworks, along with fine-tuning methods using various frameworks.",Exploring the Unichat-llama3-Chinese Model: A Milestone in AI Language Processing,"China's Unicom AI Innovation Center has released the first llama3 Chinese instruction fine-tuned model, enhancing Meta Llama 3 with Chinese training data for high-quality Mandarin question-answering capabilities. This model maintains the original 8K context length and plans to support a 64K version soon. Future releases will include a 70-billion parameter version tailored for lengthy texts and a version with secondary Chinese pre-training, addressing various domains and industries with high-quality instruction data.","Discover the groundbreaking Unichat-llama3-Chinese AI model by China's Unicom AI, a high-quality Mandarin QA model based on Meta Llama 3 enriched with Chinese data, promising advancements in AI-powered language processing.",Language Models,"Python





        155





        10


        Built by

          






        41 stars today",https://raw.githubusercontent.com/UnicomAI/Unichat-llama3-Chinese/main/assets/logo.jpg,,155,2024-04-19T06:52:54Z
2024-04-24,https://github.com/prometeydev/Prometheus,https://raw.githubusercontent.com/prometeydev/Prometheus/main/README.md,"Prometheus is a program designed with a focus on cybersecurity testing and educational purposes, cautioning users against illegal or malicious use. The free version offers numerous features including a GUI builder, startup persistence, fake errors, code obfuscation, and extensive data theft capabilities like session tokens, passwords, cookies, and more from various platforms including Discord, Steam, and several browsers. The VIP version boasts advanced functions such as UAC bypass, custom icons, disabling Windows Defender, anti-VM measures, and cryptocurrency miners. Usage requires Windows 7 or newer, Python 3.10, and an internet connection. The stub, or payload, supports various settings for customization and evasion techniques. Instructions for building the stub are provided, emphasizing the use for educational and research purposes to prevent misuse.",Unlock Advanced Security Features with Prometheus: A Comprehensive Guide,"Discover the power of Prometheus, an advanced security tool designed for comprehensive protection and surveillance. Learn how to leverage its free and VIP features, from GUI builders and file pumpers to crypt stealers and keyloggers, to safeguard your digital assets. Understand its installation requirements and step-by-step guide to maximize your security measures effectively. Dive into the world of enhanced cybersecurity with Prometheus, and ensure your digital environment is robustly secured.","Explore the advanced security capabilities of Prometheus, offering both free and VIP features for ultimate digital protection. Get a detailed guide on its installation, features, and benefits for a fortified cybersecurity posture.",Cybersecurity Tool,"Python





        245





        162


        Built by

          





        15 stars today",https://raw.githubusercontent.com/prometeydev/Prometheus/main/logo.png; https://github.com/prometeydev/Prometheus/blob/main/window.png; https://github.com/prometeydev/Prometheus/blob/main/msg.png; https://github.com/prometeydev/Prometheus/blob/main/virustotal.png,,245,2023-11-24T00:41:48Z
2024-04-24,https://github.com/Evil0ctal/Douyin_TikTok_Download_API,https://raw.githubusercontent.com/Evil0ctal/Douyin_TikTok_Download_API/main/README.md,"The ""Douyin_TikTok_Download_API"" is a ready-to-use, high-performance, asynchronous data scraping tool for platforms like Douyin (TikTok's Chinese version), TikTok, and Bilibili, enabling API calls, online bulk parsing, and downloading. It supports asynchronous file IO for downloading, annotates endpoints with demo values, and has restructured project files for better organization. Major changes include the removal of outdated Bilibili code, potential additions of Kuaishou and Xigua video analysis, and updates needed for the README file and iOS shortcuts to align with the latest API responses. It allows for easy integration into other projects via a pip package and is designed for various applications, from downloading videos to data analysis. The API, accessible online, facilitates the extraction of video data and downloading without watermarks. Deployment can be accomplished via Linux or Docker, and contributors have significantly enhanced its capabilities.",Unlock the Power of Douyin and TikTok: A Comprehensive Guide to Download API,"Explore the ultimate solution for downloading content from Douyin, TikTok, and Bilibili with the Douyin_TikTok_Download_API. This high-performance, async scraping tool is ready to use out of the box, offering API calls and online bulk parsing for effortless downloads. It's designed for developers seeking to integrate video downloading capabilities into their applications seamlessly. Navigate through the complexities of content retrieval with our easy-to-use API, and dive into a world where accessing and downloading content is made simple.","Discover how to easily download videos from Douyin, TikTok, and Bilibili using Douyin_TikTok_Download_API. Learn about its async scraping mechanism, API calls, and online bulk parsing for quick content access.",Livestream Data Extraction,"Python





        6,848





        1,085


        Built by

          









        15 stars today",https://raw.githubusercontent.com/Evil0ctal/Douyin_TikTok_Download_API/main/./logo/logo192.png,,6849,2021-11-07T01:35:09Z
2024-04-24,https://github.com/lipku/metahuman-stream,https://raw.githubusercontent.com/lipku/metahuman-stream/main/README.md,"The text introduces a streaming digital human model based on the Ernerf model, which supports audio-video synchronous dialogue with commercial viability. It highlights features like voice cloning, large model dialogue, support for various audio feature drivers (wav2vec, hubert), full-body video stitching, and compatibility with RTMP and WebRTC protocols. Installation instructions are provided for Ubuntu 20.04 with dependencies on PyTorch, CUDA, and other libraries. It covers how to run the digital human, including using RTMP servers, initiating the digital person, enabling sound cloning via services like gpt-sovits and xTTS, and how to utilize audio features using hubert. It also explains full-body video splicing, setting up background images, and using WebRTC for streaming. Docker container deployment is mentioned for easy setup. The data flow, model training files, performance analysis including frame rates and latency, and future enhancements like ChatGPT integration for dialogue are discussed. The document encourages community cooperation to enhance the project, with contact information for further engagement.",Exploring the Edge of AI: Real-Time Streaming Digital Humans with Ernerf Model,"Discover the breakthrough in AI technology with streaming digital humans based on the Ernerf model, achieving audio-video synchronous dialogue for commercial use. This cutting-edge development supports voice cloning, large model dialogue, and integrates advanced audio feature driving technologies like wav2vec and hubert. Experience the seamless interaction with full-body video splicing, optimized for RTMP and WebRTC platforms. A step forward in digital communication, showcasing the capabilities of digital replication and interactive technology.","Explore how streaming digital humans based on the Ernerf model are revolutionizing audio-video synchronized dialogues for commercial applications with voice cloning, full-body video splicing, and more.",AI Development Platform,"Python





        433





        102


        Built by

          









        22 stars today",https://raw.githubusercontent.com/lipku/metahuman-stream/main/assets/dataflow.png,,433,2023-12-19T01:32:46Z
2024-04-24,https://github.com/UKPLab/sentence-transformers,https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/README.md,"The Sentence Transformers framework offers an efficient way to compute dense vector representations for sentences, paragraphs, and images, using transformer networks like BERT and RoBERTa. It supports more than 100 languages and includes state-of-the-art pretrained models tailored for various use-cases, with the capability for users to fine-tune custom models for optimized performance in specific tasks. Installation is straightforward via pip or conda, with full documentation available online. The package also allows for the easy integration of GPU support for enhanced performance. Key applications include computing sentence embeddings, semantic textual similarity, clustering, paraphrase mining, and more, across different languages and even including image search and clustering. Developers can set up a project environment for contributions and are encouraged to cite relevant publications if the repository aids their research or project.",Harnessing the Power of BERT for Multilingual Embeddings with Sentence Transformers,"Discover how Sentence Transformers leverage BERT, RoBERTa, and XLM-RoBERTa for creating state-of-the-art sentence, paragraph, and image embeddings. This framework not only offers easy computation of dense vector representations but also provides over 100 languages support with fine-tuning capabilities for custom embeddings. Dive into our documentation and start integrating cutting-edge pretrained models or fine-tuning your specific model to enhance your application's language understanding and representation abilities today.","Explore Sentence Transformers: a powerful framework for computing dense vector representations of sentences, paragraphs, and images across 100+ languages using transformer networks like BERT. Perfect for enhancing applications with state-of-the-art language models.",Natural Language Processing,"Python





        13,772





        2,328


        Built by

          









        19 stars today",,,13772,2019-07-24T10:53:51Z
2024-04-24,https://github.com/g1879/DrissionPage,https://raw.githubusercontent.com/g1879/DrissionPage/master/README.md,"DrissionPage is a Python-based web automation tool allowing for efficient web page control and data packet management by merging browser automation's convenience with the high efficiency of HTTP requests. This combination ensures both fast writing and execution capabilities, significantly enhancing development and operational efficiency. Unique for its fully self-developed core, DrissionPage eliminates the need for webdrivers and different browser version drivers, supports working across iframes, and can operate on multiple tabs simultaneously without active window requirements. The tool boasts of improved speed, simplified element location, auto-retry functions for unstable networks, and powerful download capabilities. DrissionPage's minimalist syntax and integration of common functions make coding elegant and development friendlier for beginners. It further enhances the user experience with built-in configurations, the lxml parsing engine for faster processing, and the Page Object Model (POM) for easy testing and expansion. The project, regularly updated on Gitee, underscores the importance of legal and ethical usage, urging users to respect legal regulations and moral constraints.",DrissionPage: Revolutionizing Web Automation with Python,"Discover how DrissionPage combines the power of browser automation and data request efficacy in one Python library. With a self-developed core, DrissionPage offers seamless navigation across iframes, multiple tab operations without switching, and enhanced performance without Selenium's limitations. Its user-friendly syntax and powerful capabilities make web page automation accessible to novices and efficient for experts. Experience elegant coding with minimal effort and maximum efficiency.","Explore DrissionPage, the Python-based tool transforming web page automation by blending browser control and data requests. Learn how its unique features, like iframe navigation and Selenium-free operation, simplify web automation.",AI Browser Automation,"Python





        5,095





        520


        Built by

          









        27 stars today",https://gitee.com/g1879/DrissionPageMD/raw/master/static/img/code2.jpg,,5095,2020-05-21T09:43:59Z
2024-04-24,https://github.com/yogeshojha/rengine,https://raw.githubusercontent.com/yogeshojha/rengine/master/README.md,"This text introduces reNgine 2.0-Jasper, a major upgrade to the reNgine web application reconnaissance suite designed for security professionals, penetration testers, and bug bounty hunters. reNgine is lauded for simplifying the reconnaissance process with its highly configurable engines, efficient data correlation, continuous monitoring, an intuitive user interface, and database integration. This release marks significant contributions from [@ocervell], enhancing reNgine's capabilities with new features and refactoring. reNgine stands out for its ability to automate and improve information-gathering efforts, surpassing traditional tools and some commercial offerings in efficiency and configurability. The suite boasts advanced reconnaissance features like subdomain discovery, endpoint detection, vulnerability scanning, and OSINT capabilities, integrated with database support for refined data management and analysis. reNgine 2.0 introduces novel functionalities such as GPT-powered vulnerability and attack surface analysis, project management with role-based access control, and a streamlined process for report generation. It is open-source, with detailed documentation available, and supports a community-driven approach to continuous development and bug bounty initiatives.",Revolutionize Web App Reconnaissance with reNgine 2.0-jasper: A Comprehensive Guide,"Discover the future of web application reconnaissance with reNgine 2.0-jasper, a game-changer for security professionals, penetration testers, and bug bounty hunters. Designed to streamline and enhance the reconnaissance process, reNgine boasts configurable engines, data correlation capabilities, and an intuitive user interface. From addressing traditional tools' limitations to surpassing commercial tools' capabilities, reNgine is the comprehensive solution for automating information-gathering efforts. The release of reNgine 2.0-jasper, thanks to significant contributions from [@ocervell](https://github.com/ocervell), marks a new era in sophisticated, efficient web app reconnaissance.","Explore reNgine 2.0-jasper, the latest breakthrough in web application reconnaissance technology, offering enhanced automation and efficiency for security experts. Learn how it redefines information gathering in cybersecurity.",Cybersecurity Tool,"Python





        6,723





        1,045


        Built by

          









        20 stars today",https://raw.githubusercontent.com/yogeshojha/rengine/master/.github/screenshots/banner.gif; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/yogeshojha/rengine/master/.github/screenshots/scan_results.gif; https://user-images.githubusercontent.com/17223002/164993781-b6012995-522b-480a-a8bf-911193d35894.gif; https://user-images.githubusercontent.com/17223002/164993749-1ad343d6-8ce7-43d6-aee7-b3add0321da7.gif; https://user-images.githubusercontent.com/17223002/164993687-b63f3de8-e033-4ac0-808e-a2aa377d3cf8.gif; https://user-images.githubusercontent.com/17223002/164993689-c796c6cd-eb61-43f4-800d-08aba9740088.gif; https://user-images.githubusercontent.com/17223002/164993751-d687e88a-eb79-440f-9dc0-0ad006901620.gif; https://user-images.githubusercontent.com/17223002/164993670-466f6459-9499-498b-a9bd-526476d735a7.gif; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png; https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/aqua.png,https://www.youtube.com/watch?v=Xk_YH83IQgg; https://www.youtube.com/watch?v=Xk_YH83IQgg; https://www.youtube.com/watch?v=Xk_YH83IQgg; https://www.youtube.com/watch?v=7uvP6MaQOX0; https://www.youtube.com/watch?v=A1oNOIc0h5A; https://www.youtube.com/watch?v=VwkOWqiWW5g,6724,2020-05-03T12:13:12Z
2024-04-25,https://github.com/CyanVoxel/TagStudio,https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/README.md,"TagStudio, a document management system, is in the early stages of development and is designed with flexibility and user freedom in mind, avoiding proprietary formats and unnecessary file restructuring. It aims to provide a robust system for organizing and rediscovering files by utilizing tags and metadata effectively. The project, which is specifically verified to work on Windows, is open to contributors to shape its direction. It supports tag composition for detailed organization, allows for easy library creation and maintenance without moving files, and offers search functionality based on tags or metadata. The application serves as the initial implementation of this concept, with future developments focusing on improving usability, search capabilities, and metadata management. Features like handling unlinked entries and saving libraries are already in place, although some functionalities, like macros and image collages, are still under development. The project prioritizes privacy, portability, and an extensive feature set, aiming for a user-friendly interface and efficient file management across various platforms.",Introducing TagStudio: Revolutionize Your File Organization,"Discover TagStudio, the cutting-edge document management application designed for ultimate user flexibility and efficiency. A truly open-source project, it invites contributions to refine and enhance its functionalities from the get-go. With its innovative approach to tagging and metadata, TagStudio offers a clutter-free, highly customizable way to manage your documents and media files. Say goodbye to the chaos of conventional file systems and welcome a new era of organized, easily searchable, and aesthetically pleasing digital workspace.","Explore TagStudio - an open-source, flexible document management system for organizing files with ease. Join us in shaping its future, making file management more intuitive and efficient.",Document Conversion Tool,"Python





        710





        57


        Built by

          









        149 stars today",https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/github_header.png; https://raw.githubusercontent.com/CyanVoxel/TagStudio/main/screenshot.jpg,,710,2024-04-17T12:14:41Z
2024-04-25,https://github.com/iperov/DeepFaceLive,https://raw.githubusercontent.com/iperov/DeepFaceLive/master/README.md,"This document presents information about DeepFaceLive, a software used for face swapping and animation. It allows users to swap faces from webcams or video footage with various pre-trained face models, featuring notable examples like Keanu Reeves and fictional or non-existent persons for ethical reasons. The document highlights available public face models, emphasizing that while most characters are fictional, Keanu Reeves is an exception and he is acknowledged as ""breathtaking"". Users seeking higher quality or better face match are advised to train their own models using DeepFaceLab. Additionally, it introduces a Face Animator module that enables controlling a static face picture through video or real-time camera feed, although it notes the quality requires tuning for optimal results. System requirements include DirectX12 compatible graphics card, modern CPU, 4GB RAM, Windows 10, and instructions for use across Windows and Linux systems. The document also mentions community engagement and support options like Discord, forums, and donations, along with instructions on helping the project by sharing trained models or contributing financially.",Revolutionize Your Videos with DeepFaceLive: Ultimate Face Swapping Guide,"Discover DeepFaceLive, your go-to solution for real-time face swapping in videos and webcam streams. Utilize our range of public face models, including celebrities like Keanu Reeves, to create amusing or stunning visual content. Perfect your creations by training your own face models with DeepFaceLab for unparalleled quality and personalization. Dive into the world of virtual face manipulation for streaming, video calls, and more, while meeting system requirements for a seamless experience.",Learn how to use DeepFaceLive for real-time face swapping in videos and streams. Explore available models or train your own for personalized content creation. Discover the system requirements and tools for a seamless face swapping experience.,Deep Learning Tool,"Python





        22,131





        3,552


        Built by

          









        370 stars today",https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/deepfacelive_intro.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_onnx.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_directx.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_python.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Keanu_Reeves/Keanu_Reeves.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Irina_Arty/Irina_Arty.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Millie_Park/Millie_Park.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Rob_Doe/Rob_Doe.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Jesse_Stat/Jesse_Stat.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Bryan_Greynolds/Bryan_Greynolds.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Mr_Bean/Mr_Bean.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ewon_Spice/Ewon_Spice.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Natasha_Former/Natasha_Former.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Emily_Winston/Emily_Winston.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Ava_de_Addario/Ava_de_Addario.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Dilraba_Dilmurat/Dilraba_Dilmurat.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Matilda_Bobbie/Matilda_Bobbie.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Yohanna_Coralson/Yohanna_Coralson.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Amber_Song/Amber_Song.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Kim_Jarrey/Kim_Jarrey.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/David_Kovalniy/David_Kovalniy.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Jackie_Chan/Jackie_Chan.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Nicola_Badge/Nicola_Badge.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Joker/Joker.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Dean_Wiesel/Dean_Wiesel.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Silwan_Stillwone/Silwan_Stillwone.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Chrys/Tim_Chrys.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Zahar_Lupin/Zahar_Lupin.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tim_Norland/Tim_Norland.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Natalie_Fatman/Natalie_Fatman.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Liu_Lice/Liu_Lice.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Albica_Johns/Albica_Johns.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Meggie_Merkel/Meggie_Merkel.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/celebs/Tina_Shift/Tina_Shift.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/lukashenko.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/insight_faceswap_example.gif; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/face_animator_example.gif; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/Ng1C78Ceyxg_screenshot.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_barclay_stone.png; https://raw.githubusercontent.com/iperov/DeepFaceLive/master/doc/logo_exmo.png,https://www.youtube.com/watch?v=Ng1C78Ceyxg,22132,2020-12-15T12:19:22Z
2024-04-25,https://github.com/McGill-NLP/webllama,https://raw.githubusercontent.com/McGill-NLP/webllama/main/README.md,"WebLlama is a project focused on developing agents capable of browsing the web and interacting in dialogues, built on the Llama 3 model and fine-tuned for web navigation tasks. The initiative, spearheaded by McGill-NLP, has released its first model, Llama-3-8B-Web, which significantly outperforms GPT-4V in web navigation benchmarks. The model is designed to assist rather than replace users by providing a powerful tool for web-based tasks. It's trained on over 24K web interaction instances and is easy to integrate with existing web platforms like Playwright and BrowserGym. The project emphasizes the importance of rigorous evaluation through benchmarks such as WebLINX and plans to expand its dataset to train more generalized web agents. The code for fine-tuning and evaluation is publicly available, and the project encourages contributions from the community.",Introducing WebLlama: Revolutionizing Web Browsing with AI Agents,"Discover WebLlama, the cutting-edge project aimed at enhancing web navigation through AI-powered agents, developed using Meta Llama 3 technology. Our first model, Llama-3-8B-Web, outperforms GPT-4V by 18% in web navigation tasks. It's designed not only to automate web tasks but to create a more symbiotic relationship between users and the web, making browsing more efficient and intuitive. Available on GitHub and Huggingface, WebLlama represents a significant leap forward in the field of AI-driven web interaction.","Explore WebLlama, the forefront of web browsing technology, leveraging AI to navigate the web efficiently. Learn how Llama-3-8B-Web surpasses existing models in accuracy and utility.",AI Browser Automation,"Python





        568





        24


        Built by

          





        80 stars today",https://raw.githubusercontent.com/McGill-NLP/webllama/main/assets/WebLlamaLogo.png; https://raw.githubusercontent.com/McGill-NLP/webllama/main/assets/llama-3.jpg; https://raw.githubusercontent.com/McGill-NLP/webllama/main/assets/LlamaAndGPT.png; https://raw.githubusercontent.com/McGill-NLP/webllama/main/assets/LlamaAndGPTAndMindAct.png; https://raw.githubusercontent.com/McGill-NLP/webllama/main/assets/WebLINXTestSplits.png,,568,2024-04-19T15:48:09Z
2024-04-25,https://github.com/FareedKhan-dev/Detect-AI-text-Easily,https://raw.githubusercontent.com/FareedKhan-dev/Detect-AI-text-Easily/main/README.md,"The article discusses the challenge of recognizing AI-generated text, highlighting the importance of critical thinking skills to discern AI authorship. It notes that AI, like ChatGPT, often uses sophisticated language, citing an example from a youth science-writing competition where a phrase raised suspicions of AI use. The author demonstrates how certain words have become more prevalent since the introduction of ChatGPT by analyzing their frequency in a vast corpus of English texts and academic papers, observing significant upticks in usage. The increase in specific words, traditionally less common in human writing, suggests AI's growing influence on content creation. To assist in identifying AI-generated content, the author has developed a web app that analyzes texts for signs of AI involvement, based on a curated list of 100 indicative words. This tool aims to simplify the detection process, making it accessible even to those without professional expertise in linguistic analysis.",Unveiling AI-Generated Text: Techniques to Identify Non-Human Writing,"Discover the secret to spotting AI-generated content through specific word usage, as revealed in a recent analysis of ChatGPT and other language models. Learn how a simple phrase like 'Labyrinthian mazes' can betray AI authorship. Dive into the growing trend of words such as 'delve' in academic and online texts, indicating potential AI involvement. Harness tools and a comprehensive list of 100 AI-indicative words to discern AI-written material. Explore a user-friendly web app designed to swiftly detect AI-generated text, simplifying the identification process.",Learn how to easily identify AI-generated text using specific word patterns and a web app. This guide covers practical tips and tools for recognizing non-human writing in academic and online content.,AI Development Platform,"Python





        33





        6


        Built by

          





        8 stars today",https://cdn-images-1.medium.com/max/2448/1*fTV_vFjFWyhPSnWDauYOWw.png; https://www.english-corpora.org/now/) Corpus (by Fareed Khan)](https://cdn-images-1.medium.com/max/3856/1*Tv76vgfG7kOF5IRudR5EIg.png; https://www.english-corpora.org/now/) Corpus (by Fareed Khan)](https://cdn-images-1.medium.com/max/6512/1*EgrevS32vUy4eKx3F__oog.png; https://cdn-images-1.medium.com/max/7248/1*ypnIW51cEn7y5RqzJ0YrBw.png; https://www.kaggle.com/datasets/Cornell-University/arxiv) (by Fareed Khan)](https://cdn-images-1.medium.com/max/3856/1*Ri6_R6bLQJ6TVSmj6JXvVg.png; https://cdn-images-1.medium.com/max/4612/1*9R0i2dDcwrKXlZAqPNlvNw.png,,33,2024-04-17T13:03:38Z
2024-04-25,https://github.com/a1600012888/PhysDreamer,https://raw.githubusercontent.com/a1600012888/PhysDreamer/main/README.md,"PhysDreamer is a cutting-edge tool enabling physics-based interaction with 3D objects through video generation. To set it up, users must first install diff-gaussian-rasterization from its GitHub page. Then, they create and activate a new Python environment in conda, install required packages, and set up the software. Users need to download specific scenes and models optimized for velocity and material fields from Hugging Face and ensure these assets are correctly placed in designated folders. Running the inference involves navigating to the inference project directory and executing a provided script. The development of PhysDreamer incorporated source code from other projects, acknowledging their contributions. Moreover, an academic citation is provided for those referencing PhysDreamer in research, detailing its authors and publication information.",Exploring PhysDreamer: Revolutionizing 3D Object Interactions & Video Generation,"Discover how PhysDreamer, a cutting-edge tool, transforms the manipulation of 3D objects through physics-based video generation. Learn to set up the environment, download essential data, and run inferences for innovative video creation. This platform leverages algorithms from leading projects, offering an unparalleled experience in dynamic 3D modeling. Dive into the future of digital interaction with PhysDreamer â€“ where creativity meets advanced computational physics.","Unlock the potential of 3D object manipulation with PhysDreamer. This guide covers setup, data acquisition, and generating physics-based videos, opening new dimensions in digital creativity and interaction.",Video Generation Tool,"Python





        246





        10


        Built by

          





        26 stars today",https://raw.githubusercontent.com/a1600012888/PhysDreamer/main/figures/figure_teaser.png,,246,2024-04-16T22:36:27Z
2024-04-25,https://github.com/InternLM/lmdeploy,https://raw.githubusercontent.com/InternLM/lmdeploy/main/README.md,"LMDeploy is a toolkit designed for efficient compression, deployment, and service of large language models (LLMs), created by the MMRazor and MMDeploy teams. It introduces several optimized features for enhanced performance, including support for various quantization methods and a request distribution service for easy multi-model deployment across systems. Recent updates in 2024 include support for new models such as Llama3 and enhancements in TurboMind for rapid inference across devices, along with seamless integration options like OpenAOE. LMDeploy supports a wide range of models, offering up to 1.8 times faster request throughput compared to similar tools. It is available for installation via pip, supports CUDA 12 by default, and provides offline batch inference capabilities. Additional resources for users include detailed documentation, tutorials for getting started, user and advanced guides, and third-party project integrations. Contributions to LMDeploy are welcomed, acknowledging collaborations with projects like FasterTransformer and DeepSpeed-MII. The project is available under the Apache 2.0 license, supporting a broad developer and research community aiming at the efficient utilization of LLMs.",Maximize Your LLM Deployment with LMDeploy: A Comprehensive Guide,"Discover how LMDeploy revolutionizes LLM deployment with its latest updates for 2024, including support for Llama3, TurboMind's enhanced inference speeds, and comprehensive integration with OpenAOE. This toolkit, developed by the teams behind MMRazor and MMDeploy, offers efficiency, quantization, and ease of distribution for deploying large language models (LLMs). It's designed to improve request throughput significantly, enable effective weight-only and k/v quantization, and facilitate effortless multi-model services deployment.","Explore how LMDeploy enhances LLM deployment with features like efficient inference, effective quantization, and effortless distribution, making it easier to deploy and serve LLMs with improved performance and flexibility.",AI Deployment Platform,"Python





        2,301





        202


        Built by

          









        26 stars today",,,2301,2023-06-15T12:38:06Z
2024-04-25,https://github.com/coqui-ai/TTS,https://raw.githubusercontent.com/coqui-ai/TTS/main/README.md,"Coqui.ai has announced several updates to its Text-to-Speech (TTS) technology, including the release of TTSv2, which supports 16 languages and offers enhanced performance. The company has also released fine-tuning code for TTS with examples, introduced streaming capabilities with sub-200ms latency, and launched a production TTS model capable of speaking in 13 languages. Additionally, Bark is now available for inference with unconstrained voice cloning, and users can access around 1100 Fairseq models within Coqui's TTS framework. Support for Tortoise with faster inference, voice generation with prompts and fusion, and voice cloning features are now live on Coqui Studio. The library focuses on high-performance, deep learning-based models for text-to-speech tasks, offering tools for training, fine-tuning, dataset analysis, and utilities for model testing. It's tested on Ubuntu 18.04 with Python versions 3.9 to 3.12, providing detailed instructions for installation, Docker usage, and speech synthesis through both Python API and command line.",Revolutionizing Text-to-Speech: Coqui.ai Unleashes New Updates and Multilingual Support,"Coqui.ai's latest updates in Text-to-Speech (TTS) technology bring groundbreaking improvements and features, including TTSv2 with 16 languages, enhanced performance, and sub-200ms streaming latency. With fine-tuning code now available, users can customize voices with ease. Additionally, Coqui introduces 'Bark' for unconstrained voice cloning and supports an extensive range of Fairseq models. These advancements promise to transform voice applications, making digital interactions more natural and accessible.","Discover the latest updates from Coqui.ai, featuring TTSv2 with 16 languages, improved performance, sub-200ms latency, and Bark for voice cloning. Explore the new era of Text-to-Speech technology.",Deep Learning Tool,"Python





        29,224





        3,420


        Built by

          









        38 stars today",https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png; https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png,,29224,2020-05-20T15:45:28Z
2024-04-25,https://github.com/yisol/IDM-VTON,https://raw.githubusercontent.com/yisol/IDM-VTON/main/README.md,"IDM-VTON introduces a novel approach to enhance diffusion models for more authentic virtual try-ons in various environments, as detailed in their paper and its official implementation. Users are encouraged to participate and support the project through their GitHub and Hugging Face pages. The project emphasizes data preparation, requiring the VITON-HD and DressCode datasets, with specific instructions for organizing and utilizing this data. Inference processes for both datasets are outlined, featuring command-line and script options for executing the model. Acknowledgments cite support from ZeroGPU and contributions from related projects like OOTDiffusion and DCI-VTON. The implementation also acknowledges code borrowed from IP-Adapter. The authors of the research, Choi, Kwak, Lee, Choi, and Shin, have made their code available under the CC BY-NC-SA 4.0 license, inviting use and modification within the scope of these terms.",Revolutionizing Virtual Try-Ons: How IDM-VTON Enhances Diffusion Models,"Discover the groundbreaking IDM-VTON technology that transcends existing diffusion models for virtual try-on applications. Integrating advanced algorithms and datasets like VITON-HD and DressCode, this project offers a seamless try-on experience that's impressively realistic. Its widespread applicability in fashion technology demonstrates its potential to transform online shopping experiences. Learn how to utilize this technology for your projects with a comprehensive guide on setup, inference, and data preparation. Dive into the future of virtual fashion with IDM-VTON.","Explore the IDM-VTON project, a leap forward in virtual try-on technology utilizing cutting-edge diffusion models. Perfect for developers and fashion industry innovators looking to elevate online shopping experiences.",Image Generation Platform,"Python





        413





        44


        Built by

          





        87 stars today",https://raw.githubusercontent.com/yisol/IDM-VTON/main/assets/teaser2.png; https://raw.githubusercontent.com/yisol/IDM-VTON/main/assets/teaser.png,,413,2024-03-20T01:29:00Z
2024-04-25,https://github.com/tamilselvanarjun/pydatascraper,https://raw.githubusercontent.com/tamilselvanarjun/pydatascraper/main/README.md,"pydatascraper is a Python-based application designed for web scraping, specifically tailored for fetching Google and Yelp reviews, as well as extracting information from OpenStreetMap. This user-friendly tool comes with a GUI making it accessible for users to operate without in-depth programming knowledge. It supports various features including web scraping from provided URLs, retrieving Google and Yelp reviews using their respective APIs, and collecting data from OpenStreetMap by extracting geographical coordinates and other details. The software requires Python 3.x and several other Python packages which can be installed using `pip`. Users can easily set up pydatascraper by cloning the repository, installing the package, importing the module, and running the application, leading to a GUI for executing scraping tasks. It's adaptable for contributing to its development and is distributed under the MIT License, encouraging open collaboration.",Unlock Web Scraping Potential with PyDataScraper: Features and How-To Guide,"PyDataScraper is revolutionizing data extraction with its Python-based web scraping capabilities, allowing for easy retrieval of Google and Yelp reviews, and more. Through its intuitive GUI, users can scrape web pages, fetch business reviews, and extract valuable OpenStreetMap data effortlessly. Installation is straightforward, requiring Python 3.x and a simple pip command. Dive into the world of efficient data gathering, whether for competitive analysis or market research, with PyDataScraper. Its open-source nature invites contributions, promising continuous improvement and adaptability to users' evolving needs.","Discover how PyDataScraper leverages Python for effective web scraping, including Google and Yelp reviews extraction. Learn about its features, installation guide, and how you can contribute to its development.",AI Browser Automation,"Python





        127





        90


        Built by

          





        46 stars today",,,127,2024-03-03T06:28:07Z
2024-04-25,https://github.com/tamilselvanarjun/df2file,https://raw.githubusercontent.com/tamilselvanarjun/df2file/main/README.md,"The df2file project is a Python utility aimed at allowing efficient appending of Pandas DataFrames to a single Excel file, with each DataFrame placed in a separate sheet. It simplifies the process of adding DataFrames to an existing Excel file, offering customizable options for sheet naming, starting row, and more. The utility ensures the creation of a new Excel file if the specified one doesn't exist or appends to it otherwise. Installation is straightforward via pip, and it features commands for appending data both from scripts and the command line, with dependencies on pandas and openpyxl. Users are encouraged to contribute to its development by suggesting improvements or reporting issues. The tool is open-sourced under the MIT License.",Effortlessly Merge DataFrames into Excel with df2file: A Complete Guide,"Discover the power of df2file, a Python utility designed to streamline the process of appending Pandas DataFrames to a single Excel file, with each DataFrame in its own sheet. This tool not only simplifies data management by allowing for easy DataFrame appending and sheet customization but also addresses Excel file management efficiently. Whether you're creating a new Excel file or appending to an existing one, df2file has got you covered. Installation is straightforward with pip, making it accessible for Python developers looking to enhance their data workflow. Dive into the seamless integration of DataFrame to Excel conversion and elevate your data processing tasks today.","Learn how to easily append Pandas DataFrames to Excel with df2file, complete with sheet customization and efficient file management. Perfect for Python developers seeking streamlined data workflows.",Open Source Tool,"Python





        104





        92


        Built by

          





        50 stars today",,,104,2024-02-13T16:36:03Z
2024-04-25,https://github.com/tamilselvanarjun/knapsack_algorithm,https://raw.githubusercontent.com/tamilselvanarjun/knapsack_algorithm/main/README.md,"The knapsack_algorithm is a Python package designed to tackle the 0/1 knapsack problem efficiently using dynamic programming. It features robust error handling for input validation and a user-friendly interface that simplifies solving knapsack problems by inputting values, weights, and the maximum capacity. Installation is straightforward via pip. The package also comes with detailed documentation for further guidance and is available under the MIT License. Additionally, it encourages contributions and issue reporting from the community. An example demonstrates its ease of use by showing how to calculate the maximum value that can fit into a knapsack given specific item values, weights, and knapsack capacity.",Optimize Your Solutions with Knapsack Algorithm Python Package,"Discover how the knapsack_algorithm Python package offers an efficient solution to the 0/1 knapsack problem using dynamic programming. It features easy-to-use interfaces, comprehensive error handling, and simple installation via pip. Perfect for developers looking for a streamlined solution, knapsack_algorithm enhances problem-solving with its user-friendly approach. Dive into its documentation for detailed insights and contribute to further enhancements.","Explore the knapsack_algorithm Python package: A dynamic programming solution for the 0/1 knapsack problem with easy installation, error handling, and a user-friendly interface.",Python Libraries Collection,"Python





        100





        91


        Built by

          





        48 stars today",,,100,2024-02-16T07:56:50Z
2024-04-26,https://github.com/apple/corenet,https://raw.githubusercontent.com/apple/corenet/main/README.md,"CoreNet is a sophisticated toolkit designed for training deep neural networks in various tasks like classification, detection, and segmentation, alongside foundation models such as CLIP and LLM. Introduced in its version 0.1.0, CoreNet brings features like OpenELM and CatLIP, aiming at efficiency and speed in training. Apple's usage of CoreNet has resulted in several publications, indicating its utility in advancing research efforts. Installation guidelines are provided for different operating systems with prerequisites like Python 3.10+ for Linux and 3.9+ for macOS. The library is structured to facilitate easy access to training recipes, model implementations, and datasets, categorized by task to streamline the development process. Contributions to CoreNet are encouraged, with detailed documentation available for potential contributors. As an evolution of CVNets, CoreNet represents a move towards supporting a wider application scope, including foundational models. Key publications related to CoreNet underline its significance in the research community, making it a valuable resource for deep learning practitioners and researchers alike.",Introducing CoreNet: Revolutionizing Deep Neural Network Research and Development,"Discover CoreNet, Apple's latest deep neural network toolkit designed to streamline the training of both standard and novel models across various tasks. From foundational models to object detection, CoreNet sets the stage for cutting-edge AI research and applications. This toolkit not only facilitates easy access to Apple's research publications but also offers detailed guides for installation and usage. With its comprehensive library, CoreNet promises to be an invaluable resource for engineers and researchers looking to push the boundaries of neural network technology.","Explore CoreNet, Apple's advanced deep neural network toolkit for training and developing AI models. Access research, installation guides, and more for your neural network projects.",Deep Learning Tool,"Python





        2,894





        132


        Built by

          





        511 stars today",,,2894,2024-04-18T16:52:40Z
2024-04-26,https://github.com/Snowflake-Labs/snowflake-arctic,https://raw.githubusercontent.com/Snowflake-Labs/snowflake-arctic/main/README.md,"Snowflake introduces Snowflake Arctic, an enterprise-focused Large Language Model (LLM) distinguished by its cost-effective training and openness. Recognizing enterprise demands for conversational SQL data copilots, code copilots, and RAG chatbots, Arctic has been tailored to excel at SQL generation, coding, and complex instruction following, benchmarked against enterprise intelligence metrics. Arctic showcases efficiency by achieving parity or surpassing existing models like LLAMA on these metrics with significantly lesser compute resources. Featuring a unique Dense-MoE Hybrid transformer architecture with 480B total and 17B active parameters, Arctic demonstrates a competitive edge in training efficiency and overall performance. Licensed under Apache 2.0, Arctic ensures open access to its weights and code, supporting Snowflake's aim of fostering a collaborative research community. It is available for use through multiple inference API providers and can be directly accessed via Hugging Face. Additionally, Snowflake commits to continuous enhancements and community engagement through cookbooks, tutorials, and upcoming features aimed at advancing LLM training and inference efficiency.","Introducing Snowflake Arctic: Revolutionizing Enterprise AI with Efficient, Open LLMs","Snowflake Arctic emerges as a groundbreaking LLM tailored for enterprise AI, showcasing unparalleled efficiency in training and openness. It drastically outperforms competitors in enterprise metrics while consuming significantly less compute, thanks to its Dense-MoE Hybrid architecture. The Snowflake AI Research Team's dedication has led to a model that excels in SQL generation, coding, and complex instruction following, setting a new standard for cost-effective AI training. Arctic's commitment to openness is underscored by its Apache 2.0 license, ensuring unrestricted access to its innovative technology. This marks a pivotal moment for enterprises aiming to leverage AI, offering a blend of performance, efficiency, and accessibility unprecedented in the AI sphere.","Discover Snowflake Arctic, the latest LLM from Snowflake Labs designed for enterprise AI. Experience superior performance in SQL, coding, and more with reduced training costs, fully open-sourced for unparalleled access.",Large-scale Language Models,"Python





        306





        26


        Built by

          









        105 stars today",https://raw.githubusercontent.com/Snowflake-Labs/snowflake-arctic/main/assets/Training Efficiency Figure.png; https://raw.githubusercontent.com/Snowflake-Labs/snowflake-arctic/main/assets/Standard MoE vs Arctic.png,,306,
2024-04-26,https://github.com/microsoft/DeepSpeed,https://raw.githubusercontent.com/microsoft/DeepSpeed/master/README.md,"The text provides detailed information about DeepSpeed, a deep learning optimization software suite designed to enable the training and inference of dense or sparse models with extremely large numbers of parameters, promoting efficiency and scalability. It highlights several key news updates, including advancements in model training speed, support for large language models, and various enhancements in performance and feature capabilities across different technologies and frameworks. The software is part of Microsoft's AI at Scale initiative and is integrated with several popular deep learning frameworks, demonstrating a wide adoption in the AI community. DeepSpeed offers significant innovations through its training, inference, compression, and science-focused initiatives, addressing the challenges of training at scale, system throughput, low latency, and reduced costs for inference and compression. It is an open-source project under the Apache 2.0 license, available on PyPI for easy installation, and has a broad range of features tailored for extreme scale deep learning operations.",Exploring DeepSpeed: Revolutionizing AI Model Training & Inference,"DeepSpeed by Microsoft offers groundbreaking optimizations for deep learning, enabling training of models with trillions of parameters efficiently. It revolutionizes AI by providing extreme scale, speed, and cost efficiency in both training and inference phases. Innovations such as ZeRO, 3D-Parallelism, and Compression techniques push the boundaries of what's possible in AI, making it accessible to a wider range of developers and researchers.","Discover how Microsoft's DeepSpeed optimizes deep learning, enabling the training of gigantic AI models with unprecedented efficiency in terms of speed, scale, and cost. Learn about its key innovations like ZeRO, 3D-Parallelism, and more.",Deep Learning Platform,"Python





        32,706





        3,844


        Built by

          









        28 stars today",https://raw.githubusercontent.com/microsoft/DeepSpeed/master/docs/assets/images/DeepSpeed-pillars.png; https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg,https://www.youtube.com/watch?v=CaseqC45DNc; https://www.youtube.com/watch?v=y4_bCiAsIAk; https://www.youtube.com/watch?v=9V-ZbP92drg; https://www.youtube.com/watch?v=o1K-ZG9F6u0; https://www.youtube.com/watch?v=_NOk-mBwDYg; https://www.youtube.com/watch?v=sG6_c4VXLww; https://www.youtube.com/watch?v=k9yPkBTayos; https://www.youtube.com/watch?v=nsHu6vEgPew; https://www.youtube.com/watch?v=yBVXR8G8Bg8; https://www.youtube.com/watch?v=cntxC3g22oU; https://www.youtube.com/watch?v=pDGI668pNg0; https://www.youtube.com/watch?v=tC01FRB0M7w; https://www.youtube.com/watch?v=hc0u4avAkuM,0,
2024-04-26,https://github.com/ssili126/tv,https://raw.githubusercontent.com/ssili126/tv/main/README.md,"This text provides instructions on how to automatically obtain hotel multicast sources for TV broadcasting. There are four main methods outlined:

1. Input a provided URL into the TV live broadcasting software on a TV box.
2. For those who wish to fetch live TV addresses themselves, detailed steps are provided for both Windows PCs, including installing Chrome and necessary files, and for computers with Python installed, detailing necessary packages and scripts to run.
3. Installation and running instructions are provided for using Docker on OpenWRT or Synology NAS systems.
4. It's recommended to run these processes on a local computer rather than on GitHub due to data accuracy and speed test issues.
5. A directory for udp multicast sources is mentioned without detailing its contents or usage.",Effortlessly Access Live TV: Mastering Multicast Sources and Streaming,"Discover how to effortlessly access an abundance of live TV content through various methods including TV boxes, Windows PCs, Python-equipped computers, and Docker in openwrt or Synology NAS. From direct URL streaming to intricate setups with chromedriver and dedicated scripts, this guide offers step-by-step instructions for tech enthusiasts and cord-cutters. Whether you're looking to stream directly or delve into the technicalities of live TV access, find everything you need for a seamless setup.","Explore how to access live TV through multicast sources, including setups for TV boxes, Windows, Python, and Docker, with our comprehensive guide for seamless streaming.",Livestream Data Extraction,"Python





        505





        358


        Built by

          





        12 stars today",,,0,
2024-04-27,https://github.com/CrazyBoyM/phi3-Chinese,https://raw.githubusercontent.com/CrazyBoyM/phi3-Chinese/main/README.md,"The phi3-Chinese repository focuses on compiling various phi3 model variants scattered across the open-source community, aiming to expose lesser-known, intriguing model weights to a broader audience. It also provides simplified tutorials on training, inference, and deployment related to phi. The phi3 model, with less than half the size (3.8 billion parameters) of its counterparts, surpasses the performance of llama3's 8 billion parameter version according to Microsoft's benchmarks, increasing its feasibility for deployment on mobile platforms. The repository also lists download links for both Chinese and English versions of the phi3 model in different formats, including incremental SFT, direct DPO, and an upcoming expanded vocabulary version. However, the creator expresses disappointment in the model's performance compared to its benchmarks, noting issues like its small vocabulary size, especially for Chinese tokens, and suggests potential improvements, including vocabulary expansion and incremental pre-training, for better utility.",Exploring phi3-Chinese: Performance Insights and Deployment on Mobile,"phi3-Chinese emerges as a compact powerhouse, outperforming the llama3 8b version using less than half the size, enhancing deployability on smartphones. The phi3 repository aims to gather diverse training variants scattered across the open-source community, highlighting unique and interesting models. It also provides simple tutorials on phi-related training, inference, and deployment. Despite its advantages, users report discrepancies in performance and speed, indicating potential for improvement. The quest for an expanded vocabulary and incremental pre-training might address these concerns, hinting at vast potential for lightweight tasks.","Discover phi3-Chinese's significant performance over llama3 8b with less size, making it ideal for mobile deployment. Explore user insights, potential improvements, and how it paves the way for optimized lightweight applications.",Language Models,"Python





        98





        8


        Built by

          






        10 stars today",,,98,2024-04-23T17:44:45Z
2024-04-27,https://github.com/TagStudioDev/TagStudio,https://raw.githubusercontent.com/TagStudioDev/TagStudio/main/README.md,"TagStudio, currently in its preview/alpha stage, is a burgeoning photo and file organization tool developed as a personal project. Emphasizing user freedom and flexibility, it avoids proprietary formats and extensive file system modifications. Its creator aims for portability, privacy, extensibility, and rich feature inclusion, with hopes that the core concepts will endure beyond this particular implementation. Open-source from an early stage to involve contributors, it's candid about existing bugs and potential breaking changes.

TagStudio allows for creating libraries or vaults, adding various metadata to files, and supports rich tagging, including tag composition. Users can search for files based on tags, metadata, or filenames, with special conditions for untagged or empty entries. It's designed to accommodate a wide range of users and library sizes, looking to balance power with aesthetics.

Installation guidance is provided, primarily verified for Windows, with acknowledgment of potential issues on other platforms. Use involves creating or opening libraries, adding and editing metadata, managing unlinked files due to renaming or moving, and saving libraries. Despite its comprehensive features, certain functionalities like tag management viewing and macro applications are still in development or incomplete.

As an alpha version, TagStudio addresses the need for further testing, quality of life improvements, and features expansion, outlining both immediate priorities and future enhancements. The FAQ section clarifies the project's state, planned features, and encourages community contributions while adhering to a privacy-focused vision.",Introducing TagStudio: Revolutionize Your File Management Experience,"TagStudio, a user-centric document management application, is redefining file organization without the constraints of proprietary formats and complex file structures. Despite being in its early stages, this open-source project aims to empower users with flexible tagging and metadata options, ensuring easy file retrieval and management. With its commitment to privacy and extensibility, TagStudio is designed for a wide array of users, from individuals with large digital libraries to multi-user platforms. The project is actively seeking contributors to shape its future, embracing a community-driven approach to achieve its vision of a versatile and user-friendly file management system.","Discover TagStudio, an open-source, user-focused document management system offering unparalleled freedom and flexibility in organizing files. Join the project's journey towards creating an intuitive, feature-rich application.",Open Source Tool,"Python





        1,089





        149


        Built by

          









        186 stars today",https://raw.githubusercontent.com/TagStudioDev/TagStudio/main/github_header.png; https://raw.githubusercontent.com/TagStudioDev/TagStudio/main/screenshot.jpg,,1089,2024-04-17T12:14:41Z
2024-04-27,https://github.com/OWASP/OFFAT,https://raw.githubusercontent.com/OWASP/OFFAT/main/README.md,"OWASP OFFAT (OFFensive Api Tester) is a tool designed for automatically testing APIs for common vulnerabilities by generating tests from an openapi specification file. It enables automatic fuzzing of inputs and allows for the specification of user-provided inputs through a YAML config file. OFFAT includes a demo and focuses on various security checks, including those from the OWASP API Top 10 like SQLi, BOLA, and basic command injection, among others. Key features comprise automated and user-config-based testing, API integration, a CLI tool, and a Dockerized project setup. The tool, which is open source under the MIT License, can be installed via pip and run with specific commands as detailed in its documentation. It is important to note that even if a data leak is detected, an endpoint may still be marked as 'Success' in its results, indicating that the 'result' may not reflect the overall outcome of the test.",Exploring OWASP OFFAT: Automated API Security Testing,"Discover OWASP OFFAT, the groundbreaking tool designed for comprehensive API security testing, leveraging the OpenAPI specifications. It offers innovative features like automated fuzzing, user-config-based testing, and integration with other platforms through its API. With checks against the OWASP API Top 10, including SQL Injection and broken access control, it's an essential tool for securing your APIs. Ready to enhance your API security? Learn how to install and use OFFAT for robust testing.","Learn about OWASP OFFAT, an automated tool for testing API vulnerabilities against OWASP API Top 10, featuring automated fuzzing, user-config-based tests, and easy integration. Secure your APIs now.",Cybersecurity Tool,"Python





        230





        31


        Built by

          






        48 stars today",https://owasp.org/OFFAT/assets/images/tests/offat-v0.5.0.png,,230,2023-09-25T19:32:21Z
2024-04-27,https://github.com/dcharatan/flowmap,https://raw.githubusercontent.com/dcharatan/flowmap/main/README.md,"FlowMap is the official software implementation of the paper **""FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent""** authored by Cameron Smith, David Charatan, Ayush Tewari, and Vincent Sitzmann. The project emphasizes high-quality camera positioning, intrinsic parameters, and depth estimation using gradient descent. The software requires a Python virtual environment on Linux for installation, with specific requirements stated for setup and additional modules for pretraining. FlowMap is initialized using a checkpoint found online and is pre-trained with datasets like Real Estate 10k and CO3Dv2. It's evaluated using subsets from LLFF, Mip-NeRF 360, and Tanks & Temples datasets. The software also supports running ablations and generating figures and tables as presented in the paper. The creators acknowledge support from various research funds and organizations, noting the work's academic and developmental background.",Exploring FlowMap: Revolutionary Camera Poses and Depth Estimation,"Discover the groundbreaking technology of FlowMap, a method for high-quality camera poses, intrinsics, and depth via gradient descent, developed by a talented team of researchers. This official implementation unlocks new potentials in the realms of 3D mapping and virtual reality. By leveraging advanced datasets like Real Estate 10k and CO3Dv2, FlowMap significantly enhances video and image analysis. Explore the projectâ€™s versatility through pre-training, running code, and various ablations. Dive into the fascinating world of FlowMap and how it's setting new standards in imaging technology on its [official project website](https://cameronosmith.github.io/flowmap/).","FlowMap offers an innovative approach to achieving high-quality camera poses, intrinsics, and depth through gradient descent, enhancing 3D mapping and virtual realities. Explore the official implementation now.",Computer Vision,"Python





        317





        14


        Built by

          






        56 stars today",,,317,2024-04-22T22:10:49Z
2024-04-27,https://github.com/Flagsmith/flagsmith,https://raw.githubusercontent.com/Flagsmith/flagsmith/main/README.md,"Flagsmith is an open-source platform used for feature flagging and remote configuration management across web, mobile, and server-side applications. It allows developers to toggle features on or off for different environments or user segments without needing to deploy new code. This facilitates phased rollouts, A/B and multivariate testing, and obtaining early user feedback through beta programs. Flagsmith supports organization management and can be integrated with various tools. It offers a hosted SaaS option as well as an open-source version that can be deployed using Docker, Kubernetes, Redhat OpenShift, or directly to major cloud providers or on-premise. The platform includes a REST API and a web-based admin dashboard for managing features. Technical documentation and support are readily available for users.",Unlocking Next-Level Feature Management with Flagsmith,"Discover how Flagsmith, the open-source feature flag and remote config service, empowers developers to seamlessly manage features across applications. With easy setup, feature flags, remote configuration, A/B testing, and robust organization management, Flagsmith is the go-to solution for enhancing your software development lifecycle. Whether you prefer a hosted API, private cloud deployment, or on-premise solutions, Flagsmith offers flexibility and control over your feature management processes. Dive into the world of feature flags and optimize your development workflow with Flagsmith today.","Explore Flagsmith for advanced feature flag management, remote configuration, and A/B testing across your web, mobile, and server-side applications. Start with our open-source platform for better development flexibility.",Open Source Tool,"Python





        4,301





        320


        Built by

          









        38 stars today",https://raw.githubusercontent.com/Flagsmith/flagsmith/main/static-files/hero.png; https://raw.githubusercontent.com/Flagsmith/flagsmith/main/static-files/screenshot.png,,4301,2018-06-05T10:49:57Z
2024-04-28,https://github.com/pytorch/torchtitan,https://raw.githubusercontent.com/pytorch/torchtitan/main/README.md,"`torchtitan` is a PyTorch-based proof-of-concept designed for the large-scale training of language models (LLMs), showcasing cutting-edge distributed training features. It aims to complement, not replace, established codebases by encouraging the adoption of its showcased features. The project, still in its pre-release phase, emphasizes a clean and minimal codebase for ease of understanding and adaptability. Currently, it supports the training of Llama 2 and 3 models with features like FSDP2, Tensor Parallel, selective checkpointing, and performance metrics visualization through TensorBoard. Prepared for PyTorch ""nightly"" versions, `torchtitan` offers extensive documentation for its installation, feature utilization, and multi-node training setups. Despite its ambition, `torchtitan` does not aim to foster a large community but rather to influence existing frameworks through its innovative approach to model training. Key upcoming features include asynchronous checkpointing, FP8 support, and expanded parallel training methods. The code is available under the BSD 3 license, with dependencies on other legally governed content or services.",Exploring torchtitan: A Step Forward in Large-scale LLM Training with PyTorch,"torchtitan emerges as a groundbreaking proof-of-concept for large-scale LLM training, leveraging the powerful features of PyTorch in a minimal and understandable codebase. Designed for easy use, modification, and extension, it introduces minimal model code changes across various parallel computing techniques. This pre-release project complements existing LLM training frameworks, aspiring for quick feature adoption by industry giants. With pre-configured datasets, advanced training features, and promising pre-training capabilities with Llama models, torchtitan sets a new benchmark for distributed training efficiency and performance optimization.","Discover torchtitan, the latest pre-release project for large-scale LLM training using PyTorch's distributed features. A minimal codebase designed for efficiency, easy understanding, and quick start with advanced training features.",Deep Learning Tool,"Python





        632





        23


        Built by

          









        81 stars today",https://raw.githubusercontent.com/pytorch/torchtitan/main/assets/images/titan_play_video.png,https://www.youtube.com/watch?v=ee5DOEqD35I,632,2023-12-13T01:51:37Z
2024-04-28,https://github.com/plaintextpackets/netprobe_lite,https://raw.githubusercontent.com/plaintextpackets/netprobe_lite/master/README.md,"Netprobe Lite is a straightforward tool designed for home users to measure ISP performance, including metrics like packet loss, latency, jitter, and DNS performance. These metrics are combined into a common score to monitor internet connection health. It requires a Docker-capable PC connected directly to the ISP router, with an installation process that includes cloning a git repository and using docker compose commands to start or stop the app. Users can access the tool's interface through a web browser, login with default credentials, and are encouraged to change them. The tool offers customization options for the port and DNS testing, and data persistency management. It can be set to run on startup for continuous monitoring. Netprobe Lite is intended for non-commercial use under a custom license, prohibiting commercial use without authorization. For commercial inquiries, contact is provided.",Maximize Your Home Internet Performance with Netprobe Lite,"Discover how Netprobe Lite, a simple yet powerful tool, can help you measure and improve your ISP's performance right from your home. This tool offers detailed insights by measuring packet loss, latency, jitter, and DNS performance, providing a comprehensive score to monitor your internet connection's health. With an easy setup that requires a Docker-compatible PC connected to your ISP router, Netprobe Lite is accessible for anyone seeking to optimize their internet service. Learn how to install, setup, and utilize Netprobe Lite to ensure your internet connection is running at its best.","Learn how to use Netprobe Lite to measure and improve your ISP performance. Perfect for home use, this tool provides critical insights into internet health. Easy setup and detailed metrics analysis.",Network Performance Monitor,"Python





        172





        20


        Built by

          







        20 stars today",,,172,2023-11-13T04:31:39Z
2024-04-28,https://github.com/stitionai/devika,https://raw.githubusercontent.com/stitionai/devika/main/README.md,"Devika is an early-stage AI software engineer project designed to understand high-level human instructions, research, and write code. It uses large language models and web browsing for intelligent software development, aiming to work with minimal human guidance. Despite it being in the experimental phase with unimplemented/broken features, contributions are encouraged. Devika supports models like Claude 3, GPT-4, and others, offering features like AI planning, code writing in various languages, and a chat interface. Installation requires Python, NodeJs, and other tools, with detailed steps provided. Users can create projects, provide instructions, and monitor progress through a web interface. Configurations for API keys like Bing and OpenAI are needed for full functionality. Contributions to enhance Devika are welcome, with support offered through issue tracking and a Discord server. Devika is open-source under the MIT License.",Introducing Devika: Revolutionize Your Coding with AI Software Engineering,"Meet Devika, the advanced AI software engineer designed to transform the software development landscape. Leveraging large language models and sophisticated planning algorithms, Devika can understand complex instructions, conduct research, and write code with minimal human oversight. Aimed at being an open-source counterpart to Devin, it promises to enhance coding efficiency and innovation. With its support for multiple AI models and programming languages, Devika is not just a tool but a revolution in AI-assisted coding.","Discover Devika, the AI-powered software engineer changing how we approach coding. With advanced AI, planning capabilities, and support for multiple languages, see how it simplifies and accelerates development.",AI Development Platform,"Python





        16,469





        2,099


        Built by

          









        57 stars today",https://raw.githubusercontent.com/stitionai/devika/main/.assets/devika-avatar.png; https://raw.githubusercontent.com/stitionai/devika/main/.assets/devika-screenshot.png,,16469,2024-03-21T03:03:21Z
2024-04-28,https://github.com/mbzuai-oryx/LLaVA-pp,https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/README.md,"LLaVA++ is a project extending the capabilities of the LLaVA 1.5 model by integrating the latest LLMs, Phi-3 Mini Instruct 3.8B, and LLaMA-3 Instruct 8B. Released by researchers at Mohamed bin Zayed University of AI, it aims to enhance visual and instruction-following abilities in LLMs. New updates include online demos, fully fine-tuned models, and Google Colab access for interactive experiments. The model zoo provides an array of pre-trained and fine-tuned models available on Hugging Face. Installation and training instructions ensure users can effectively employ these models. Acknowledgements extend to contributing projects and open-source communities. For issues or questions, contact details are provided, and contributions are recognized in the citation section.",Revolutionizing Visual AI: LLaVA++ Unveils Enhanced LLaMA-3 and Phi-3 Integration,"Discover the groundbreaking LLaVA++: an advanced integration of LLaMA-3 and Phi-3, setting new benchmarks in visual AI capabilities. This innovative upgrade introduces cutting-edge models for exceptional visual and instructional performance, showcased in comprehensive benchmarks and an accessible online demo. Explore our detailed model zoo for pre-trained and fine-tuned versions, enhancing AI research and development. Dive into the future of visual AI with LLaVA++, a blend of precision, efficiency, and accessibility.","Explore LLaVA++, the latest advancement integrating LLaMA-3 and Phi-3 for unprecedented visual AI capabilities. See benchmarks, access downloadable models, and try the live demo.",Multimodal AI Model,"Python





        364





        18


        Built by

          






        118 stars today",https://i.imgur.com/waxVImv.png; https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/logos/face.png; https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/lava++_radar_plot.png; https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/LLaVA-pp-results.png; https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/logos/IVAL_logo.png; https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/logos/Oryx_logo.png; https://raw.githubusercontent.com/mbzuai-oryx/LLaVA-pp/main/images/logos/MBZUAI_logo.png,,364,2024-04-26T02:52:07Z
2024-04-28,https://github.com/sarperavci/GoogleRecaptchaBypass,https://raw.githubusercontent.com/sarperavci/GoogleRecaptchaBypass/main/README.md,"This text introduces a Python script that can bypass Google's reCAPTCHA in less than 5 seconds using the DrissionPage library, with plans to add Selenium support. Capsolver, an AI-powered service specializing in solving various captcha types, sponsors the solution. Capsolver supports a wide range of captchas and offers API integration for developers, browser extensions for Chrome and Firefox, and various pricing packages. Instructions for installing necessary dependencies and ffmpeg are provided, along with a usage example and a demonstration file named `test.py`. The method exploits audio captcha as a more programmatically solvable option compared to image captchas, with a caution about potential IP blocking by Google for excessive use. Future updates include implementing Selenium, improving exception handling, and a visual demonstrating the script's efficacy.",Bypass Google reCAPTCHA in Seconds: Your Ultimate Guide,"Discover how to bypass Google reCAPTCHA quickly and effectively with our Python script. Using the DrissionPage library, solve reCAPTCHAs in under 5 seconds, enhancing your project's efficiency. The script, suitable for developers, simplifies CAPTCHA solving with potential Selenium enhancements on the horizon. Overcome Google's bot restrictions with ease, but remember to use this power responsibly to avoid IP bans.","Learn to bypass Google reCAPTCHA effortlessly with our Python script, featuring easy integration, fast solving with the DrissionPage library, and upcoming Selenium support.",Cybersecurity Tool,"Python





        168





        19


        Built by

          





        65 stars today",https://raw.githubusercontent.com/sarperavci/GoogleRecaptchaBypass/main/docs/capsolver.jpg; https://raw.githubusercontent.com/sarperavci/GoogleRecaptchaBypass/main/docs/demo.gif,,168,2024-04-24T18:39:25Z
2024-04-28,https://github.com/Visualize-ML/Book4_Power-of-Matrix,https://raw.githubusercontent.com/Visualize-ML/Book4_Power-of-Matrix/main/README.md,"The text provides links to discounted (50% off) access for three books: ""Simplifying Statistics,"" ""Elements of Mathematics,"" and ""The Power of Matrices"" available on a platform, possibly Zhihu. These discounts appear to be open-ended, suggesting they do not expire. Additionally, there's an incentive mentioned where individuals who frequently correct errors or provide feedback might receive free books as a token of appreciation. The message emphasizes that access to these resources is aimed at personal improvement and is open source, implying they are freely accessible or shareable under certain conditions.",Unlock the Power of Math: Exclusive Discounted Access to Premier Math Resources,"Dive into the world of mathematics with our half-price offers on essential resources: 'Statistics Simplified', 'Fundamentals of Mathematics', and 'The Power of Matrices'. Gain lifetime access to open-source content designed to elevate your understanding and expertise in mathematics. Plus, the opportunity to receive a complimentary book for contributing corrections as our way of saying thank you.","Grab this opportunity to master mathematics with half-off premier resources. Lifetime access to high-quality, open-source mathematical content and a chance to receive a free book for helpful feedback.",Data Science Resources,"Python





        7,638





        1,119


        Built by

          





        13 stars today",,,7638,2022-07-02T19:58:34Z
2024-04-29,https://github.com/autonomousvision/gaussian-opacity-fields,https://raw.githubusercontent.com/autonomousvision/gaussian-opacity-fields/main/README.md,"Gaussian Opacity Fields (GOF) present an efficient method for geometric extraction and surface reconstruction in unbounded scenes, leveraging the identification of a level set with 3D Gaussians and enhancing surface quality through regularization. The process involves compact and adaptive mesh extraction using Marching Tetrahedra. Installation instructions and dataset acquisition details are provided, encompassing Mip-NeRF 360, NeRF-Synthetic, DTU, and Tanks and Temples datasets. Instructions for training and evaluation on these datasets, as well as on custom datasets with potential modifications for decoupled appearance modeling, are outlined. The project acknowledges contributions from various sources, including 3DGS, Mip-Splatting, and others, for their foundational technologies and evaluation tools. Citations are included for those whose work significantly contributed to this project, with specific requests to credit their code or findings where applicable.",Exploring Gaussian Opacity Fields: Revolutionizing 3D Surface Reconstruction,"Gaussian Opacity Fields (GOF) mark a significant advancement in 3D surface reconstruction, offering a method to directly extract geometry with 3D Gaussians. This technique enhances surface reconstruction through effective regularization and leverages Marching Tetrahedra for creating compact, adaptive meshes. The approach, outlined by Zehao Yu, Torsten Sattler, and Andreas Geiger, presents a scalable solution for unbounded scenes, promising substantial improvements over traditional methods. Its application spans various fields, including virtual reality, gaming, and architectural design, showcasing the potential to redefine how we interact with digital 3D environments.","Discover how Gaussian Opacity Fields innovate 3D surface reconstruction, offering efficient, high-quality, and compact solutions for digital environments. Learn about its groundbreaking approach.",3D Image Rendering,"Python





        337





        7


        Built by

          





        11 stars today",https://raw.githubusercontent.com/autonomousvision/gaussian-opacity-fields/main/./media/teaser_gof.png,,337,2024-04-16T10:17:21Z
2024-04-29,https://github.com/janeczku/calibre-web,https://raw.githubusercontent.com/janeczku/calibre-web/master/README.md,"The maintainer of Calibre-Web, after six years of dedicated programming, has decided to take a break due to feeling overwhelmed by the workload and community pressure. As a result, they have temporarily turned off all GitHub/Discord notifications to focus on developing the project according to their vision, warning that responses to issues and PRs will be slower. Calibre-Web itself is a comprehensive web app built upon a Calibre database that enhances eBook management and accessibility, supporting multiple languages and offering features like eBook reading, metadata editing, and user management. Installation can be executed via pip, and detailed guides for setup and configuration are available. The program is open-source under the GPL v3 License and acknowledges contributions from its community, implying a collaborative improvement and extension over time. For further details and support, users are encouraged to refer to the project's GitHub wiki or join the Discord community.",Taking a Break: A Calibre-Web Maintainer's Journey towards Rediscovery,"After 6 years of dedication to Calibre-Web, its maintainer declares a need for a hiatus, turning off all notifications to focus on development without disturbances. The commitment to this open-source ebook management web app, evolving from a hobby to feeling like a demanding job, prompts this decision. While the maintainer will still peek at issues and PRs, the Calibre-Web community is advised to temper expectations for immediate responses. This period aims to refocus and rejuvenate the maintainer's passion for enhancing Calibre-Web's features and user experience.",Discover the reasons behind the Calibre-Web maintainer's decision to take a break from intense programming and how it aims to improve the project's future development. Read more.,Open Source Tool,"Python





        11,568





        1,236


        Built by

          









        9 stars today",https://github.com/janeczku/calibre-web/wiki/images/main_screen.png,,11568,2015-08-02T19:01:42Z
2024-04-30,https://github.com/zk-Call/zkp-hmac-communication-python,https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/README.md,"The text introduces **zk-Call & Labs**, focusing on the implementation of ""Zero-Knowledge"" Proof with HMAC (Hash-Based Message Authentication Code) communication in Python, specifically aimed at enhancing security in messaging applications through state-of-the-art cryptographic methods. It delves into Schnorr's Protocol, elliptic curve cryptography, and the principle of ""Zero-Knowledge"" Proofs (ZKPs) â€” where a prover can prove the truth of a statement without conveying any additional information. The project combines ZKPs with HMAC to validate text-based secrets securely, providing a robust authentication system that ensures data integrity and authenticity without compromising privacy. The core of the system lies in its Python API, which simplifies the creation, verification, and management of cryptographic proofs, including detailed examples of usage in client-server communication scenarios. These examples demonstrate the practical application of these cryptographic techniques in ensuring secure message exchange and authentication in digital communication systems.",Implementing Zero-Knowledge Proofs and HMAC in Python: A Step-by-Step Guide,"Discover how to enhance your Python applications' security with Zero-Knowledge Proofs and HMAC for message authentication. This post introduces the core concepts of 'Zero-Knowledge' proofs and HMAC, illustrating their application within Python environments. Learn to implement these cryptographic techniques to secure communication without compromising privacy.",Unveil the power of Zero-Knowledge Proofs and HMAC in Python for securing applications. This guide offers insights into implementing cryptographic protocols for enhanced message security and authentication without revealing sensitive information.,Cybersecurity Tool,"Python





        1,864





        728


        Built by

          








        102 stars today",https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/zk-Call Preview [Python].png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/Schnorr's%20Protocol.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/Elliptic%20Curve.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/Purpose-1.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/ZKP-HMAC-1.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/ZKP-HMAC-2.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/ZKP-HMAC-3.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/ZKP-HMAC-4.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/HMAC.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/Synergistic%20Operation.png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/Core%20Components%20(Python).png; https://raw.githubusercontent.com/zk-Call/zkp-hmac-communication-python/main/assets/ZeroKnowledge%20(Python).png,,1864,2023-10-06T01:27:58Z
2024-04-30,https://github.com/JackAILab/ConsistentID,https://raw.githubusercontent.com/JackAILab/ConsistentID/main/README.md,"ConsistentID is a project focusing on portrait generation that emphasizes maintaining identity fidelity while enabling diversity and text controllability. It incorporates FaceParsing and FaceID information into the Diffusion model, allowing for quick customization without extra training. It can integrate with other models using LoRA modules, enhancing the communityâ€™s collaborative possibilities. A major goal is releasing its training and evaluation code and improving its model with more data for better aesthetics and generalization. The project introduces a batch of 50,000 multimodal fine-grained ID datasets and a unified measurement benchmark, FGIS, for fine-grained identity preservation. Through extensive experiments, ConsistentID demonstrated state-of-the-art performance in facial personalization, highlighting its ability to improve identity consistency and modify facial features with detailed prompts. The project calls for community support to advance its development and emphasizes responsible usage. Installation and usage instructions, along with acknowledgments and a citation request, are provided.",Revolutionizing Portrait Generation: ConsistentID Unveiled,"ConsistentID marks a breakthrough in AI-driven portrait generation, promising unparalleled ID fidelity without compromising diversity or text controllability. By integrating FaceParsing and FaceID into the diffusion model, it offers rapid, highly customizable portraits in seconds. This technology not only sets new standards for facial personalization but also serves as an adaptable tool for enhancing base models with LoRA modules, fostering innovation in the community. With its state-of-the-art performance verified through extensive testing, ConsistentID opens new avenues for research in fine-grained facial personalization.","Discover ConsistentID, the latest in AI portrait generation that maintains high ID fidelity with dynamic text control, offering customizability and collaboration with base models. Learn how it's changing the face of digital identity.",Image Generation Platform,"Python





        255





        26


        Built by

          






        26 stars today",,,255,2024-04-15T08:33:46Z
2024-04-30,https://github.com/ultralytics/ultralytics,https://raw.githubusercontent.com/ultralytics/ultralytics/main/README.md,"Ultralytics presents YOLOv8, an advanced object detection model built on the success of previous YOLO versions, featuring enhancements for improved performance and flexibility. Designed for speed, accuracy, and ease of use, it's suitable for various tasks, including object detection and tracking, instance segmentation, image classification, and pose estimation. Resources and detailed documentation are available to maximize the model's potential. YOLOv8 supports multiple languages, offers comprehensive support through GitHub issues and Discord, and requires an Enterprise License for commercial use. It also provides pretrained models on COCO and ImageNet datasets, covering detection, segmentation, pose, and classification tasks. Interactive notebooks with tutorials for training, validation, and more are accessible, alongside integrations with top AI platforms like Roboflow and Comet for enhanced AI workflows. Ultralytics invites community contributions and offers different licensing options: an open-source AGPL-3.0 License and an Enterprise License for commercial applications.",Exploring the Advanced Capabilities of YOLOv8: A Comprehensive Guide,"YOLOv8 revolutionizes object detection and tracking, posing estimation, and image classification with its state-of-the-art features and improvements. Built on the success of previous YOLO versions, YOLOv8 combines speed, accuracy, and ease of use for versatile applications. Resources, documentation, and support options are readily available to ensure users can fully leverage YOLOv8's capabilities. Enterprises can leverage YOLOv8's advancements by securing an enterprise license. YOLOv8's performance is visually summarized, showcasing its superiority in object detection tasks.","Discover the advancements YOLOv8 brings to object detection, tracking, and more. Learn how its features, performance, and resources make it a top choice for practical applications.",Deep Learning Tool,"Python





        22,985





        4,582


        Built by

          









        48 stars today",https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png; https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/yolo-comparison-plots.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-social-youtube-rect.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-social-youtube-rect.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-social-youtube-rect.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-social-youtube-rect.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-social-youtube-rect.png; https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-social-youtube-rect.png; https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png; https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png; https://github.com/ultralytics/assets/raw/main/partners/logo-roboflow.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/partners/logo-clearml.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png; https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png; https://github.com/ultralytics/assets/raw/main/im/image-contributors.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png; https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png; https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png,https://www.youtube.com/watch?v=j8uQc0qB91s; https://www.youtube.com/watch?v=lveF9iCMIzc; https://www.youtube.com/watch?v=hHyHmOtmEgs; https://www.youtube.com/watch?v=Ag2e-5_NpS0; https://www.youtube.com/watch?v=4ezde5-nZZw; https://www.youtube.com/watch?v=3VryynorQeo,22985,2022-09-11T16:39:45Z
2024-04-30,https://github.com/microsoft/FILM,https://raw.githubusercontent.com/microsoft/FILM/main/README.md,"The official repository for ""Make Your LLM Fully Utilize the Context"" introduces FILM-7B, a 32K-context Large Language Model (LLM) designed to address the ""lost-in-the-middle"" issue. FILM-7B is an advancement developed from Mistral-7B-Instruct-v0.2, employing Information-Intensive (In2) Training methodologies. This model excels in probing tasks with near-perfect accuracy, showcases state-of-the-art (SOTA) results in tasks requiring extensive context understanding, and maintains efficiency in shorter contexts, amongst models of a similar ~7 billion size. The repo provides setup instructions using Conda or Pytorch Docker, usage guidelines, and comprehensive guides to reproducing probing, real-world long-context, and short-context task results. Contributions are encouraged, subject to a Contributor License Agreement (CLA) and adherence to the Microsoft Open Source Code of Conduct.",Unlocking Long-Context Understanding with FILM-7B: The Next-Level LLM,"Discover how FILM-7B, a state-of-the-art language model, is revolutionizing long-context understanding without compromising short-context performance. Trained using Information-Intensive Training, FILM-7B sets new benchmarks in near-perfect probing tasks and real-world applications among 7B-sized LLMs. Dive into the official repo for insights, setup instructions, and how to fully utilize FILM-7B's capabilities for your projects. Explore this groundbreaking model's journey to overcoming the lost-in-the-middle problem, offering unparalleled depth in context comprehension.","Explore FILM-7B, a cutting-edge 32K-context LLM, trained for exceptional long-context task performance alongside maintaining short-context accuracy. Learn how it surpasses existing benchmarks, offering a deep dive into the future of LLMs.",Large-scale Language Models,"Python





        160





        8


        Built by

          







        29 stars today",https://raw.githubusercontent.com/microsoft/FILM/main/./figures/probing_results_new.png; https://raw.githubusercontent.com/microsoft/FILM/main/./figures/real_world_long.png; https://raw.githubusercontent.com/microsoft/FILM/main/./figures/short.png,,160,2024-04-03T05:30:42Z
2024-04-30,https://github.com/VinciGit00/Scrapegraph-ai,https://raw.githubusercontent.com/VinciGit00/Scrapegraph-ai/main/README.md,"ScrapeGraphAI is a Python library designed to simplify web scraping by leveraging Large Language Models (LLMs) and direct graph logic. It facilitates the creation of scraping pipelines for websites, documents, and XML files, requiring users to specify only the information they wish to extract. The library supports integration with various LLMs such as Ollama and GPT-3.5, as well as other tools like Docker for easy deployment. ScrapeGraphAI is compatible with Playwright for scraping JavaScript-heavy websites and offers a quick installation via pip. It comes with a comprehensive documentation and a demo for users to try out its features. The project encourages community contributions and is available under the MIT License.",ScrapeGraphAI: Revolutionize Web Scraping with AI-Led Technologies,"ScrapeGraphAI leverages the power of LLM and graph logic to simplify web scraping, offering seamless pipelines for websites, documents, and XML files. Designed for both beginners and advanced users, it promises efficiency and accuracy. Install it through pip and explore its capabilities via demo projects or detailed documentation. Whether for academic research or project development, ScrapeGraphAI stands as a pioneering tool, transforming data extraction with AI innovation.","Discover ScrapeGraphAI, a cutting-edge Python library that revolutionizes web scraping using large language models and direct graph logic. Perfect for extracting information seamlessly from various sources.",Web Scraping Tool,"Python





        947





        69


        Built by

          









        176 stars today",https://raw.githubusercontent.com/VinciGit00/Scrapegraph-ai/main/docs/assets/scrapegraphai_logo.png; https://raw.githubusercontent.com/VinciGit00/Scrapegraph-ai/main/docs/assets/logo_authors.png,,947,2024-01-27T16:54:38Z
2024-04-30,https://github.com/xlang-ai/OpenAgents,https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/README.md,"OpenAgents establishes an inclusive platform for deploying and interacting with language agents in real-world settings, aiming to bridge the gap between proof-of-concept tools and user-accessible applications. Currently housing three agents (Data, Plugins, and Web agents), OpenAgents presents a free-to-use demo showcasing its capability to perform complex data analysis, integrate with over 200 daily-use tools, and autonomously navigate web pages. This platform distinguishes itself by its openness to code, ease of deployment, and a comprehensive suite tailored for both general users and developers. It emphasizes a chat-based web UI for user interaction, streamlined response handling for common failures, and smooth deployment on local setups. Furthermore, it encourages community contributions by providing a detailed guide for enhancing the platform, extending to new agents, or incorporating additional tools and models. The project acknowledges contributions from various individuals and expresses gratitude towards tech communities and research funds supporting open-source development.",Exploring OpenAgents: Revolutionizing Language Agents for Everyday Use,"OpenAgents introduces an innovative platform designed to host and use language agents effectively in daily scenarios, offering a unique blend of three specially crafted agents: Data, Plugins, and Web Agents. These agents are crafted to simplify tasks ranging from data analysis, accessing numerous daily tools, to autonomous web browsing. Beyond mere functionality, OpenAgents emphasizes easy deployment, full-stack solutions, and an interactive chat web UI, making it an accessible solution for both developers and general users. This platform not only bridges the gap between advanced language agent capabilities and user-friendly interfaces but also encourages community contributions to enhance its offerings. Explore how OpenAgents is shaping the future of language agent interaction and deployment in real-world settings.","Discover how OpenAgents is transforming the use of language agents with its open platform, featuring Data, Plugins, and Web Agents for everyday tasks. Learn about its easy deployment, full-stack approach, and interactive chat UI.",AI Development Platform,"Python





        3,499





        356


        Built by

          









        25 stars today",https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/openagents_overview.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/data_agent.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/data_agent_demo.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/plugins_agent.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/plugins_agent_demo.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/web_agent.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/web_agent_demo.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/system_design.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/transparent.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/transparent.png; https://raw.githubusercontent.com/xlang-ai/OpenAgents/main/pics/transparent.png,,3499,2023-08-08T09:15:28Z
2024-05-01,https://github.com/ymcui/Chinese-LLaMA-Alpaca-3,https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/README.md,"The project, based on Meta's newly released open-source large model Llama-3, marks the third phase in the Chinese-LLaMA-Alpaca series. It introduces the Chinese version of Llama-3 base models and Llama-3-Instruct instructionally fine-tuned models. These models are incrementally pre-trained using extensive Chinese datasets on top of the original Llama-3, further enhanced with carefully selected instruction data to improve Chinese semantic understanding and instruction interpretation, achieving significant performance improvements over previous versions. The project provides models for source and instruction fine-tuning scripts, enabling further training or fine-tuning as needed. Additionally, it offers tutorials for local quantization and deployment on personal computers using CPU/GPU, supporting various Llama-3 ecosystem tools like transformers, llama.cpp, and text-generation-webui. Major contents include open-sourcing the base and instruction models, pre-training and fine-tuning scripts, instruction fine-tuning data, and guides for model quantization and deployment. The initiative supports a broad range of applications and further development within the Llama-3 ecosystem.",Exploring the Evolution of Chinese LLaMA-Alpaca Models: Unveiling the Third Generation,"Meta's latest open-source release, Chinese LLaMA-Alpaca-3, marks the third phase in the development of Chinese LLaMA-Alpaca open-source models. Leveraging extensive Chinese datasets, this iteration introduces the Chinese LLaMA-3 base and LLaMA-3-Instruct models, significantly enhancing Chinese semantic understanding and instruction-following capabilities. Compared to its predecessors, this version shows remarkable improvements, supporting the ðŸ¤—transformers ecosystem and offering tutorials for local deployment and quantization on personal computers.","Discover the advancements in the Chinese LLaMA-Alpaca-3 project by Meta, featuring improved Chinese language models and instruction-following capabilities, support for the ðŸ¤—transformers ecosystem, and local deployment tutorials.",Language Models,"Python





        279





        24


        Built by

          






        19 stars today",https://raw.githubusercontent.com/ymcui/Chinese-LLaMA-Alpaca-3/main/./pics/banner.png,,279,2023-11-09T08:11:19Z
2024-05-01,https://github.com/magic-research/PLLaVA,https://raw.githubusercontent.com/magic-research/PLLaVA/main/README.md,"PLLaVA (Pooling LLaVA) extends image-language pre-training models to video data for dense captioning tasks, with a focus on computational efficiency and resource-light approaches. Traditional direct fine-tuning of image models for video tasks often results in performance saturation or even degradation, partly due to the dominance of certain patches in video frames. To address this, PLLaVA introduces a simple pooling strategy to smooth feature distribution over time, reducing the impact of dominant tokens. This modification leads to state-of-the-art performance on multiple datasets, including a notable improvement on the Video ChatGPT benchmark and the multi-choice benchmark MVBench. The work emphasizes easy experiment setup and result tracking, aiming to support further research and prototype development. PLLaVA's code, models, and datasets have been made publicly available, inviting contributions from the community for optimization and enhancement.",Unlocking Video Dense Captioning Excellence with PLLaVA: A Revolutionary Approach,"Discover PLLaVA, a groundbreaking parameter-free extension of LLaVA from images to videos, revolutionizing video dense captioning with state-of-the-art performance. Bridging the gap in vision-language pre-training (VLP) for video tasks, PLLaVA introduces a simple, highly efficient pooling strategy, minimizing computational resources while maximizing outputs. Dive deeper into our recent findings, methodology, and unparalleled achievements across major benchmarks, setting new standards in video question answering and caption generation. Explore more about how PLLaVA is steering the future of VLP and its remarkable impact on simplifying the adaptation of pre-trained models for video data.","PLLaVA extends image-language modeling to video data with minimal computational resources, offering unparalleled video dense captioning performance. Learn about its innovative approach and state-of-the-art achievements.",Deep Learning Platform,"Python





        219





        10


        Built by

          








        19 stars today",https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/logo.png; https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/teaser.jpg; https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/module.png; https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/zeroshot.png; https://raw.githubusercontent.com/magic-research/PLLaVA/main/assert/performance.png,https://www.youtube.com/watch?v=nAEje8tu18U; https://github.com/magic-research/PLLaVA/assets/55656210/a6619702-12d3-489d-bfcc-0ef7105544b2,219,2024-04-18T05:36:08Z
2024-05-01,https://github.com/openai/gpt-2,https://raw.githubusercontent.com/openai/gpt-2/master/README.md,"The text provides an overview of the gpt-2 repository, stemming from the paper ""Language Models are Unsupervised Multitask Learners."" It announces the archive status of the code, meaning it's provided as-is without updates. The repository includes models from the paper, a staged release discussion, a dataset for behavior research, and acknowledges an error in the original parameter counts. It is intended as a starting point for researchers and engineers to explore GPT-2, highlighting the necessity for careful evaluation due to potential biases and inaccuracies in the models derived from their training data. The text also invites collaboration, especially from those researching malicious uses, biases, and their mitigations. It includes development resources, contributor acknowledgments, citation details, and a hint at possible future code releases for model evaluation. The licensing is under a modified MIT license.",Exploring GPT-2: A Milestone in Natural Language Processing,"Discover the intricacies of GPT-2, a transformative language model by OpenAI detailed in 'Language Models are Unsupervised Multitask Learners'. This revolutionary technology offers insights into the development and potential of AI in understanding and generating human-like text. GPT-2's release marks a significant moment in AI research, highlighting both the model's capabilities and the ethical considerations in AI deployment. Researchers are encouraged to explore its code, understand its biases, and contribute to its responsible use in various applications.","Dive into the world of GPT-2 by OpenAI, an advanced language model that's pushing the boundaries of AI research, its development, applications, and the critical discussion around its ethical use.",Open Source Tool,"Python





        21,192





        5,313


        Built by

          









        14 stars today",,,21192,2019-02-11T04:21:59Z
2024-05-01,https://github.com/pydantic/pydantic,https://raw.githubusercontent.com/pydantic/pydantic/main/README.md,"Pydantic is a data validation library that utilizes Python type hints for fast, extensible validation, aligning well with linters, IDEs, and developer expectations. It enables definition and validation of data in Python 3.8+. Pydantic has founded a company to continue its developmental success and principles. Version 2 of Pydantic presents a significant update over V1, with many new features, performance enhancements, and some compatibility-breaking changes. Included is backward compatibility with the latest V1 version to aid in gradual codebase updates. Installation can be done via pip or conda, and the documentation provides a simple example, installation instructions, guidance on contributing to its development, and information on reporting security vulnerabilities.",Maximizing Data Validation with Pydantic for Python Developers,"Discover the power of data validation using Python type hints with Pydantic. Fast, extensible, and IDE-friendly, Pydantic ensures your data conforms to your Pythonic schema with ease. Learn how the newly formed Pydantic Company propels this tool into the future. Dive into the enhancements of Pydantic V2, offering new features and improved performance. Whether you're upgrading from Pydantic V1 or starting fresh, this blog post guides you through the essentials.","Explore Pydantic for effective data validation in Python. Get insights into Pydantic V2's new features, the Pydantic Company, and how to seamlessly validate data with Python type hints.",Python Libraries Collection,"Python





        18,729





        1,652


        Built by

          









        37 stars today",,,18729,2017-05-03T21:23:58Z
2024-05-01,https://github.com/VikParuchuri/surya,https://raw.githubusercontent.com/VikParuchuri/surya/master/README.md,"Surya is a comprehensive OCR toolkit named after the Hindu sun god, endorsing its universal vision capability. It outperforms cloud services in OCR with support for over 90 languages, offering line-level text detection, layout analysis to determine the document's structure, and reading order detection. Surya is suitable for various document types, proving its effectiveness through benchmarks detailed on usage and performance indicators. It encourages community engagement and development on its Discord platform. Installation requires Python 3.9+, PyTorch, and a specific transformers version, with model weights downloading upon the first run. Features include an interactive Streamlit app for trials, commands for OCR, text and layout detection, along with a Python usage model for direct implementation. The toolkit emphasizes on documents, not images or handwritten texts, and offers specific environmental variables for optimization. Manual installation for development purposes and dual licensing for commercial use over a certain revenue threshold are also available.",Elevate Your Document Processing with Surya: The Ultimate OCR Toolkit,"Discover Surya, the cutting-edge OCR toolkit that benchmarks against cloud services with support for 90+ languages. It revolutionizes document processing with line-level text detection, comprehensive layout analysis, and precise reading order detection. Surya works seamlessly across various documents, enhancing accuracy and efficiency in text recognition tasks. Dive into the future of document OCR with Surya, where powerful performance meets versatility.","Explore Surya, an advanced OCR toolkit for flawless document processing in over 90 languages, featuring line-level text detection, layout analysis, and reading order detection for unparalleled accuracy and efficiency.",Document Conversion Tool,"Python





        6,112





        368


        Built by

          








        308 stars today",https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt.png; https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_text.png; https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_layout.png; https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt_reading.jpg; https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_rec_chart.png; https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/gcloud_rec_bench.png; https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_chart_small.png; https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_layout_chart.png,,6112,2024-01-10T05:17:42Z
2024-05-01,https://github.com/plotly/dash,https://raw.githubusercontent.com/plotly/dash/master/README.md,"Dash is a leading Python framework for creating web applications for machine learning and data science. It integrates modern UI components with analytical Python code using technologies like Plotly.js, React, and Flask. Dash enables rapid development of interactive applications with features such as dropdowns, sliders, and graphs linked directly to the backend code. The framework offers extensive documentation, a gallery of example apps, and is designed for both beginners and seasoned developers. Dash Open Source allows for local development, while Dash Enterprise offers scalability, deployment, and enhanced productivity tools for commercial applications. Key features include Kubernetes scaling, no-code authentication, Job Queue for asynchronous tasks, Design Kit for low-code app styling, Snapshot Engine for sharing app states, and comprehensive AI and big data integrations.",Unlocking Analytics: The Power of Dash for Python Web Apps,"Discover Dash, the leading Python framework for creating interactive ML and data science web apps. Built on Plotly.js, React, and Flask, Dash seamlessly integrates modern UI elements with your data analytics code. With options for both beginners and seasoned developers, including comprehensive documentation and a gallery of sample Dash apps, learning to create dynamic web applications has never been easier. Whether for individual use or enterprise-scale deployments, Dash offers extensive features for building and scaling your web apps.","Explore the capabilities of Dash, the most trusted Python framework for building machine learning and data science web applications. Learn how Dash combines Plotly.js, React, and Flask to create interactive, user-friendly web apps.",Python Web Development,"Python





        20,523





        1,989


        Built by

          









        7 stars today",https://user-images.githubusercontent.com/1280389/30086128-9bb4a28e-9267-11e7-8fe4-bbac7d53f2b0.gif; https://user-images.githubusercontent.com/1280389/30086123-97c58bde-9267-11e7-98a0-7f626de5199a.gif; https://user-images.githubusercontent.com/1280389/30086299-768509d0-9268-11e7-8e6b-626ac9ca512c.gif; https://user-images.githubusercontent.com/2678795/161153710-57952401-6e07-42d5-ba3e-bab6419998c7.gif; https://user-images.githubusercontent.com/2678795/161155614-21c54a22-f821-4dda-b910-ee27e27fb5f2.png,,20523,2015-04-10T01:53:08Z
2024-05-01,https://github.com/OpenInterpreter/01,https://raw.githubusercontent.com/OpenInterpreter/01/main/README.md,"The 01 Project is an open-source initiative aimed at creating an ecosystem for AI-powered devices, aspiring to be the GNU/Linux for conversational technology. It offers the 01 Light, an ESP32-based voice interface, and is working on the 01 Heavy, designed for local operations. The project is at the experimental stage, warning against use on devices with sensitive information until a stable release is achieved. A notable update includes integrating RealtimeTTS and RealtimeSTT for enhanced voice interaction. The software supports Mac OSX and Ubuntu, with experimental support for Windows, and involves cloning the repository, installing dependencies, and running simulations. They plan to expand hardware compatibility and invite contributions to support and build more hardware. The system operates a server for connectivity, supports local mode operations, and allows for customization to tailor functionality. Contributors are welcomed to further the project's development.",Revolutionizing AI Devices: Introducing The Open Source 01 Project,"The 01 Project aims to transform the AI device ecosystem with its open-source operating system, enabling conversational devices like never before. Its commitment to openness, modularity, and freedom is poised to make it the GNU/Linux of conversational AI. With rapid development and enhancements including RealtimeTTS and RealtimeSTT integration, the project is on the cusp of major breakthroughs. Users can participate in this exciting journey by cloning the repository, setting up their environment, and running simulations or local setups. Join the community to contribute towards building a more connected and interactive world.","Discover the 01 Project, an open-source initiative for AI devices designed to power conversational interfaces with ease. Learn how to join, contribute, and transform the future of conversational AI.",Open Source Tool,"Python





        4,444





        419


        Built by

          









        43 stars today",https://www.openinterpreter.com/OI-O1-BannerDemo-3.jpg,https://www.youtube.com/watch?v=1ZXugicgn6U,4444,2024-01-11T05:45:13Z
2024-05-01,https://github.com/UniModal4Reasoning/ChartVLM,https://raw.githubusercontent.com/UniModal4Reasoning/ChartVLM/main/README.md,"The paper introduces ChartX, a comprehensive evaluation set for benchmarking Multi-modal Large Language Models' (MLLMs) abilities in interpreting and reasoning over visual charts. ChartX covers 18 chart types, 7 tasks, and 22 topics with high-quality data. Additionally, the authors develop ChartVLM, a new model designed for multi-modal tasks requiring interpretable pattern recognition, such as those involving charts or geometric images. ChartVLM is evaluated against mainstream MLLMs using ChartX, showing superior performance in chart-related tasks, even comparable to that of GPT-4V. This work aims to advance the exploration of more interpretable multi-modal models and enhance the understanding of MLLMs' capabilities in the chart domain. The evaluation set and models presented seek to facilitate further research in this area.",Exploring ChartX & ChartVLM: Enhancing Multi-modal Large Language Models with Visual Charts,"Discover the innovative ChartX and ChartVLM, the cutting-edge tools advancing the comprehension and reasoning capabilities of Multi-modal Large Language Models (MLLMs) within the visual chart domain. ChartX sets a new benchmark with its extensive evaluation set covering diverse chart types and tasks across various disciplines. ChartVLM emerges as a superior model, outperforming existing MLLMs and chart-specific models with its interpretable pattern handling for charts and geometric images. This breakthrough study not only showcases ChartVLM's remarkable achievements but also opens new avenues for future research in multi-modal model interpretability and evaluation.",Unveil the potential of ChartX and ChartVLM in advancing MLLM's abilities to interpret and reason with visual charts. Explore how these tools set new benchmarks and outperform leading models in the field.,Multimodal AI Model,"Python





        172





        15


        Built by

          









        6 stars today",https://raw.githubusercontent.com/UniModal4Reasoning/ChartVLM/main/assets/motivation.png; https://raw.githubusercontent.com/UniModal4Reasoning/ChartVLM/main/assets/tsne.png; https://raw.githubusercontent.com/UniModal4Reasoning/ChartVLM/main/assets/chartvlm.png; https://raw.githubusercontent.com/UniModal4Reasoning/ChartVLM/main/assets/radar_se.png; https://raw.githubusercontent.com/UniModal4Reasoning/ChartVLM/main/assets/radar_qa.png; https://raw.githubusercontent.com/UniModal4Reasoning/ChartVLM/main/assets/radar_desc.png; https://raw.githubusercontent.com/UniModal4Reasoning/ChartVLM/main/assets/radar_summ.png,,172,2024-01-29T10:31:30Z
2024-05-01,https://github.com/dusty-nv/jetson-containers,https://raw.githubusercontent.com/dusty-nv/jetson-containers/master/README.md,"The project focuses on creating modular container build systems, offering a wide variety of AI/ML packages specifically designed for NVIDIA Jetson platforms. It encompasses a comprehensive suite of machine learning and artificial intelligence components, including packages for PyTorch, TensorFlow, ONNXRuntime, DeepStream, JupyterLab, and various large language models (LLMs) such as NanoLLM and Transformers. Additionally, it supports CUDA, robotics applications, audio processing, smart home technologies, and more. Users can easily build and run their own containers by combining different packages, leveraging tools provided for system setup, container building, and execution, ensuring compatibility with JetPack/L4T. The documentation guides through package lists, definitions, setup instructions, and building/running containers, supported by tutorials for getting started. The initiative facilitates the deployment of advanced AI capabilities on Jetson devices, streamlining the development process for applications ranging from voice chats and object detection to smart home solutions.",Optimizing AI: Unleashing Machine Learning Containers on NVIDIA Jetson,"Discover the power of machine learning containers designed for NVIDIA Jetson and JetPack, offering seamless integration and efficient AI development. This modular build system includes a wide array of AI/ML packages, from PyTorch to TensorFlow, making it easier than ever to deploy advanced AI applications on Jetson devices. Whether you're running ROS2 with PyTorch or need a specific CUDA version, these containers streamline the process. Dive into the world of AI with Jetson's containers and transform your projects with cutting-edge technology.","Explore how machine learning containers for NVIDIA Jetson and JetPack can revolutionize AI development with a comprehensive suite of AI/ML packages, from PyTorch to TensorFlow, for seamless integration and deployment.",AI Development Platform,"Python





        1,674





        388


        Built by

          









        5 stars today",https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/header_blueprint_rainbow.jpg; https://nvidia-ai-iot.github.io/jetson-generative-ai-playground/images/JON_Gen-AI-panels.png; https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/llamaspeak_llava_clip.gif; https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/llamaspeak_70b_yt.jpg; https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/nanodb_tennis.jpg; https://github.com/NVIDIA-AI-IOT/nanoowl/raw/main/assets/jetson_person_2x.gif; https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava.gif; https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava_bear.jpg; https://www.jetson-ai-lab.com/images/slm_console.gif,https://www.youtube.com/watch?v=UOjqF3YCGkY; https://www.youtube.com/watch?v=9ObzbbBTbcc; https://www.youtube.com/watch?v=hswNSZTvEFE; https://www.youtube.com/watch?v=wzLHAgDxMjQ; https://www.youtube.com/watch?v=OJT-Ax0CkhU; https://www.youtube.com/watch?v=wzLHAgDxMjQ; https://www.youtube.com/watch?v=w48i8FmVvLA; https://www.youtube.com/watch?v=X-OXxPiUTuU; https://www.youtube.com/watch?v=wZq7ynbgRoE; https://www.youtube.com/watch?v=X-OXxPiUTuU,1674,2020-04-29T19:50:58Z
2024-05-01,https://github.com/epfLLM/meditron,https://raw.githubusercontent.com/epfLLM/meditron/main/README.md,"Meditron introduces Meditron-7B and Meditron-70B, advanced open-source medical Large Language Models (LLMs) derived from Llama-2. Focused on the medical domain, these models underwent additional pretraining on a diverse medical corpus, including PubMed papers, medical guidelines, and general domain data, significantly outperforming competitors like GPT-3.5 in medical reasoning tasks. However, it cautions against Meditron's clinical application without further alignment and testing due to its foundational training stage. Meditron, primarily in English, utilizes a causal decoder-only transformer architecture and is accessible through HuggingFace. Its training leveraged extensive computational resources, including NVIDIA's A100 GPUs, to achieve optimal parallelism and efficiency. Future updates will aim to enhance model performance, with Meditron-70B already available for non-clinical AI assistance in healthcare, including educational and informational queries, despite not being recommended for production use in natural language generation.","Revolutionizing Healthcare: Introducing Meditron-70B, a Medical Domain Language Model","Discover Meditron, the groundbreaking suite of open-source medical Large Language Models (LLMs), including Meditron-7B and Meditron-70B. These models have been expertly adapted from Llama-2 and fine-tuned on a vast medical corpus, outperforming other LLMs in medical reasoning tasks. Despite their advanced capabilities, Meditron is advised against use in direct medical applications without further testing. Incorporating a wide range of medical literature and guidelines, Meditron represents a significant step forward in AI-assisted healthcare.","Explore Meditron-70B & Meditron-7B: Open-source medical domain LLMs surpassing Llama-2, GPT-3.5 & Flan-PaLM in medical tasks. Learn about their development, capabilities, and advisory notice.",Large-scale Language Models,"Python





        1,607





        147


        Built by

          









        20 stars today",https://raw.githubusercontent.com/epfLLM/meditron/main/figures/meditron_LOGO.png; https://raw.githubusercontent.com/epfLLM/meditron/main/figures/meditron-pipeline.png; https://raw.githubusercontent.com/epfLLM/meditron/main/figures/prompt_example.png,,1607,2023-11-23T17:45:46Z
2024-05-01,https://github.com/snap-stanford/stark,https://raw.githubusercontent.com/snap-stanford/stark/main/README.md,"STaRK is a benchmark designed to evaluate the performance of large language models (LLMs) on retrieval tasks within both textual and relational knowledge bases. It addresses the challenge of extracting relevant nodes from a knowledge base given a user query, a task that tests the LLM's ability to understand and manipulate complex query requirements. STaRK is significant for its introduction of a novel task combining textual and relational data, its provision of large and diverse knowledge bases across three areas, and its use of natural, practical queries that reflect real-life information needs. The benchmark includes detailed setup instructions for environment configuration, dataset loading, and evaluation, supporting both automatic data downloading and custom preprocessing. Evaluation metrics involve embedding node documents and queries to assess model performance, with specific tools and models recommended for generating these embeddings. The provided code and documentation facilitate the use of STaRK for benchmarking purposes, with a call for researchers to cite the work if it informs their own.",STaRK: A New Benchmark in LLM Retrieval from Textual and Relational Knowledge Bases,"STaRK introduces a novel benchmark for evaluating large language models (LLMs) in retrieving relevant nodes from textual and relational knowledge bases in response to user queries. It highlights the unique challenge of managing the complex relationship between text and data within queries, offering a large-scale and diverse set of knowledge bases for thorough testing. The benchmark focuses on real-life, practical queries that integrate rich relational data and textual nuances to mimic actual user scenarios. With STaRK, researchers can push the boundaries of information retrieval, providing insights into the capacities and limitations of current LLM technologies in handling intricate data retrieval tasks.","Discover STaRK, the benchmark revolutionizing how LLMs are evaluated on retrieving data from textual and relational knowledge bases, emphasizing complex query understanding and practical, real-life scenarios.",Deep Learning Tool,"Python





        155





        18


        Built by

          






        19 stars today",https://raw.githubusercontent.com/snap-stanford/stark/main/media/overview.png; https://raw.githubusercontent.com/snap-stanford/stark/main/media/kb.jpg; https://raw.githubusercontent.com/snap-stanford/stark/main/media/questions.jpg,,155,2024-04-22T17:56:26Z
2024-05-02,https://github.com/dnhkng/GlaDOS,https://raw.githubusercontent.com/dnhkng/GlaDOS/main/README.md,"The GLaDOS Personality Core project aims to create a real-life, interactive, and aware version of the GLaDOS character, blending hardware and software. Key achievements so far include developing a GLaDOS voice generator and designing a realistic personality core. Future tasks include integrating a memory system via MemGPT, implementing vision with LLaVA, producing 3D-printable components, and developing an animatronics system. The software architecture focuses on low-latency voice interaction, minimizing dependencies, and simplifying the system for constrained hardware. The hardware will feature servo and stepper motors for animations. Installation instructions are provided for Linux, with potential adaptability to Windows, covering text-to-speech engines, voice recognition, and necessary models. Testing can be conducted through a provided demonstration notebook.",Bringing GLaDOS to Life: Real-World AI Personality Core Project,"Discover the journey of creating a real-life GLaDOS, an ambitious project combining hardware and software to develop an aware, interactive AI. This endeavor involves training a voice generator, crafting a realistic personality core, and integrating advanced features like memory and vision systems. With a focus on low-latency voice interaction and minimal dependencies, this project aims to install GLaDOS into a physical form complete with animations and the ability to track and engage with humans. Explore the cutting-edge technology and passion driving the creation of a real-world GLaDOS, embodying the iconic character with a blend of animation, voice, and AI.","Join us in building a real-life GLaDOS: an interactive, AI-driven project that breathes life into the iconic character through advanced software architecture, 3D animatronics, and innovative voice interaction.",AI Development Platform,"Python





        1,076





        102


        Built by

          







        71 stars today",https://img.youtube.com/vi/KbUfWpykBGg/0.jpg,https://www.youtube.com/watch?v=KbUfWpykBGg,1077,2023-03-23T14:49:16Z
2024-05-02,https://github.com/TheOfficialFloW/PPPwn,https://raw.githubusercontent.com/TheOfficialFloW/PPPwn/master/README.md,"PPPwn is an exploit targeting PlayStation 4 systems up to firmware version 11.00, offering kernel-level remote code execution capabilities. It leverages a long-standing vulnerability identified as CVE-2006-4304, with proper disclosure to PlayStation's team. This proof-of-concept exploit, which simply prints ""PPPwned"" on affected PS4s, necessitates further adaptation for running homebrew enablers like Mira. It requires a computer with an Ethernet port (or USB adapter), an Ethernet cable, Linux (virtual machines are supported), and Python3 plus gcc for execution. Users need to clone the PPPwn repository, install prerequisites, compile payloads for the relevant firmware version, and run the exploit script. The setup on the PS4 involves configuring a PPPoE connection and testing the internet connection to trigger the exploit. The process involves stages from initialisation to arbitrary payload execution. Note for Mac Apple Silicon users: the code doesn't natively compile due to architecture differences, but a workaround using Docker to build the required binaries is available.",Unlocking PlayStation 4: Exploring the PPPwn Exploit for Remote Code Execution,"Discover how the PPPwn exploit enables kernel-level Remote Code Execution (RCE) on PlayStation 4 firmware up to version 11.00, utilizing a proof-of-concept for the recognized CVE-2006-4304. This groundbreaking exploit showcases a method to display 'PPPwned' on affected consoles, hinting at further possibilities for homebrew applications with additional payload modification. Requirements include a Linux-based system, Python3, and gcc, among a few hardware necessities. The process involves cloning the repository, installing requirements, compiling payloads, and executing the exploit, demonstrating a significant advancement in PS4 hacking capabilities.","Learn about the PPPwn exploit for PlayStation 4, a method for achieving kernel remote code execution on firmware up to version 11.00, advancing the realm of console hacking and homebrew applications.",Cybersecurity Tool,"Python





        872





        108


        Built by

          









        189 stars today",,,872,2024-04-30T16:09:20Z
2024-05-02,https://github.com/zju3dv/LoG,https://raw.githubusercontent.com/zju3dv/LoG/main/README.md,"LoG is an advanced technology that employs an RTX 4090 for training highly realistic urban-scale models and for their real-time rendering. The project's codebase, which is built on PyTorch, utilizes gaussian-splatting techniques to achieve its objectives. For users interested in a hands-on experience, a detailed installation guide and preprocessing documentation are provided, alongside a minimal example dataset. The training process is straightforward, with commands for both training the model and generating interpolation visualization videos. Additionally, an update is anticipated for a real-time rendering tool aimed at immersive visualization. The project acknowledges various prior works and datasets that have been inspirational in its development. Contributions to the project are encouraged, and for academic citations, a specific BibTeX entry is shared.",Harnessing RTX 4090 for Urban-Scale Model Training and Real-Time Rendering with LoG,"Discover how LoG leverages the power of RTX 4090 to train highly realistic urban-scale models for real-time rendering. Built on PyTorch and employing advanced gaussian-splatting techniques, LoG offers a new horizon in 3D visualization and immersive experiences. Dive into our project for easy setup, dataset preparation, and efficient training guides. A breakthrough in rendering technology, LoG invites contributions and further innovation in the field.",Learn how LoG utilizes RTX 4090 for groundbreaking training of urban-scale models and their real-time rendering. Experience seamless setup and advanced techniques for unparalleled 3D visualization.,3D Image Rendering,"Python





        469





        15


        Built by

          





        80 stars today",,,469,2024-03-29T14:10:48Z
2024-05-02,https://github.com/Textualize/rich,https://raw.githubusercontent.com/Textualize/rich/master/README.md,"Rich is a versatile Python library designed to enhance terminal output with rich text and beautiful formatting capabilities. It supports colorful and styled text, pretty tables, progress bars, markdown rendering, syntax-highlighted code, tracebacks, and more, requiring no extra configuration for Jupyter notebooks. Rich is compatible with Linux, OSX, Windows (with certain color limitations on classic terminals), and requires Python 3.7 or later. Installation is straightforward via pip. Rich offers an easy way to produce richly formatted console output, including a special print method that enriches text with styles and emojis, and a REPL mode for beautified display of data structures. It also includes features for debugging and development, such as log and inspect methods, and various utilities like progress bars, tables, markdown, and syntax highlighting for enhanced code readability and debugging. Rich is extendable, encouraging users to implement their content using the Console Protocol.",Master Rich: Turbocharge Your Python Terminal Experience,"Discover how the Rich library can elevate your Python terminal applications with stunning text formatting, progress bars, markdown rendering, and more. Designed for compatibility across major operating systems, Rich supports Python 3.7 and later, making it widely accessible. Its API simplifies adding colors, styles, and complex layouts to terminal outputs, transforming mundane scripts into visually engaging experiences. From syntax-highlighted code blocks to elegant tables, Rich offers a toolkit to enhance CLI applications and debugging processes. Get started with Rich today to unlock a world of possibilities in terminal aesthetics and functionality.","Explore the Rich library for Python to beautify terminal applications with colors, styles, markdown, and more. Compatible with Python 3.7+, Rich is perfect for developers seeking to improve CLI aesthetics and usability.",Python Libraries Collection,"Python





        47,171





        1,671


        Built by

          









        18 stars today",https://github.com/textualize/rich/raw/master/imgs/features.png; https://github.com/textualize/rich/raw/master/imgs/print.png; https://github.com/textualize/rich/raw/master/imgs/repl.png; https://github.com/textualize/rich/raw/master/imgs/hello_world.png; https://github.com/textualize/rich/raw/master/imgs/where_there_is_a_will.png; https://github.com/textualize/rich/raw/master/imgs/inspect.png; https://github.com/textualize/rich/raw/master/imgs/log.png; https://github.com/textualize/rich/raw/master/imgs/logging.png; https://github.com/textualize/rich/raw/master/imgs/table_movie.gif; https://github.com/textualize/rich/raw/master/imgs/table.png; https://github.com/textualize/rich/raw/master/imgs/table2.png; https://github.com/textualize/rich/raw/master/imgs/progress.gif; https://github.com/textualize/rich/raw/master/imgs/downloader.gif; https://github.com/textualize/rich/raw/master/imgs/status.gif; https://github.com/textualize/rich/raw/master/imgs/spinners.gif; https://github.com/textualize/rich/raw/master/imgs/tree.png; https://github.com/textualize/rich/raw/master/imgs/columns.png; https://github.com/textualize/rich/raw/master/imgs/markdown.png; https://github.com/textualize/rich/raw/master/imgs/syntax.png; https://github.com/textualize/rich/raw/master/imgs/traceback.png; https://raw.githubusercontent.com/Textualize/rich-cli/main/imgs/rich-cli-splash.jpg; https://raw.githubusercontent.com/Textualize/textual/main/imgs/textual.png,,47171,2019-11-10T15:28:09Z
2024-05-02,https://github.com/cpacker/MemGPT,https://raw.githubusercontent.com/cpacker/MemGPT/main/README.md,"MemGPT is a platform designed to build and deploy stateful LLM (Large Language Model) agents equipped with long-term memory capabilities and customizable tools. It supports connecting to external data sources like PDFs for Retriever-Augmented Generation (RAG) and defining custom functions, such as performing Google searches. Installation is straightforward, requiring `pip` for Python and setting environment variables for integration with OpenAI or using MemGPT's hosted endpoint. MemGPT offers CLI and server deployment options, the latter of which supports Docker and direct execution for running services accessible via a web developer portal. It's model-agnostic, supporting various LLM and embedding endpoints from providers like OpenAI, Azure, Google AI, and others, with performance dependent on the LLM's function-calling ability. The platform provides comprehensive documentation online, encourages community support through Discord, and aligns with legal standards through privacy and service terms. MemGPT encourages feedback on its developer roadmap and performance benchmarking to enhance model compatibility.",Elevate Your AI: Harnessing MemGPT for Advanced LLM Agents with Long-Term Memory,"MemGPT revolutionizes the way LLM agents are built and deployed by offering long-term memory management, integration with external data sources, and the ability to define and utilize custom tools. This cutting-edge framework supports a wide range of LLM and embedding endpoints from leading providers including OpenAI, Google AI, and more. Easy installation and versatile setup options, including CLI and server deployment, make MemGPT accessible for developers of all skill levels. Additionally, the platform ensures robust support and continuous improvement through its dynamic community and developer roadmap. Dive into the future of AI development by leveraging MemGPT's comprehensive documentation and versatile capabilities.","Discover how MemGPT enables the creation of stateful LLM agents with long-term memory, external data integration, and custom tools. Learn about its easy setup, diverse endpoint support, and vibrant developer community.",AI Development Platform,"Python





        9,001





        979


        Built by

          









        211 stars today",,,9001,2023-10-11T07:38:37Z
2024-05-02,https://github.com/P1sec/QCSuper,https://raw.githubusercontent.com/P1sec/QCSuper/master/README.md,"QCSuper is a tool designed for Qualcomm-based phones and modems, enabling the capture of raw 2G/3G/4G and some 5G radio frames. It supports generating PCAP captures using a rooted Android phone, USB dongle, or existing captures in other formats. Installation and usage depend on the operating system, but it has been tested on Ubuntu LTS 22.04 and Windows 11, recommending Linux for better compatibility. Rooting the phone or exposing a diag service port is necessary for use. Wireshark can open the PCAP files for analyzing the frames, with specific version requirements based on the radio technology. QCSuper operates through the Qualcomm Diag protocol to communicate with the phone's baseband for diagnostics. Users can report device compatibility on its GitHub issue page. The tool offers various functionalities, including live Wireshark integration, PCAP dumping, memory dumping, and detailed device information gathering. It has been tested with several devices, showing a range of compatibility but aims to support a wide range of Qualcomm-based devices.",Unlock the Power of QCSuper: A Comprehensive Guide for Qualcomm Phone Analysis,"QCSuper is a powerful tool for analyzing Qualcomm-based phones, providing capabilities to capture raw radio frames across 2G, 3G, 4G, and select 5G networks. It allows users to generate PCAP captures using a rooted Android phone, USB dongle, or existing captures. With its ability to utilize the Qualcomm Diag protocol, QCSuper enables detailed communication analysis through easy-to-use commands. Whether you're troubleshooting or gathering insights, QCSuper equips you with the data needed, directly accessible in Wireshark for in-depth examination. Discover how to install and optimize QCSuper for your device, ensuring you harness its full potential for your telecommunications analysis needs.","Explore how to use QCSuper for capturing comprehensive 2G, 3G, 4G, and 5G radio frames on Qualcomm-based phones. This guide covers installation, usage, and how to analyze captured data with Wireshark for effective Qualcomm phone diagnostics.",Cybersecurity Tool,"Python





        1,118





        214


        Built by

          









        74 stars today",,,1118,2019-07-09T08:20:21Z
2024-05-02,https://github.com/run-llama/llama_parse,https://raw.githubusercontent.com/run-llama/llama_parse/main/README.md,"LlamaParse is an API developed by LlamaIndex for efficiently parsing and representing files, facilitating better retrieval and context enhancement with LlamaIndex frameworks. It seamlessly integrates with LlamaIndex, offering a free plan for up to 1000 pages daily and a paid plan for 7000 pages weekly at $0.003 per extra page. To start, users must obtain an API key from the LlamaIndex cloud, ensure they have the latest LlamaIndex version, and follow a migration guide if upgrading from an older version. Installation involves using pip for LlamaIndex and LlamaParse packages. The API supports synchronous and asynchronous operations, including batch processing, with options for output format, worker allocation, verbosity, and language specifications. Integration with `SimpleDirectoryReader` for default PDF processing and full documentation is available, alongside various examples for easy understanding and application. Terms of Service are provided.",Maximize Your File Parsing with LlamaParse: The Ultimate Guide,"Discover the capabilities of LlamaParse, the comprehensive API designed by LlamaIndex for optimal file parsing and contextual representation. With seamless integration into the LlamaIndex frameworks, it offers a scalable solution for managing documents up to 1000 pages daily for free, and more on a budget-friendly paid plan. Learn how to get started easily, from obtaining your API key to the full installation process, and elevate your file management system today. Dive into the future of efficient file parsing and unlock new potentials for your project or organization.","Unlock efficient file parsing with LlamaParse â€“ the robust API by LlamaIndex. Learn how to streamline your document management with our free and paid plans, easy setup guide, and powerful features for enhanced retrieval and context augmentation.",API Management Tool,"Python





        898





        77


        Built by

          









        32 stars today",,,898,2024-01-31T18:33:07Z
2024-05-02,https://github.com/pydantic/logfire,https://raw.githubusercontent.com/pydantic/logfire/main/README.md,"Pydantic Logfire is an observability platform designed to be both simple and powerful, tailored specifically for Python applications. Developed by the team behind Pydantic, it offers an easy-to-use dashboard that provides deep insights into Python application behavior, such as rich Python object display, event-loop telemetry, and profiling for code and database queries. Logfire allows users to query data using standard SQL and integrates seamlessly with OpenTelemetry for broader language support. It also enhances understanding of data through Pydantic model validations. Logfire can be easily installed and used for manual tracing or integrated with popular Python packages like FastAPI for automatic instrumentation. The platform aims to make powerful tools accessible without complexity, encouraging usage across engineering teams. Additional details on using Logfire, including installation and authentication, can be found in the online documentation. The project welcomes contributions and addresses security vulnerabilities according to its published policies.",Elevate Your Python Projects with Pydantic Logfire: Ultimate Observability Guide,"Discover how Pydantic Logfire transforms observability for Python applications, offering a Python-centric approach to insights, SQL query capabilities, and integration with OpenTelemetry and Pydantic models. From simplifying complex data analysis via a powerful dashboard to enhancing Python application behavior visibility and streamlining data queries with SQL, Logfire is engineered for ease of use without compromising on performance. Explore the documentation and contribute to the evolving platform designed for developers seeking to amplify their engineering team's efficiency.","Learn how Pydantic Logfire revolutionizes Python application observability with a user-friendly platform, providing powerful insights into application behavior, SQL query functionalities, and comprehensive OpenTelemetry and Pydantic integration.",Developer Monitoring Platform,"Python





        419





        8


        Built by

          









        92 stars today",https://docs.pydantic.dev/logfire/images/index/logfire-screenshot-fastapi-200.png,,419,2024-04-23T11:50:23Z
2024-05-02,https://github.com/Free-TV/IPTV,https://raw.githubusercontent.com/Free-TV/IPTV/main/README.md,"This text introduces a M3U playlist for accessing free TV channels globally, emphasizing the availability of channels both locally over the air in various countries and online via platforms like Plex TV and Pluto TV. The key philosophy driving this playlist includes a focus on quality over quantity, ensuring channels are in HD and work well, maintaining that only free channels are listed, and ensuring the content is mainstream, avoiding channels dedicated to adult content, specific religions, political parties, or foreign-funded channels targeting another country. The playlist is compiled using a Python script and markdown files, with each file representing a different region or group of channels. The document also outlines guidelines for submitting additions or changes through pull requests, specifying that changes should only be made to markdown files, and providing clear instructions for adding or removing channels to maintain the playlist's integrity and relevance.",Accessing Worldwide Free TV Channels: Ultimate Guide to Free M3U Playlists,"Discover the ultimate guide to accessing free TV channels worldwide using M3U playlists. From local broadcasts to digital streaming services, learn how to seamlessly enjoy a variety of content across different regions without the hassle of subscriptions. This concise overview introduces you to a range of free platforms including Plex TV, Pluto TV, and more, alongside a philosophy that prioritizes quality, ensuring only the best viewing experience. Start exploring today!","Unlock a world of entertainment with our comprehensive guide on free TV channels accessible globally through M3U playlists. From Pluto TV to Plex, enjoy quality streaming without subscriptions.",Open Source Tool,"Python





        3,782





        650


        Built by

          









        9 stars today",,,3782,2021-04-13T09:13:48Z
2024-05-02,https://github.com/fishaudio/fish-speech,https://raw.githubusercontent.com/fishaudio/fish-speech/main/README.md,"The text pertains to the ""Fish Speech"" codebase and its associated models, which are available under BSD-3-Clause License and CC-BY-NC-SA-4.0 License respectively. Links to licenses, disclaimers about legal responsibilities regarding its use, documentations in English and Chinese, as well as sample outputs, are provided. Additionally, it acknowledges contributions from various projects like VITS2, Bert-VITS2, GPT VITS, MQTTS, GPT Fast, and GPT-SoVITS in the credits section. The project also highlights its sponsorship for data processing by 6Block, including a link and a logo of the sponsor. This summary encapsulates the key information about the project's licensing, documentation, legal disclaimer, contributions, and sponsorship.",Exploring the Technological Waves of Fish Speech: The Future of Audio Processing,"Discover the innovative Fish Speech project, revolutionizing audio processing with its cutting-edge technology. Released under BSD-3-Clause and CC-BY-NC-SA-4.0 Licenses, Fish Speech presents a blend of legal compliance and technological marvel. Its diverse models, including VITS2 and Bert-VITS2, underscore its commitment to transforming audio interactions. Notably, 6Block's support in data processing propels Fish Speech into the forefront of audio technology. Dive into the world of Fish Speech for an insight into tomorrow's audio advancements.","Explore Fish Speech, the forefront of audio processing technology, supported by 6Block for data handling. Learn about its licenses, models like VITS2, and its role in shaping the future.",Voice Conversion Tool,"Python





        884





        77


        Built by

          









        13 stars today",,,884,2023-10-10T03:16:51Z
2024-05-02,https://github.com/Qiskit/qiskit,https://raw.githubusercontent.com/Qiskit/qiskit/main/README.md,"Qiskit is an open-source SDK focused on quantum computing, specifically handling quantum circuits, operators, and primitives. It provides tools for creating, optimizing, and working with quantum circuits through a transpiler and a quantum information toolbox for advanced operators. Qiskit supports quantum programming with constructs like quantum circuits, classical output definitions, and primitive functions (`sampler` and `estimator`) for outcome sampling and value estimation. Installation is recommended via `pip`, ensuring users get the latest version, along with all necessary dependencies. The documentation guides new users through installation, creating quantum programs, and executing them on real quantum hardware. Qiskit bridges classical to quantum computing by optimizing quantum circuits for specific hardware configurations using its transpiler, mapping circuits to hardware's native gates and connectivity. Execution on real quantum hardware necessitates the use of the Qiskit runtime environment and provider-specific backends that offer optimized primitives implementations. Contributions to Qiskit are welcomed, with guidelines available for interested developers. It is a collective effort of many contributors, and users are encouraged to cite Qiskit in their work. The project is partially supported by the DOE Office of Science and adheres to the Apache License 2.0.",Unlocking Quantum Computing: A Beginner's Guide to Qiskit,"Dive into the world of quantum computing with Qiskit, the open-source SDK designed for creating and running quantum circuits, optimizing them for real quantum hardware. Qiskit offers a comprehensive set of tools for quantum circuit design, including primitives for sampling outcomes and estimating values, and a robust transpiler for circuit optimization. With its detailed documentation and supportive community, getting started with Qiskit is accessible to programming enthusiasts and quantum computing beginners alike. Learn how to install Qiskit, build your first quantum program, and take your first steps into the quantum realm today.","Explore the basics of quantum computing with our beginner's guide to Qiskit. Learn how to install Qiskit, create quantum circuits, and execute them on real quantum hardware.",Quantum Computing,"Python





        4,629





        2,232


        Built by

          









        4 stars today",,,4629,2017-03-03T17:02:42Z
2024-05-02,https://github.com/acantril/learn-cantrill-io-labs,https://raw.githubusercontent.com/acantril/learn-cantrill-io-labs/master/README.md,"The repository by learn-cantrill-io-labs provides a rich collection of free demos and mini-projects focused on AWS, with plans to include other cloud platforms in the future. These educational resources are presented in three formats: freely accessible instruction and architecture diagrams, comprehensive courses with theory lessons and guided videos on learn.cantrill.io, and some mini projects that come with video guides directly in the repo. Additionally, creators can earn by contributing new mini projects, supported by course sales on learn.cantrill.io. The projects cover a wide range of topics, including web application architecture, serverless applications, VPNs, using AWS for hybrid DNS setups, pipeline deployments, and database migration among others. Each demo is designed for easy setup, with a 1-Click Deployment feature and detailed instructions. The repository also offers opportunities for users to get course credits by submitting bug fixes through pull requests.",Explore Free AWS Demos and Mini Projects: Elevate Your Cloud Skills,"Discover a treasure trove of AWS demos and mini projects at learn-cantrill-io-labs, perfect for enhancing your cloud expertise. From serverless applications to hybrid DNS, these free resources are designed to complement theoretical knowledge with practical experience. Dive into instruction & architecture diagrams for a hands-on learning experience. Elevate your skills further with video guides available for popular projects. Plus, learn how you can contribute and get paid for creating new mini projects.","Unlock a collection of free AWS demos and mini projects at learn-cantrill-io-labs. Enhance your cloud skills with practical, hands-on resources. Learn more today.",Cloud Development Tool,"Python





        5,252





        2,022


        Built by

          









        8 stars today",https://github.com/acantril/learn-cantrill-io-labs/raw/master/demogrid.png,,5252,2020-06-24T22:39:30Z
2024-05-02,https://github.com/encode/httpx,https://raw.githubusercontent.com/encode/httpx/master/README.md,"HTTPX is a modern HTTP client for Python 3.8+, featuring both synchronous and asynchronous APIs, support for HTTP/1.1 and HTTP/2, and an integrated command line client. It provides a user-friendly API compatible with the well-known `requests` library, adding new capabilities such as direct requests to WSGI or ASGI applications, strict timeouts, and full typing annotations. HTTPX assures 100% test coverage, enforcing reliability and robustness. Its installation is straightforward using pip, with optional support for HTTP/2, SOCKS proxy, rich terminal environments, and decompression of brotli or zstd compressed responses. The project benefits from the foundational work of `requests` and `urllib3`, offering advanced features like session management, automatic content decoding, and multipart file uploads. Extensive documentation is available online for both beginners and advanced users, with a dedicated guide for contributions. HTTPX's dependencies include `httpcore` for transport, `certifi` for SSL certificates, and `idna` for internationalized domain name support, among others.",Maximizing Efficiency with HTTPX: The Next-Generation HTTP Client for Python,"Discover HTTPX, the cutting-edge HTTP client for Python that brings an integrated command line client, full HTTP/1.1 and HTTP/2 support, and both synchronous and asynchronous APIs. Elevate your Python projects with HTTPX's advanced features, including WSGI/ASGI application requests and complete type annotation, for robust and high-performing web requests. Perfect for developers seeking the blend of `requests` simplicity with the power of asynchronous support.","Explore the perks of HTTPX, an advanced HTTP client for Python offering HTTP/1.1, HTTP/2, sync/async APIs, and an integrated CLI. Perfect for modern Python web projects.",Python Libraries Collection,"Python





        12,354





        799


        Built by

          









        18 stars today",https://raw.githubusercontent.com/encode/httpx/master/docs/img/butterfly.png; https://raw.githubusercontent.com/encode/httpx/master/docs/img/httpx-help.png; https://raw.githubusercontent.com/encode/httpx/master/docs/img/httpx-request.png,,12354,2019-04-04T12:27:00Z
2024-05-03,https://github.com/GistNoesis/FourierKAN,https://raw.githubusercontent.com/GistNoesis/FourierKAN/main/README.md,"FourierKAN introduces a PyTorch layer inspired by Kolmogorov-Arnold Networks, utilizing 1D Fourier coefficients instead of spline coefficients for enhanced optimization due to the global, dense nature of Fourier transforms compared to local splines. Upon optimization, the 1D function can be replaced with a spline approximation for quicker evaluations while maintaining similar results. Fourier-based functions offer the advantage of periodicity, ensuring numerical boundedness and avoiding out-of-grid issues. Users are instructed to place the provided file in the same directory for usage and can run a provided script for a demonstration. The code works on both CPU and GPU but remains untested. It's highlighted that while the current version uses memory proportional to grid size, future versions might not require temporary memory. The layer's approach to computation can either materialize a product before summing or use the einsum function for potentially less memory usage but slower performance. The project is under the MIT license, though future developments, including fused kernels, are planned to be proprietary.",Introducing FourierKAN: Revolutionizing PyTorch with Fourier Layers,"Discover the FourierKAN, a groundbreaking Pytorch layer designed to revolutionize neural networks by replacing traditional linear and non-linear activations with 1D Fourier coefficients. Inspired by Kolmogorov-Arnold networks, FourierKAN offers improved optimization, numerical bounds, and flexibility with periodic functions, promising enhancements in speed and performance once optimized with spline approximations. Open-sourced under MIT license, this innovative approach is set to change how we model with PyTorch.","Explore the capabilities of FourierKAN, a pioneering PyTorch layer using 1D Fourier coefficients for more efficient and bounded neural network modeling. Learn how it bridges the gap between theoretical innovation and practical application.",Deep Learning Tool,"Python





        240





        21


        Built by

          






        71 stars today",,,240,2024-05-01T11:39:36Z
2024-05-03,https://github.com/a-real-ai/pywinassistant,https://raw.githubusercontent.com/a-real-ai/pywinassistant/main/README.md,"PyWinAssistant, launched on December 31, 2023, is the inaugural open-source framework for Windows 10/11, designed to aid user interface interaction through an Artificial Narrow Intelligence employing Visualization-of-Thought (VoT) for spatial reasoning in large language models. Distinguishing itself by not requiring OCR or object detection, this tool aims to enhance the general quality of language and vision models while being data-efficient. It enables users to control their computers using natural language, facilitates UI testing for Win32api applications via natural language, and supports automation across a multitude of desktop applications. PyWinAssistant operates on advanced natural language processing, task automation algorithms, and offers features like dynamic case generation, single-action execution, and customizable AI identity. Essentially, it streamlines workflows, boosts productivity, and makes technology more accessible, marking a shift towards more interactive and intuitive AI integration in daily computing tasks.",Revolutionizing Windows UI Automation with PyWinAssistant: A New AI-Powered Framework,"Unveiling PyWinAssistant, the groundbreaking Open Source framework released on 31 Dec 2023, designed for Windows 10/11 that leverages Large Action Model technology for unparalleled UI assistance without the need for OCR. This innovative tool utilizes Visualization-of-Thought (VoT) to enhance spatial reasoning, enabling more natural, efficient human-computer interaction while ensuring minimal data usage. Offering built-in options for improved usability and testing, PyWinAssistant represents a technical leap in UI and UX assistance, catering to any natural language prompt with security and precision at its core.","Explore PyWinAssistant, the first of its kind Large Action Model and Open Source framework for Windows, offering advanced UI assistance with Visualization-of-Thought technology. Released on 31 Dec 2023, it promises a new era of human-computer interaction.",AI Task Automation,"Python





        420





        35


        Built by

          





        25 stars today",,,420,2024-01-01T05:54:22Z
2024-05-03,https://github.com/fcori47/basdonax-ai-rag,https://raw.githubusercontent.com/fcori47/basdonax-ai-rag/master/README.md,"The Basdonax AI RAG v1.0 repository enables the creation of an AI-based personal assistant utilizing open-source models from Meta and Microsoft, namely `Llama3-7b` and `Phi3-4b`. It allows users to upload documents and query them, simplifying tasks with AI technology. The setup requires Docker, and optionally an RTX graphics card for enhanced performance. Installation steps include choosing the data model based on GPU capability, with `Llama3-7b` recommended for those with a capable graphics card, and `Phi3-4b` for others. Following Docker setup, users download their chosen model and modify the assistant prompt as desired. Finally, accessing the RAG is done via a local server. Future access is simplified through a desktop shortcut to reopen the RAG.",How to Build Your AI Assistant with Basdonax AI RAG v1.0: A Step-by-Step Guide,"Dive into the revolutionary world of Basdonax AI RAG v1.0 to create your own AI secretary. Utilizing open-source models from Meta and Microsoft, like `Llama3-7b` and `Phi3-4b`, this guide offers you the chance to enhance your productivity by teaching you how to upload documents and query them with ease. Get started with essential prerequisites including Docker and an optional GPU for optimal performance. Follow our simple installation process to choose the right data model and execute your AI assistant. Finally, access your AI assistant effortlessly with a final setup, allowing you to enjoy the fruits of AI technology directly from your desktop.","Learn how to create your own AI-secretary using Basdonax AI RAG v1.0, leveraging `Llama3-7b` and `Phi3-4b` models. This guide covers from installation requirements to accessing your AI assistant, making AI technology easily accessible and practical for everyday tasks.",AI Coding Assistant,"Python





        69





        36


        Built by

          





        7 stars today",,https://www.youtube.com/watch?v=ZyBBv1JmnWQ,69,2024-04-26T21:22:09Z
2024-05-03,https://github.com/rougier/numpy-100,https://raw.githubusercontent.com/rougier/numpy-100/master/README.md,"The text introduces a collection of 100 numpy exercises compiled from various sources including the numpy mailing list, Stack Overflow, numpy documentation, and problems created by the author to make up the number. It is aimed at providing a quick reference for numpy users at all levels, and also as a resource for educators. The collection can be accessed and tested on Binder or read on GitHub. It is generated from source data using a script and is under the MIT license. Additionally, there is mention of a similar collection for Julia, called ""100 Julia Exercises.""",Master Numpy with these 100 Exercises for Python Developers,"Dive into the ultimate collection of 100 Numpy exercises compiled from various sources including the Numpy mailing list, Stack Overflow, and official Numpy documentation. Designed to be a comprehensive guide for both beginners and experienced users, this collection not only serves as a quick reference but also as an interactive learning tool. Through a mix of problems sourced and created, this compilation aims to enhance your Numpy skills effectively. Extend your learning by exploring 'From Python to Numpy' for more in-depth exercises. Don't miss out on testing your knowledge directly on Binder or browsing through on GitHub.","Discover the perfect way to hone your Numpy skills with our collection of 100 exercises. Ideal for both new and seasoned Python developers, this guide is your roadmap to mastering Numpy.",Python Libraries Collection,"Python





        11,549





        5,511


        Built by

          









        21 stars today",,,11549,2014-05-27T04:09:47Z
2024-05-03,https://github.com/e2b-dev/code-interpreter,https://raw.githubusercontent.com/e2b-dev/code-interpreter/main/README.md,"E2B's Code Interpreter SDK offers capabilities to integrate code interpreting functionalities into AI applications. Hosted in the secure, open-source E2B Sandbox, it supports running untrusted AI-generated code and AI agents across any LLM and AI framework. Key features include support for Python and JavaScript/TypeScript, compatibility with serverless and edge functions, and the ability to handle streaming content. Installation is straightforward with npm for JavaScript/TypeScript and pip for Python, followed by simple code snippets for executing code within the sandbox environment. It includes a Quickstart guide, Hello World tutorials, and a variety of example use cases and supported AI frameworks in its cookbook. Currently, it supports Python with plans to soon include JavaScript/TypeScript.",Enhance Your AI Apps with E2B's Code Interpreter SDK,"Discover how E2B's Code Interpreter SDK revolutionizes AI app development by providing easy code interpreting capabilities inside a secure sandbox. This SDK, fully open-source, supports a wide range of AI frameworks and languages, including Python and JavaScript/TypeScript. Stream content effortlessly, run AI-generated code safely, and embark on a quickstart journey with practical guides and examples. Upgrade your AI development with E2B's comprehensive coding solution.","E2B's Code Interpreter SDK offers seamless integration for AI app development with support for multiple languages, secure sandbox environments, and comprehensive guides. Learn more about enhancing your AI projects today.",AI Development Platform,"Python





        449





        25


        Built by

          









        130 stars today",,,449,2024-03-11T22:08:25Z
2024-05-03,https://github.com/dai-motoki/zoltraak,https://raw.githubusercontent.com/dai-motoki/zoltraak/main/README.md,"Zoltraak represents the dawn of a new era of magic by introducing a powerful creation magic system that elevates unorganized spells into a structured magical language, enabling swift and broad spell casting and potent magic activation. Set to become a norm in magic, Zoltraak owes its name to the basic attack magic in Freylen's arsenal. It's an advanced system capable of converting spell constructs rapidly into ancient system languages for instant execution, enhancing both the range and power of magic dramatically. The system also promises flexibility and speed in casting, making it possible to outmaneuver opponents significantly.

The latest release, version 0.1.27, improves on the file writing automation process, showcasing Zoltraakâ€™s capability in translating natural language prompts into executable programming scripts. The project welcomes contributions and has seen participation from various developers who have contributed to its coding, documentation, and other aspects.

Zoltraak utilizes a prompt compiler system, allowing for the expansion of complex magical formulas from simple spell prompts, essentially enabling the encoding of powerful magic incantations through streamlined spell casting mechanics. Future plans include compiling into unique language spells and encryption to safeguard against reverse-engineering. 

It's distributed through Zoltraak Docker and Streamlit, featuring deployment instructions for Windows use, including setting up Python, acquiring an ANTHROPIC API key, and activating the virtual environment for Zoltraak's utilization. It encourages participation in its development and outlines how contributors can add to the project efficiently.",Unleashing Creativity with Zoltraak: The Dawn of a New Magical Era,"Discover Zoltraak, a formidable system of creation magic marking the beginning of a true age of magic. It elevates unstructured spells into a systematic framework, enabling rapid and extensive magical deployments. Zoltraak is set to become a fundamental part of mainstream magic, offering spellcasters an unparalleled ability to cast powerful spells swiftly and with great range. The name Zoltraak pays homage to the general attack magic in Frieren, symbolizing innovation in magical practices.","Explore Zoltraak â€“ a revolutionary creation magic system heralding the new age of magic. Learn how it transforms spellcasting into a rapid, wide-reaching force, setting the stage for future magical practices.",Game Development Tool,"Python





        159





        22


        Built by

          









        12 stars today",https://raw.githubusercontent.com/dai-motoki/zoltraak/main/assets/videos/zoltraak2_smooth_high_quality.gif; https://raw.githubusercontent.com/dai-motoki/zoltraak/main/assets/images/cast_zoltraak.png; https://raw.githubusercontent.com/dai-motoki/zoltraak/main/assets/videos/zoltraak0.1.27_high_quality.gif; https://raw.githubusercontent.com/dai-motoki/zoltraak/main/assets/images/last_frame.png; https://raw.githubusercontent.com/dai-motoki/zoltraak/main/assets/images/llmcomment.png; https://raw.githubusercontent.com/dai-motoki/zoltraak/main/assets/images//graph.png,,159,2024-04-27T13:46:24Z
2024-05-03,https://github.com/microsoft/PubSec-Info-Assistant,https://raw.githubusercontent.com/microsoft/PubSec-Info-Assistant/main/README.md,"As of November 15, 2023, Azure Cognitive Search is now Azure AI Search, and Azure Cognitive Services are Azure AI Services. The Information Assistant (IA) Accelerator integrates Azure and OpenAI's large language models, utilizing Azure AI Search for data retrieval and ChatGPT-style interactions. It employs the Retrieval Augmented Generation (RAG) design, enabling natural language responses based on user queries. It supports multiple response generation approaches, including grounded, ungrounded, and web-sourced responses, with customization options for AI interactions. The accelerator requires an Azure account and specific Azure OpenAI model access. Deployment involves Azure account setup, permissions, and accepting Azure AI Services Responsible AI Notice. It emphasizes responsible AI use, including transparency and content safety measures. Users must be aware of data collection practices, which they can opt out of. The documentation provides comprehensive guides on deployment, usage, and navigating the source code for further customization and development.",Maximizing AI Potential: Integrating Azure AI Search with ChatGPT for Intelligent Solutions,"Discover how the Information Assistant Accelerator, now leveraging Azure AI Search, transforms data retrieval and ChatGPT-style Q&A interactions. Unveiling a seamless integration with OpenAI's large language models, this accelerator offers a retrieval augmented generation design, ensuring precise, multilingual data handling and customizable AI interactions. Dive into its advanced features, from explainable AI to personalized settings, and watch our video to explore practical use cases.","Explore the Information Assistant Accelerator's latest integration with Azure AI Search and ChatGPT, offering enhanced data retrieval, personalized AI interactions, and a multitude of innovative features for building intelligent solutions.",Collaborative AI Framework,No specific star count found,https://raw.githubusercontent.com/microsoft/PubSec-Info-Assistant/main/docs/process_flow_chat.png; https://raw.githubusercontent.com/microsoft/PubSec-Info-Assistant/main/docs/process_flow_agent.png,,230,2023-02-14T14:28:14Z
2024-05-03,https://github.com/ToTheBeginning/PuLID,https://raw.githubusercontent.com/ToTheBeginning/PuLID/main/README.md,"PuLID, standing for Pure and Lightning ID Customization via Contrastive Alignment, is a project developed by Zinan Guo, Yanze Wu (the corresponding author), Zhuowei Chen, Lang Chen, and Qian He at ByteDance Inc. It focuses on enhancing ID customization through contrastive alignment techniques. The project has made its resources, including the code and models, available to the public alongside a demo on the HuggingFace platform as of May 1, 2024, following the release of its arXiv paper on April 25, 2024. PuLID facilitates easy deployment and experimentation through its requirement of Python 3.7 or higher, PyTorch 2.0 or above, and its installation guidance provided for setting up a working environment. For quick demonstrations, a local Gradio demo and an online HuggingFace demo are offered. The creators encourage the academic community to cite their work if it proves helpful for research and invite feedback and queries via GitHub issues or direct contact with Yanze Wu and Zinan Guo.",Introducing PuLID: Revolutionizing ID Customization with Contrastive Alignment,"Discover PuLID, ByteDance's innovative approach to ID customization, which leverages contrastive alignment for enhanced personalization and efficiency. Recently released on arXiv and featuring a hands-on HuggingFace demo, PuLID offers cutting-edge technology for quick and tailored identity solutions. With its latest updates and easy installation, PuLID is poised to redefine the norms of ID customization.","Explore PuLID by ByteDance: A groundbreaking ID customization tool using contrastive alignment, now available with an arXiv paper and a HuggingFace demo. Learn how it redefines personalized technology.",Deep Learning Tool,"Python





        358





        13


        Built by

          





        36 stars today",,,358,2024-04-17T04:04:57Z
2024-05-03,https://github.com/xcapt0/gpt2_chatbot,https://raw.githubusercontent.com/xcapt0/gpt2_chatbot/master/README.md,"The GPT-2 Chatbot is designed for daily conversations and is trained on multiple dialogue datasets including `Daily Dialogue`, `Empathetic Dialogues`, `PERSONA-CHAT`, and `Blended Skill Talk`. It is built on the GPT2 Model transformer architecture. Users can download the model from AWS S3 storage and set it up using Docker. The chatbot operates in two modes: `train` for training the model and `interact` for engaging in conversations. Training and interaction require specifying the model's checkpoint path. The project, developed by xcapt0, is available under an MIT license, indicating it is open for modification and distribution.",Revolutionize Your Daily Conversations with GPT-2 Chatbot,"Discover how the GPT-2 chatbot, trained on diverse datasets like Daily Dialogue and PERSONA-CHAT, can transform your daily interactions. This innovative chatbot, built on the powerful GPT2 model, offers personalized and empathetic conversations. Installation is a breeze with a downloadable model from AWS S3, and users can choose between train and interact modes for customization. Dive into the future of daily dialogue with the easy-to-use GPT-2 chatbot.","Elevate your conversations with the GPT-2 chatbot, a user-friendly tool trained on extensive datasets for meaningful interactions. Download now and experience the future of chatbots.",Language Models,"Python





        50





        11


        Built by

          





        4 stars today",https://user-images.githubusercontent.com/70326958/151570518-ce70261a-6e8e-47a0-92e5-2d7638e7aa68.jpg,,50,2021-12-29T10:56:49Z
2024-05-04,https://github.com/isocpp/CppCoreGuidelines,https://raw.githubusercontent.com/isocpp/CppCoreGuidelines/master/README.md,"The C++ Core Guidelines are a set of recommendations led by Bjarne Stroustrup to facilitate the use of modern C++ (C++11 and newer) effectively and safely. These guidelines, resulting from extensive collaboration and discussion, aim to help coders produce code that is safer, simpler, and faster by focusing on higher-level issues such as interfaces, resource management, and concurrency. They are designed to evolve over time, adapting to new understandings and changes in the language. The document is available in a simple, MarkDown format to support ease of use and automatic processing. Additionally, the guidelines encourage the use of a Guidelines Support Library for implementation and are open for contributions and suggestions for improvement. Hosted by DigitalOcean, these rules are intended to be incrementally adopted and supported by analysis tools to ease integration and compliance.",Mastering Modern C++: An Insight into the C++ Core Guidelines,"Discover the essence of modern C++ with the C++ Core Guidelines, a collaborative effort led by Bjarne Stroustrup aimed at safer and more effective coding practices. These guidelines serve as a blueprint for developing robust C++ code for future projects, emphasizing safety, simplicity, and performance. Ideal for both new and experienced programmers, they are continuously updated to incorporate the latest in coding standards and practices. Dive into this evolving document to shape your coding skills for the better, ensuring your code is type-safe, efficient, and leak-free.","Explore the C++ Core Guidelines, a comprehensive guide by Bjarne Stroustrup and collaborators for writing safer, simpler, and faster C++ code. Perfect for modern C++ programming.",System Design Education,"Python





        41,613





        5,369


        Built by

          









        21 stars today",https://raw.githubusercontent.com/isocpp/CppCoreGuidelines/master/cpp_core_guidelines_logo_text.png,,41613,2015-08-19T20:22:52Z
2024-05-04,https://github.com/pypa/pipx,https://raw.githubusercontent.com/pypa/pipx/main/README.md,"pipx is a tool for installing and running Python applications in isolated environments, similar to brew for macOS, npx for JavaScript, and apt for Linux. It uses pip for package management but focuses on applications that can be run from the command line, providing environment isolation for each installed application to prevent dependency conflicts. Pipx can install programs from PyPI as well as other sources like git URLs or local directories. It supports macOS, Linux, and Windows, offering installation through methods like Homebrew, Scoop, and pip. Users can utilize pipx to install applications globally, upgrade them, and uninstall them. It also allows running applications in temporary environments with the `run` command for one-time use without permanent installation. This makes pipx a secure and convenient way to manage command-line tools in Python without risking dependency conflicts or pollution of global package space. Additionally, it offers features like shell completions and the ability to inject additional packages into existing pipx-managed environments.",Maximizing Python Efficiency: The Power of pipx for Isolated Environment Management,"Discover the prowess of pipx, a dynamic tool for installing and executing Python applications in isolated environments, ensuring a clutter-free workspace. Unlike traditional methods that risk dependency conflicts, pipx maintains each application in its dedicated space, leveraging the vast repository of PyPI. Learn how to effortlessly install, manage, and execute Python applications with pipx, all while keeping your system secure and your dependencies harmonized. Upgrade your Python toolkit today with the simplicity and reliability of pipx.","Learn how to use pipx for installing and running Python applications in isolated environments, avoiding dependency conflicts and system clutter. Discover tips for efficient Python application management with pipx.",Python Libraries Collection,"Python





        8,935





        374


        Built by

          









        22 stars today",https://github.com/pypa/pipx/raw/main/pipx_demo.gif,,8935,2018-10-06T18:47:46Z
2024-05-04,https://github.com/netbox-community/netbox,https://raw.githubusercontent.com/netbox-community/netbox/master/README.md,"NetBox, since its 2016 release, has established itself as a critical tool for network engineers worldwide, providing an advanced solution for the modeling and documentation of network infrastructure. It offers a unified, extensive data model and APIs for managing everything network-related, from cable layouts to device configurations, positioning itself as the essential source of truth for modern networking. Designed to function as the authoritative center for network state information, NetBox supports robust automation systems by making detailed network data programmatically available, without directly interfacing with network nodes. It is built with a comprehensive data model that includes all components of network infrastructure, supports focused development on making network infrastructure programmatically accessible, and allows extensive customization to meet specific network requirements. NetBox also features a flexible permission system, custom validation and protection rules for network data, configuration rendering using Jinja2 templates, the ability to automate workflows through custom scripts and event rules, and comprehensive change logging. Additionally, it encourages community involvement, offering a platform for users to contribute, suggest features, or report bugs.",NetBox: Revolutionizing Network Automation and Documentation,"Since its inception in 2016, NetBox has emerged as a pivotal tool for network engineers worldwide, setting a new standard for modeling and documenting network infrastructure. It surpasses legacy IPAM and DCIM solutions by offering a comprehensive data model and a singular interface for all network-related tasks. With its robust user interface and programmable APIs, NetBox functions as the ultimate source of truth in modern network management, facilitating seamless integration with automation and monitoring tools. This transformative approach not only enhances accuracy and efficiency but also propels organizations towards fully automated network operations.","Discover how NetBox, the leading network modeling and documentation platform since 2016, is revolutionizing network infrastructure management with its comprehensive data model and integrative APIs.",Network Performance Monitor,"Python





        14,995





        2,426


        Built by

          









        8 stars today",https://raw.githubusercontent.com/netbox-community/netbox/master/docs/media/screenshots/home-light.png; https://raw.githubusercontent.com/netbox-community/netbox/master/docs/media/misc/reference_architecture.png; https://raw.githubusercontent.com/netbox-community/netbox/master/docs/media/misc/netbox_cloud.png; https://raw.githubusercontent.com/netbox-community/netbox/master/docs/media/screenshots/home-light.png; https://raw.githubusercontent.com/netbox-community/netbox/master/docs/media/screenshots/home-dark.png; https://raw.githubusercontent.com/netbox-community/netbox/master/docs/media/screenshots/prefixes-list.png; https://raw.githubusercontent.com/netbox-community/netbox/master/docs/media/screenshots/rack.png; https://raw.githubusercontent.com/netbox-community/netbox/master/docs/media/screenshots/cable-trace.png,,14995,2016-02-29T14:15:46Z
2024-05-04,https://github.com/florestefano1975/ComfyUI-HiDiffusion,https://raw.githubusercontent.com/florestefano1975/ComfyUI-HiDiffusion/main/README.md,"ComfyUI-HiDiffusion introduces a custom node leveraging HiDiffusion technology, still in development and testing phases. Users are warned of potential bugs and advised to test in a separate environment to avoid issues, noting that the project lacks a requirements file to prevent dependency conflicts. Users must manually resolve any missing dependencies based on console errors. The SDXL node has been updated to include numerous options like checkpoint naming, optimizations, and scheduler settings. An example showcases a high-resolution image generated with specific model parameters. Resources and further information are available through links to the HiDiffusion project, repository, and related documentation. Additionally, references to other projects and a donation link for support are provided.",Exploring ComfyUI-HiDiffusion: A Leap in HiDiffusion Technology Integration,"Discover the cutting-edge ComfyUI-HiDiffusion custom node integration for HiDiffusion technology, designed to enhance your tech experience with advanced options in the SDXL model, including ckpt_name and apply_raunet. However, tread carefully as the project is still in testing, posing risks of bugs and malfunctions. Important to note, the removal of the requirements.txt file necessitates manual dependency installations for smooth operation. This development aims at refining and expanding the functionality of ComfyUI nodes, promising significant advancements in technology utilization.","Unveil the potential of ComfyUI-HiDiffusion in technology integration, a project under development bringing advanced options and cautions against possible malfunctions. Learn more about this intriguing venture.",Custom Node Pack,"Python





        70





        5


        Built by

          





        13 stars today",https://raw.githubusercontent.com/florestefano1975/ComfyUI-HiDiffusion/main/assets/overview.png; https://raw.githubusercontent.com/florestefano1975/ComfyUI-HiDiffusion/main/assets/sdxl-2.png; https://raw.githubusercontent.com/florestefano1975/ComfyUI-HiDiffusion/main/assets/sdxl-3.png; https://raw.githubusercontent.com/florestefano1975/ComfyUI-HiDiffusion/main/assets/comfyui-hidiffusion-4096x2304.jpg,,70,2024-04-27T15:45:35Z
2024-05-04,https://github.com/zgimszhd61/prompt-collection-quickstart,https://raw.githubusercontent.com/zgimszhd61/prompt-collection-quickstart/main/README.md,"The text outlines various templates for organizing, summarizing, and analyzing information, especially focusing on creating concise summaries, extracting prompts for easy understanding by Chinese readers, synthesizing answers from provided text, categorizing customer service inquiries, determining the accuracy of solutions to questions, and giving hints without revealing answers directly. It includes instructions for grouping arguments, summarizing texts into shorter forms or bullet points, answering questions based on provided documents, categorizing customer service inquiries into main and subcategories, evaluating solutions, and providing feedback without direct answers.",Maximizing Your Arguments: Strategies for Effective Debate,"Understanding and dissecting arguments is crucial in any debate. Grouping pro and con cases for each argument allows for clearer analysis and presentation. Doing so helps in effectively conveying your stance and addressing counter-arguments. This method ensures a comprehensive understanding and robust defense of your points, leading to a more persuasive argument.","Discover how to enhance your debating skills by organizing arguments into pro and con cases for a clear, persuasive presentation. Learn strategies for effective argumentation.",Natural Language Processing,"Python





        178





        12


        Built by

          





        23 stars today",,,178,2024-04-28T04:42:18Z
2024-05-04,https://github.com/Nixtla/neuralforecast,https://raw.githubusercontent.com/Nixtla/neuralforecast/main/README.md,"Neural Forecast by Nixtla offers a user-friendly collection of state-of-the-art neural forecasting models, aiming to improve the accuracy and efficiency of forecasting pipelines. The library includes a wide range of models from RNNs to transformers and supports features like exogenous variables, interpretability, and probabilistic forecasting. Nixtla created NeuralForecast to address the challenges of using neural networks in forecasting, such as their complexity and computational demands, by focusing on efficient and accurate implementations. The library provides easy installation, a quick start guide, extensive documentation, and features automatic model selection with hyperparameter tuning. It also offers integration with related forecasting tools and the possibility to add new models. NeuralForecast is designed for both beginners and experts in forecasting, emphasizing usability and performance. Contributions are welcome, and the project appreciates the work of all contributors and the influence of preceding scholarly works on neural forecasting methods.",Leveraging NeuralForecast for Cutting-Edge Time Series Prediction,"Discover NeuralForecast, the top-tier library for neural forecasting models, designed to enhance the accuracy and efficiency of your forecasting pipeline. With user-friendly models like RNN, LSTM, and GRU, alongside advanced transformers, NeuralForecast is built for performance and ease of use. Whether you're implementing classic networks or exploring state-of-the-art models, this library supports a wide array of functionalities including exogenous variables and probabilistic forecasting. Dive into the world of advanced forecasting with NeuralForecast today.","Explore NeuralForecast for state-of-the-art neural forecasting models like RNN, LSTM, and advanced transformers. Enhance your forecasting with high performance, usability, and robust features. Start now.",Deep Learning Tool,"Python





        2,469





        286


        Built by

          









        16 stars today",https://raw.githubusercontent.com/Nixtla/neuralforecast/main/nbs/imgs_indx/logo_new.png,,2469,2021-04-26T00:15:19Z
2024-05-04,https://github.com/quixio/quix-streams,https://raw.githubusercontent.com/quixio/quix-streams/main/README.md,"Quix Streams is a cloud-native Python library for processing data in Kafka, focusing on simplicity and integration with the Python ecosystem. It eliminates the need for JVM, orchestrators, or server-side engines, offering a Stream DataFrame API for easy data transformation, akin to pandas. Quix Streams supports JSON and Quix-specific serialization, stateful operations via RocksDB, and various aggregation methods, ensuring ""at-least-once"" processing guarantees. Designed for resilience and scalability, it can run locally or in containers, integrating seamlessly with Quix Cloud for enhanced features like auto-configuration and monitoring. The documentation provides installation instructions, example applications, and additional resources for getting started, including tutorials on specific use cases. Active development continues to add new features, and the community is encouraged to contribute. Quix Streams is Apache 2.0 licensed, with a supportive Slack community and social media presence for updates and support.",Leveraging Real-Time Data with Quix Streams: A Revolutionary Python Library for Kafka,"Discover Quix Streams, a Python library maximizing real-time data processing in Kafka without compromising simplicity. Engineered for both beginners and seasoned developers, it offers a seamless experience with intuitive Pythonic interfaces, robust support for stateful operations, and seamless integration with cloud ecosystems. Its lightweight design ensures efficient data management, enabling sophisticated data transformation and analysis on-the-fly. Perfect for machine learning, AI, and real-time analytics projects, Quix Streams is transforming how developers approach stream processing in Python.","Explore Quix Streams, the cloud-native Python library for Kafka, designed for effortless real-time data processing. Dive into its Pythonic simplicity and robust features for modern data-driven applications.",Data Ingestion Tool,"Python





        764





        34


        Built by

          









        18 stars today",https://github.com/quixio/quix-streams/blob/main/images/quixstreams-banner.jpg,,764,2022-11-17T20:36:03Z
2024-05-04,https://github.com/Shubhamsaboo/awesome-llm-apps,https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/README.md,"The text introduces ""Awesome LLM Apps,"" a curated collection of applications utilizing Large Language Models (LLMs) like RAG from OpenAI, Anthropic, Gemini, and open-source options. It details several projects, including a local chat app that interacts with webpages using Llama-3 and RAG, a Gmail assistant, a Substack newsletter chat using GPT-4, a PDF document interaction tool, and a YouTube video query app. Each app enhances user interaction with digital content through natural language processing. Instructions for getting started with these projects involve cloning the repository, navigating to a project directory, installing dependencies, and following project-specific setup instructions. The text encourages contributions to the collection, asking for pull requests that adhere to the project's structure and include a detailed README.md for any new app additions.",Discover the Latest in AI: Top LLM Apps for Enhanced Productivity,"Explore our curated list of innovative LLM applications, from chatting with webpage content to interactive Gmail management, all harnessing the power of models like RAG, OpenAI, and more. These apps, including Streamlit and local AI solutions, offer 100% offline capabilities, transforming how you interact with digital content. Whether it's engaging with newsletters, PDFs, or YouTube videos through natural queries, these tools are set to revolutionize your digital interaction. Get started by cloning our repository and dive into a world where technology meets convenience.","Unlock the potential of AI with our guide to the best LLM apps. From local webpage chats to interactive Gmail, PDFs, and YouTube content, discover tools that change how you engage with the digital world.",Large-scale Language Models,"Python





        176





        15


        Built by

          





        16 stars today",https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind.png,,177,2024-04-29T05:30:25Z
2024-05-04,https://github.com/SuperpoweredAI/spRAG,https://raw.githubusercontent.com/SuperpoweredAI/spRAG/main/README.md,"spRAG is an advanced RAG (Retrieval-Augmented Generation) framework designed for unstructured data, improving upon traditional RAG systems. It shows exceptional performance in handling complex queries across dense texts such as financial reports, legal documents, and academic papers, significantly outperforming vanilla RAG baselines. For example, in the FinanceBench benchmark, spRAG accurately answers questions 83% of the time compared to the 19% accuracy of vanilla RAG baselines. 

The framework enhances performance through two key features: AutoContext and Relevant Segment Extraction (RSE). AutoContext enriches text chunks with document-level context before embedding, leading to more accurate retrieval of information. RSE, a post-processing step, combines relevant chunks into comprehensive segments for better context, proving particularly effective for complex queries.

spRAG can be easily installed via Python package and configured for use with various models and APIs like OpenAI, Anthropic, and Cohere. It also supports basic customization allowing users to adapt it according to their requirements. The architecture of spRAG includes components such as KnowledgeBase, VectorDB, ChunkDB, Embedding, Reranker, and LLM, all of which are customizable. The community can engage with spRAG's development and seek support through its Discord.",Revolutionizing Data Analysis with spRAG: The Future of RAG Frameworks,"Discover how spRAG, a cutting-edge RAG framework for unstructured data, is transforming the way we handle dense text queries in finance, law, and academia. By remarkably outperforming traditional RAG systems in complex question-answering tasks, spRAG utilizes innovative methods like AutoContext and Relevant Segment Extraction to achieve higher accuracy and relevance. Its ability to intelligently contextualize and segment data sets a new benchmark in data processing, making it an essential tool for professionals and researchers alike. Learn how to easily integrate spRAG into your projects and customize it to your needs, significantly improving information retrieval and analysis.","Explore the advancements spRAG brings to processing unstructured data with unparalleled accuracy in dense text analysis. Learn how it leverages AutoContext and Relevant Segment Extraction to transform data handling in finance, law, and academia.",Deep Learning Tool,"Python





        301





        10


        Built by

          






        81 stars today",,,301,2024-04-17T19:56:08Z
2024-05-05,https://github.com/Blealtan/efficient-kan,https://raw.githubusercontent.com/Blealtan/efficient-kan/master/README.md,"This repository offers an efficient version of the Kolmogorov-Arnold Network (KAN), addressing performance issues found in the original implementation by optimizing the way activation functions are handled. Instead of expanding intermediate variables, it utilizes a reformulation involving B-splines for activation, streamlining computations through straightforward matrix multiplication, beneficial for both forward and backward processes. The updated implementation modifies the sparsification approach for better interpretability by applying L1 regularization on weights rather than inputs, a change necessitated by compatibility with the computational reformulation. Additional enhancements include optional learnable scales for activation functions to potentially improve results. The update on May 4, 2024, introduced a new initialization technique for certain parameters, significantly improving performance on the MNIST dataset. Further experimentation is necessary to validate these modifications and their impacts on efficiency and accuracy.",Revolutionizing Neural Networks: Efficient Kolmogorov-Arnold Network Implementation,"Discover the latest advancement in neural network technology with an efficient implementation of the Kolmogorov-Arnold Network (KAN). This new approach significantly reduces memory costs by reformulating computations for activation functions, leading to straightforward matrix multiplication that enhances both forwards and backward pass efficiency. Challenges in sparsification and regularization are addressed by replacing L1 regularization on input samples with weight regularization, compatible with the new computational model. Initial experiments, including promising results on the MNIST dataset with reinitialized weight parameters, suggest a promising direction, though further experimentation is necessary.","Explore an efficient implementation of Kolmogorov-Arnold Network that optimizes memory use and computational efficiency for neural networks, with insights into regularization modifications and promising preliminary results.",Deep Learning Tool,"Python





        395





        25


        Built by

          





        68 stars today",,,395,2024-05-02T14:19:28Z
2024-05-05,https://github.com/PromtEngineer/localGPT,https://raw.githubusercontent.com/PromtEngineer/localGPT/main/README.md,"LocalGPT is an open-source platform for secure interactions with documents, maintaining privacy by operating entirely locally on a user's computer. It integrates a variety of open-source models and allows for seamless data embedding, alongside features like chat history and a user-friendly API for building RAG applications. It offers support for various computing platforms including GPU, CPU, and MPS. The technical infrastructure leverages the LangChain tool for document parsing and Chroma for storing vectorized document data. Setup involves cloning the repository, creating a virtual environment, and installing dependencies. LocalGPT offers detailed documentation and a graphical interface for ease of use. Users can ingest their own data, chat with documents, and customize LocalGPT for different large language models (LLMs). It requires a specific Python version, a C++ compiler for certain installations, and outlines GPU/VRAM requirements for different model sizes. The project, inspired by the privateGPT initiative, emphasizes user privacy and local data processing.",Introducing LocalGPT: Enhance Your Privacy with Local Document Conversations,"Discover LocalGPT, the open-source platform that transforms the way you interact with your documents, securely and privately on your own computer. With LocalGPT, your data never leaves your digital boundaries, offering you the ultimate privacy and security. Embrace the power of local processing with a variety of support for open-source models and enjoy features like seamless API integration, diverse embeddings, and graphical interfaces without compromising your data privacy. Get started easily with our pre-configured virtual machine and unlock the future of document interaction today.","Explore LocalGPT, the innovative tool for secure, private conversations with your documents. Enjoy ultimate data protection, open-source model support, and easy setup. Dive into privacy-first document interactions now.",AI Development Platform,"Python





        19,210





        2,136


        Built by

          









        13 stars today",,https://www.youtube.com/watch?v=MlyoObdIHyo; https://www.youtube.com/watch?v=lbFmceo4D5E; https://www.youtube.com/watch?v=d7otIM_MCZs; https://www.youtube.com/watch?v=G_prHSKX9d4,19210,2023-05-24T05:32:40Z
2024-05-05,https://github.com/eureka-research/DrEureka,https://raw.githubusercontent.com/eureka-research/DrEureka/main/README.md,"DrEureka introduces a novel approach for transferring robot skills from simulation to the real world using Large Language Models (LLMs) to automate sim-to-real transfers. This methodology significantly streamlines the process by eliminating the need for manual design and tuning of task reward functions and simulation physics parameters. The team demonstrates DrEureka's efficacy through competitive results on tasks such as quadruped locomotion and dexterous manipulation, and its ability to tackle new challenges like balancing and walking on a yoga ball without manual iteration. The project includes code to generate rewards, configure domain randomization, and manage deployment infrastructure, all supported by comprehensive documentation. The system is built to be modular and easily configurable for rapid development and deployment of robotic skills. DrEureka, which primarily runs on PyTorch and is tested on Ubuntu 20.04, is open-sourced under the MIT License, encouraging further advancements and applications in the field.",Revolutionizing Robotics: DrEureka's LLM-Guided Sim-to-Real Transfer,"Discover how DrEureka leverages Large Language Models (LLMs) to streamline sim-to-real robotics tasks, significantly reducing human labor and speeding up the process. By automating the design of reward functions and domain randomization, DrEureka not only matches but potentially surpasses manually-designed configurations in complex tasks like quadruped locomotion and dexterous manipulation. This groundbreaking approach also shows promise in novel tasks, such as balancing and walking on a yoga ball, opening new doors for robotics applications.","Explore DrEureka's innovative approach to robot skills acquisition, using Large Language Models for efficient sim-to-real transfer in robotics. Learn how it automates reward function design and domain randomization for advanced tasks.",AI Development Platform,"Python





        290





        17


        Built by

          





        67 stars today",,,290,2024-05-03T07:40:01Z
2024-05-05,https://github.com/apple/ml-stable-diffusion,https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/README.md,"This repository provides tools to run Stable Diffusion, a state-of-the-art image generation model, on Apple Silicon devices using Core ML. It contains a Python package (`python_coreml_stable_diffusion`) for converting PyTorch models into Core ML format and generating images with Hugging Face diffusers in Python. Additionally, a Swift package (`StableDiffusion`) is available for integrating image generation capabilities into Xcode projects, relying on Core ML model files produced by the Python package. Users are advised to check system requirements, including macOS, Python, coremltools versions, and target device specifications, before starting. Performance benchmarks for various devices and configurations are provided, showcasing latency and diffusion speed. Advanced weight compression techniques supported by coremltools for optimizing model performance on mobile devices are discussed, including 6-bit or higher compression and Mixed-Bit Palettization (MBP) for further optimization. The repository also covers model conversion for the XL version of Stable Diffusion, usage of ControlNet for guided image generation, and leveraging the System Multilingual Text Encoder for multilingual image generation. Ready-made Core ML models from Hugging Face Hub are available for easy use.",Harnessing Apple Silicon for Advanced ML: Core ML Stable Diffusion Implementation Guide,"Learn how to deploy Stable Diffusion on Apple Silicon using Core ML to leverage advanced image generation capabilities in your applications. This guide covers converting PyTorch models to Core ML format with python_coreml_stable_diffusion and integrating Stable Diffusion into Xcode projects with Swift. Ensure compatibility by reviewing the system requirements for model conversion, project build, and target device runtime.","An essential guide on deploying Stable Diffusion with Core ML on Apple Silicon, covering system requirements, model conversion to Core ML, and integration into Swift apps for next-gen image generation.",Image Generation Platform,"Python





        16,219





        864


        Built by

          









        38 stars today",https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/readme_reel.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/palette6_cpuandne_readmereel.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/float16_cpuandne_readmereel.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/float16_gpu_readmereel.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/mbp/a_high_quality_photo_of_a_surfing_dog.7667.final_3.41-bits.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/mbp/a_high_quality_photo_of_a_surfing_dog.7667.final_4.50-bits.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/mbp/a_high_quality_photo_of_a_surfing_dog.7667.final_6.55-bits.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/mbp/a_high_quality_photo_of_a_surfing_dog.7667.final_float16_original.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/mbp/stabilityai_stable-diffusion-xl-base-1.0_psnr_vs_size.png; https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/controlnet_readme_reel.png,,16219,2022-11-16T00:48:18Z
2024-05-05,https://github.com/karpathy/makemore,https://raw.githubusercontent.com/karpathy/makemore/master/README.md,"Makemore is a simple, hackable tool designed for educational purposes, which generates text based on input files through an autoregressive character-level language model. It can create unique, nonexistent items like baby names, company names, or English-like words when fed with a relevant database. Its simplicity is highlighted by being a single file application with PyTorch as its sole requirement. The tool supports various models, from bigrams to Transformers, aligning with methodologies from several key research papers. To use, one simply feeds it a text file such as the included `names.txt`, specifying input and output through simple command-line instructions. Training doesn't need special hardware, and the system can produce results on a range of devices, even enhancing speed with GPU support. Sample names generated exhibit uniqueness and creativity, showcasing makemore's capability for generating inventive and plausible names. The software is available under the MIT license.",Exploring makemore: The Ultimate Tool for Generating Unique Names,"Discover the power of makemore, a versatile autoregressive language model for generating original names, whether for babies, companies, or just for fun. Utilizing advanced techniques from bigrams to Transformers, this lightweight, educational tool requires only PyTorch. Perfect for those looking to dive into machine learning applications without the complexity, makemore simplifies the process of creating new, English-like terms or names. Its implementation spans across several key models, offering insights into the workings of neural networks. Get started with just a text file and let makemore inspire you with unique name ideas.","Learn how makemore uses machine learning from bigrams to Transformers for generating unique names. Ideal for educational purposes, this tool only needs PyTorch. Dive into our blog to discover its simplicity and potential.",Language Models Tool,"Python





        2,093





        536


        Built by

          





        2 stars today",,,2093,2022-06-09T19:29:36Z
2024-05-05,https://github.com/edgedb/edgedb,https://raw.githubusercontent.com/edgedb/edgedb/master/README.md,"EdgeDB is presented as an innovative graph-relational database amalgamating the best features of relational databases, graph databases, and ORMs, introducing a fresh concept for database design. It employs types and object-oriented schema over traditional tables, aiming to align more intuitively with modern programming languages and eliminating the need for foreign keys or tabular data modeling. EdgeDB introduces EdgeQL, a powerful query language designed to fetch complex, nested data structures efficiently and without the complexities of SQL JOINs. This database system supports a broad range of functionalities including a strict type system, indexes, constraints, computed properties, and stored procedures, while also incorporating modern features like link properties and advanced JSON support. Beyond merely serving as an ORM alternative, EdgeDB promises a comprehensive developer experience with easy-to-use migration systems, client libraries, command-line tools, and forthcoming cloud hosting solutions. Enthusiasts and developers can get involved or start learning through various resources including a quick start guide, cloud service, interactive tutorial, comprehensive e-book, and detailed documentation. EdgeDB encourages community contributions and offers an open-source Apache 2.0 licensed codebase.",Exploring EdgeDB: The Future of Graph-Relational Databases,"EdgeDB is redefining database architecture by merging the best features of relational, graph databases, and ORMs into a single, innovative graph-relational model. It introduces a schema-first approach with object types instead of tables, making data modeling more intuitive and compatible with modern programming languages. EdgeDB's powerful query language, EdgeQL, eliminates the need for JOINs by enabling rich, structured object queries and compositions. Beyond a mere ORM, EdgeDB provides a comprehensive toolset for database management, including migrations, client libraries, and an upcoming cloud hosting platform. It promises to streamline every aspect of database usage for developers, from modeling and migration to querying and management.","Discover EdgeDB, the cutting-edge graph-relational database combining the power of relational and graph databases with ORM's ease, aimed at simplifying database management and queries for developers.",Database System,"Python





        12,521





        386


        Built by

          









        62 stars today",https://www.edgedb.com/github_banner.png,,12521,2017-06-29T20:30:48Z
2024-05-05,https://github.com/Datalux/Osintgram,https://raw.githubusercontent.com/Datalux/Osintgram/master/README.md,"Osintgram is an open-source OSINT (Open Source Intelligence) tool designed for performing reconnaissance on Instagram profiles. It allows users to collect, analyze, and retrieve information from Instagram accounts using an interactive shell that operates via a nickname. The tool is capable of extracting various types of data, including addresses, captions, comments, followers, following, emails, phone numbers, hashtags, likes, media types, photos, profile pictures, stories, and tagged users. It is specified for educational purposes only, with a strict warning against using personal or primary accounts due to potential security risks. Osintgram supports command line operations and offers a Docker container for easy deployment. The tool is available in both a stable and a development version, with the latter containing the latest features and fixes. Updates are managed through Git, and the community is encouraged to contribute to its development. Installation involves cloning the repository, setting up a virtual environment, installing dependencies, and configuring credentials.",Mastering Osintgram: An Ultimate Guide to Instagram OSINT Tools,"Discover the power of Osintgram, the ultimate OSINT tool for Instagram, designed to help you conduct detailed reconnaissance and analysis on any Instagram account. From retrieving followers and comments to downloading stories and identifying user interactions, Osintgram equips you with an interactive shell for comprehensive Instagram account investigations. Ensure you use it responsibly and avoid your primary account to steer clear of potential security issues. With continuous updates and a supportive community, dive into the depths of Instagram like never before.","Explore how Osintgram, the leading OSINT tool for Instagram, can transform your investigative and analytical capabilities on the platform with our in-depth guide.",Cybersecurity Tool,"Python





        8,800





        1,885


        Built by

          









        12 stars today",https://raw.githubusercontent.com/Datalux/Osintgram/master/.img/carbon.png,,8800,2019-06-07T23:23:40Z
2024-05-05,https://github.com/prometheus-eval/prometheus-eval,https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/README.md,"Prometheus-Eval is an advanced repository aimed at evaluating Large Language Models (LLMs) in generation tasks. It recently released its Prometheus 2 models (7B & 8x7B versions), with the latter offering state-of-the-art evaluation capabilities, including improved performance over its predecessors and support for both absolute grading and pairwise ranking. These models demonstrate high correlation and agreement with human judgments across various benchmarks. The Prometheus 2 (7B) version, targeted at users with limited hardware, delivers substantial performance with lower resource requirements. The repository offers tools for training, evaluating, and utilizing models for internal evaluation pipelines, promoting fairness, controllability, and affordability in LLM evaluation. The provided `prometheus-eval` Python package facilitates easy application of these models, supporting both direct individual responses evaluation and comparison between two responses. Additionally, the initiative underscores its commitment to openness and community support by encouraging contributions and feedback.",Unveiling Prometheus 2: A New Era of Language Model Evaluation,"Discover the revolutionary Prometheus-Eval, a cutting-edge tool designed for evaluating language models in generation tasks. Released in May 2024, Prometheus 2 introduces open-source evaluator models (7B & 8x7B) surpassing its predecessor with improved performances and support for both direct assessment and pairwise ranking formats. With a Pearson correlation ranging from 0.6 to 0.7 with GPT-4 and high agreement rates with human judgments across multiple benchmarks, Prometheus 2 represents a significant advancement in language model evaluation, providing an affordable, controllable, and fair alternative to proprietary evaluations.","Explore Prometheus-Eval, an innovative repository for evaluating language models in generation tasks. Learn about the latest Prometheus 2 models that offer enhanced evaluation accuracy and support for various grading formats.",Large-scale Language Models,"Python





        191





        12


        Built by

          







        38 stars today",https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/assets/logo.png; https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/assets/finegrained_eval.png; https://raw.githubusercontent.com/prometheus-eval/prometheus-eval/main/assets/formats.png,,191,2024-04-18T17:19:09Z
2024-05-05,https://github.com/PSGO/PPPwnGo,https://raw.githubusercontent.com/PSGO/PPPwnGo/main/README.md,"The text outlines instructions for using a specific tool to exploit PS4 systems across various firmware versions (FW 9.00 with GoldHEN through FW 11.00), emphasizing the need for a Windows 7 or higher operating system. It sets expectations by stating the tool is a demo with a current low success rate, advising patience. Users are instructed to update ""pppwn.py"" and ""offsets.py"" without needing to update the executable file. The procedure involves installing Python, preparing a payload on a USB drive, connecting the PS4 to a PC via network cable, configuring the PS4's network settings, and running ""PPPwnGo.exe"" to initiate the exploit. If problems occur, users are advised to repeat steps or restart both the PS4 and PC if multiple failures happen. The text concludes with special thanks to Andy for his contributions.",Ultimate Guide to PS4 Jailbreak: FW 9.00 to 11.00 Exploit Tutorial,"Discover the step-by-step method to jailbreak your PS4, supporting firmware versions from 9.00 to 11.00. This tutorial requires a Windows 7 or newer OS and emphasizes patience due to its varying success rate. Covering everything from preparing your USB drive with the necessary files to connecting your PS4 to your PC for the jailbreak process, it's crucial to follow each step carefully. Remember, if you encounter issues, restarting both your PS4 and PC might be necessary. Special thanks to Andy for his contributions to this guide.","Follow our detailed PS4 jailbreak tutorial for firmware versions 9.00 to 11.00. Learn how to prepare your setup, execute the jailbreak, and troubleshoot common issues. Special thanks to Andy for the guidance.",Cybersecurity Tool,"Python





        78





        3


        Built by

          






        21 stars today",,,78,2024-05-02T02:08:02Z
2024-05-05,https://github.com/NVlabs/DoRA,https://raw.githubusercontent.com/NVlabs/DoRA/main/README.md,"DoRA: Weight-Decomposed Low-Rank Adaptation, presented at ICML 2024, introduces a PyTorch implementation for fine-tuning pre-trained weights with enhanced efficiency and stability. By decomposing weights into magnitude and direction, and specifically updating direction using LoRA, DoRA minimizes trainable parameters without increasing inference overhead. It outperforms LoRA in various tasks, including commonsense reasoning, visual instruction, and image/video-text understanding with models like LLaMA and VL-BART. DoRA is fully integrated into the HuggingFace PEFT package, supporting an array of layers and offering practical benefits in fine-tuning. Experimental results show DoRA's superiority in quality, especially at lower ranks, compared to LoRA. This repository provides detailed code for reproducing the results, highlighting DoRAâ€™s robust performance across commonsense reasoning tasks, significantly improving accuracy over LoRA.",Unveiling DoRA: Pioneering Weight-Decomposed Low-Rank Adaptation for AI Models,"Discover the revolution in AI model fine-tuning with [ICML2024] DoRA: Weight-Decomposed Low-Rank Adaptation, offering a groundbreaking approach by decomposing pre-trained weights into magnitude and direction. This method not only enhances learning capacity and stability over LoRA but also ensures efficiency by minimizing trainable parameters. DoRA shines across various AI tasks, surpassing LoRA in performance without additional inference costs. Explore the full potential of DoRA for commonsense reasoning, visual instruction tuning, and more.","Explore DoRA, the weight-decomposed low-rank adaptation method presented at ICML 2024, enhancing AI model training efficiency and performance without extra inference overhead. Perfect for AI enthusiasts and researchers.",Deep Learning Tool,"Python





        105





        6


        Built by

          






        13 stars today",https://raw.githubusercontent.com/NVlabs/DoRA/main/./imgs/dora.png; https://raw.githubusercontent.com/NVlabs/DoRA/main/./imgs/dora_lora_yoda_emoji.jpg; https://raw.githubusercontent.com/NVlabs/DoRA/main/./imgs/dora_lora_lego.jpeg,,105,2024-04-11T15:55:29Z
2024-05-05,https://github.com/SCLBD/DeepfakeBench,https://raw.githubusercontent.com/SCLBD/DeepfakeBench/main/README.md,"DeepfakeBench, presented at NeurIPS 2023, is a comprehensive benchmark for deepfake detection, introduced by Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, and Baoyuan Wu. This platform supports 34 detectors, including both image and video models, and incorporates state-of-the-art (SoTA) detection methods such as LSDA (CVPR '24) and IID (CVPR '23). It offers an integrated framework for training and evaluation, featuring data preprocessing for efficiency, multi-GPUs training, and extensive evaluation metrics like frame-level AUC and accuracy for fake and real detections. DeepfakeBench aims to address the lack of standardization in deepfake detection by providing a unified platform, consistent data management, and standardized evaluations to foster new technology developments. The benchmark also includes a diverse set of datasets and welcomes contributions from the community for further advancements in deepfake detection methods.",DeepfakeBench-v2: Elevating Deepfake Detection with 34 Detectors and Enhanced Benchmarks,"DeepfakeBench-v2 now supports 34 detection methods, including advanced image and video detectors, providing LMDB for IO efficiency, DDP for multi-GPUs training, and an integrated framework. It introduces standardized evaluations across various metrics, ensuring comprehensive analysis and new insights for deepfake detection technology. This benchmark represents a significant step forward in the fight against deepfakes, offering an extensive toolkit for researchers.","Discover DeepfakeBench-v2's latest enhancements, featuring support for 34 deepfake detection methods, LMDB data preprocessing, multi-GPUs training, and more. Explore the integrated framework and standardized evaluations designed to advance deepfake detection technology.",Deep Learning Tool,"Python





        270





        41


        Built by

          








        14 stars today",https://raw.githubusercontent.com/SCLBD/DeepfakeBench/main/figures/archi.png,,270,2023-06-06T09:29:52Z
2024-05-05,https://github.com/lancedb/lancedb,https://raw.githubusercontent.com/lancedb/lancedb/main/README.md,"LanceDB, an open-source vector-search database, boasts efficient retrieval, filtering, and management of embeddings with persistent storage. It's designed for production-scale vector search without the need for server management, supporting storage, querying, and filtering of vectors, metadata, and multimodal data like text, images, videos, and more. Key features include support for vector similarity search, full-text search, SQL, native Python and JavaScript/TypeScript support, zero-copy, automatic versioning, and GPU support for building vector indexes. LanceDB integrates with various ecosystems and is built in Rust using Lance, an open-source columnar format for ML workloads. Quick start guides are available for JavaScript and Python, showcasing simple code snippets to connect to and interact with LanceDB. Additionally, resources like blogs and tutorials offer insights into leveraging LanceDB for enhanced performance and practical applications like building a Q&A bot.",Leveraging LanceDB for Advanced Vector Search in AI Applications,"Discover how LanceDB, an open-source vector-search database, revolutionizes data retrieval and management for AI applications. With key features like production-scale vector search without server management, multi-modal data support, and ecosystem integrations, LanceDB simplifies the complexities involved in handling vast amounts of embeddings. Its core, crafted in Rust, and the utilization of the columnar format Lance, ensure optimal performance for ML workloads. Dive into the future of data management with LanceDB's seamless support for vector similarity search, full-text search, SQL, and no-code version management.","Explore how LanceDB redefines vector search and data management in AI with serverless production-scale search, multi-modal data handling, and powerful ecosystem integrations. Perfect for developers leveraging AI.",Database System,"Python





        2,879





        186


        Built by

          









        19 stars today",,,2879,2023-02-28T01:15:17Z
2024-05-06,https://github.com/flet-dev/flet,https://raw.githubusercontent.com/flet-dev/flet/main/README.md,"Flet is a framework designed to simplify the creation of real-time web, mobile, and desktop applications without requiring frontend experience. It enables rapid development of apps such as internal tools, dashboards, and prototypes using just Python, with support for other languages planned. Flet offers a simple architecture, eliminating the need for a JavaScript frontend, REST API backend, and other complex infrastructure, allowing developers to build stateful, real-time single-page applications easily. It comes with a built-in web server, facilitates development with any IDE or text editor, and does not require SDKs or multiple dependencies. Powered by Flutter, Flet allows the development of professional-looking apps that can be deployed across all platforms and devices, including as web apps, desktop applications for Windows, macOS, Linux, and mobile apps through PWA or Flet's iOS and Android apps. The documentation provides guides, control references, and sample Python apps to help get started, alongside a community for support and contributions to the project.",Build Apps Fast with Flet: The Ultimate Real-Time App Framework,"Discover Flet, the groundbreaking framework that revolutionizes app development by allowing you to build real-time web, mobile, and desktop applications effortlessly with your favorite programming language. Say goodbye to complex architecture and hello to a streamlined process with Flet's simple, stateful app design. From professional-looking UI powered by Flutter to support for multiple languages and devices, Flet includes everything you need to bring your app ideas to life in minutes, no frontend experience required.","Explore how Flet enables swift creation of real-time web, mobile, and desktop apps with no frontend experience needed. Build with Flutter, deploy anywhere, and develop in your preferred language.",Software Development,"Python





        9,283





        358


        Built by

          









        19 stars today",https://flet.dev/img/docs/getting-started/flet-counter-macos.png; https://flet.dev/img/docs/getting-started/flet-counter-safari.png,,9283,2022-03-24T15:44:12Z
2024-05-06,https://github.com/judahpaul16/gpt-home,https://raw.githubusercontent.com/judahpaul16/gpt-home/main/README.md,"GPT Home turns a Raspberry Pi into a smart home assistant, surpassing Google Nest Hub or Amazon Alexa with features built using the OpenAI API. The setup is designed for those with a Raspberry Pi 4B, utilizing Ubuntu Server as the OS, and includes detailed steps for configuring Wi-Fi, installing dependencies, and setting up the system. Highlighting both necessary and optional componentsâ€”with costs ranging from $102-$198â€”the guide emphasizes the hardware needed and the configurations for Wi-Fi connection using `wpa_supplicant`, system dependencies installation, along with a helpful setup script to automate the installation process. Moreover, it tackles setting up system services, NGINX for reverse proxy, and highlights extensive documentation for further customization and troubleshooting. Contributions from the community are welcomed, and the project is freely distributed under the GNU GPL v3.0 License.",Build Your Own Smart Home Device with Raspberry Pi and OpenAI GPT,"Discover how to create your own smart home assistant using a Raspberry Pi, incorporating the advanced capabilities of OpenAI GPT for a truly personalized experience. This step-by-step guide will show you how to assemble the hardware, configure software dependencies, and integrate OpenAI's GPT technology, transforming your Raspberry Pi into a powerful home assistant. Perfect for tech enthusiasts and DIYers, this project not only enhances your home but also provides a rewarding challenge.","Learn to build a smart home device with Raspberry Pi & OpenAI GPT. Follow our step-by-step guide to create a powerful, personalized home assistant.",AI Development Platform,"Python





        203





        13


        Built by

          





        49 stars today",https://raw.githubusercontent.com/judahpaul16/gpt-home/main/screenshots/my_build.jpg; https://raw.githubusercontent.com/judahpaul16/gpt-home/main/screenshots/schematic_bb.png; https://raw.githubusercontent.com/judahpaul16/gpt-home/main/screenshots/schematic_schem.png,,203,2023-09-05T17:17:22Z
2024-05-06,https://github.com/Efficient-Large-Model/VILA,https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/README.md,"VILA represents a breakthrough in pre-training visual language models (VLMs) by utilizing interleaved image-text data, which significantly enhances video and multi-image understanding capabilities. Pre-trained using a novel approach that includes unfreezing language learning models (LLMs) and re-blending text instruction data, VILA has shown proficiency in video reasoning, in-context learning, visual chain-of-thought, and improved world knowledge. Supporting various model sizes and deployable on the edge using AWQ 4bit quantization and the TinyChat framework, VILA boasts impressive performance across multiple image and video QA benchmarks. It efficiently operates on diverse NVIDIA GPUs, offering considerable speedups in inference times. VILA has been open-sourced, including training and evaluation code, datasets, and model checkpoints, under specific licenses for both code and pre-trained weights, promoting accessibility and further research in visual language processing.",VILA: Revolutionizing Visual Language Models with Pre-training Innovations,"Discover VILA, a state-of-the-art visual language model that empowers video understanding and multi-image interpretation like never before. Utilizing advanced pre-training on interleaved image-text data, VILA excels in video reasoning, in-context learning, and visual chain-of-thought processing. Explore its deployment possibilities with AWQ 4bit quantization for cutting-edge efficiency on diverse platforms. Join us in exploring how VILA's unique pre-training approach and compression techniques redefine the capabilities of visual language models.","Explore VILA, the groundbreaking visual language model pre-trained with interleaved image-text data, offering unparalleled video understanding and multi-image interpretation. Learn about its efficient deployment with AWQ 4bit quantization.",Vision-Language Model,"Python





        449





        32


        Built by

          









        57 stars today",https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_images/vila-logo.jpg; https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_images/demo_img_1.png; https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_images/demo_img_2.png; https://raw.githubusercontent.com/Efficient-Large-Model/VILA/main/demo_images/demo_img_3.png,,449,2024-02-23T09:19:16Z
2024-05-07,https://github.com/NVlabs/RADIO,https://raw.githubusercontent.com/NVlabs/RADIO/main/README.md,"AM-RADIO, presented by NVIDIA Research and set to feature in CVPR 2024, introduces a revolutionary single vision foundation model named RADIO that integrates multiple domains into one framework. This model outperforms several existing models in various metrics by effectively distilling Large Vision Foundation models and integrating CLIP variants, DINOv2, and SAM through distillation. It maintains unique feature capabilities such as text grounding and segmentation correspondence and is efficient and scalable, supporting images of any resolution and aspect ratio. Newly released versions, including RADIOv2.1, offer improved metrics and training in bf16 precision. Available for use through TorchHub and HuggingFace platforms, RADIO and its efficient variant E-RADIO boast significant advancements in throughput and zero-shot learning performance over existing models like OpenAI CLIP and DINOv2. The project is open for business inquiries through NVIDIA Research Licensing, presenting opportunities for further exploration and utilization in the vision and AI research community.",Exploring AM-RADIO: The Next Frontier in Agglomerative Vision Foundation Models,"Discover how AM-RADIO reshapes visual domain models by integrating the prowess of CLIP variants, DINOv2, and SAM into one unified framework, as presented at CVPR 2024. This groundbreaking work by NVIDIA Research not only streamlines domain-specific models but also enhances efficiency and performance in vision-related tasks. Delve into the details of how AM-RADIO outperforms its predecessors in zero-shot, kNN, and segmentation metrics, making it a versatile tool for researchers and developers alike. Explore the potential of this innovative model to revolutionize image processing and analysis across various applications.","Unveil the capabilities of AM-RADIO, an agglomerative vision foundation model presented at CVPR 2024 by NVIDIA Research, improving upon CLIP, DINOv2, and SAM models for superior visual domain performance.",Computer Vision Platform,"Python





        276





        7


        Built by

          








        28 stars today",https://raw.githubusercontent.com/NVlabs/RADIO/main/assets/radio.png; https://raw.githubusercontent.com/NVlabs/RADIO/main/assets/radio_overview_github.png,,276,2023-12-08T19:53:01Z
2024-05-07,https://github.com/huggingface/datatrove,https://raw.githubusercontent.com/huggingface/datatrove/main/README.md,"DataTrove is a scalable library providing tools for processing, filtering, and deduplicating text data efficiently. It includes a variety of prebuilt processing blocks and supports customization to meet specific needs. Designed for large-scale workloads, such as processing training data for Large Language Models (LLMs), DataTrove operates with low memory usage across various platforms, including local setups and slurm clusters. It smoothly integrates with local and remote file systems via fsspec. The framework allows building robust data processing pipelines, customizable with a range of built-in and user-defined blocks for reading, writing, extracting, filtering, and deduplicating data. Pipelines are modular, supporting execution in diverse environments with logging and execution control features to streamline processing tasks. DataTrove is equipped for both individual modules and complex, scalable data processing activities, encouraging contributions and offering guides for setup, execution, and customization.",Maximizing Text Data Processing with DataTrove: Efficient Pipelines for Large-Scale Tasks,"Explore DataTrove, a powerful library designed for processing, filtering, and deduplicating text data on a massive scale. DataTrove offers prebuilt processing blocks and custom functionality, supporting platform-agnostic pipelines across local machines and slurm clusters. With low memory consumption and support for various file systems through fsspec, it's the ideal tool for handling extensive workloads like LLM's training data preparation.","Discover how DataTrove can transform your text data processing tasks with its efficient, scalable pipelines and support for large datasets. Ideal for LLM training data preparation and more.",Data Ingestion Tool,"Python





        1,347





        79


        Built by

          









        7 stars today",,,1347,2023-06-14T12:05:28Z
2024-05-07,https://github.com/dmunozv04/iSponsorBlockTV,https://raw.githubusercontent.com/dmunozv04/iSponsorBlockTV/main/README.md,"iSponsorBlockTV is a project that allows users to skip sponsor segments in YouTube videos on various YouTube TV devices, using asynchronous Python for fast operation. It's compatible with a wide range of devices including Apple TV, Android TV, Chromecast, gaming consoles like Nintendo Switch and PlayStation, and more. To install, users are directed to check the project's wiki, with a note that docker armv7 builds are deprecated. The software functions by connecting to a device on the same network, utilizing the SponsorBlock API to skip or mute sponsor segments and ads. It involves libraries like pyytlounge and aiohttp for device interaction and network requests. Users can contribute via GitHub, and the project is under GNU GPLv3 license. Key contributors include dmunozv04, HaltCatchFire, and Oxixes.",Enhance Your YouTube TV Experience with iSponsorBlockTV: Seamless Ad Skipping,"Say goodbye to intrusive sponsor segments on YouTube TV with iSponsorBlockTV, the innovative project that brings ad-free viewing to various devices. Utilizing asynchronous Python for swift operation, this tool integrates with the SponsorBlock API to skip sponsor segments and ads seamlessly. With broad compatibility across devices like Apple TV, Android TV, and gaming consoles, and simple installation through its wiki, iSponsorBlockTV enhances your streaming experience without hassle.","Discover how iSponsorBlockTV can transform your YouTube TV watching experience by skipping annoying ads and sponsor segments on a wide range of devices. Installation is straightforward, ensuring an uninterrupted viewing journey.",AI Browser Automation,"Python





        1,882





        70


        Built by

          









        725 stars today",https://www.gnu.org/graphics/gplv3-127x51.png,,1882,2022-01-15T18:26:33Z
2024-05-07,https://github.com/ansible/awx,https://raw.githubusercontent.com/ansible/awx/master/README.md,"AWX is a web-based interface, REST API, and task engine leveraging Ansible. It's the upstream project for the Red Hat Ansible Automation Platform. Users interested in installing AWX can find instructions in their Install Guide, and can learn more about the platform through the AWX documentation site or the project's FAQ section. AWX logos and branding assets are protected under trademark guidelines. Community contributions are welcome; guidelines for contributing, including code submission and the sign-off process, are outlined in their Contributing Guide. Issues or suggestions for improvements can be submitted via their issues guide. All community members are expected to adhere to the Ansible Code of Conduct, with contact information provided for assistance. Engagement with the community is encouraged through various channels such as the Ansible AWX channel on Matrix and the Ansible Community Forum.",Maximizing Automation with AWX: An Essential Guide to Ansible's Web UI,"AWX serves as a crucial web-based UI, REST API, and task engine built atop Ansible, streamlining project workflows for Red Hat Ansible Automation Platform users. This comprehensive guide offers insights into installing, utilizing, and contributing to AWX, expanding your automation capabilities. Explore the resources available, including installation instructions, detailed documentation, and community support options. Contribute to its development or seek assistance and share feedback through various community channels. Embrace the full potential of automation by integrating AWX into your development lifecycle.","Explore how AWX, the web-based interface for Ansible, enhances automation efforts. Learn to install, use, and contribute to AWX with our comprehensive guide, and join the vibrant community for support and collaboration.",Open Source ERP,"Python





        13,473





        3,333


        Built by

          









        8 stars today",,,13473,2017-05-17T15:50:14Z
2024-05-07,https://github.com/RevoltSecurities/Subdominator,https://raw.githubusercontent.com/RevoltSecurities/Subdominator/main/README.md,"Subdominator is a potent subdomain enumeration tool essential for cybersecurity professionals during reconnaissance and bug hunting. It utilizes 35+ passive sources for subdomain enumeration, offering features such as configurable API keys, an integrated notification system, and support for various output formats. Its usage is straightforward, requiring simple command lines for operations, and it supports extensive customizations for optimizing searches, including proxy use and source selection. The tool integrates with numerous free API sources, and for maximal efficiency, some sources need API key setups. Installation is via Python and pip, and post-installation, users can configure API keys through a specified YAML file setup. Subdominator emphasizes security, promising user safety in operations and updates. It encourages community contributions and improvement suggestions.",Maximize Bug Hunting Success with Subdominator: A Subdomain Enumeration Tool,"Discover the capabilities of Subdominator, an essential tool for cybersecurity experts and bug hunters. By providing efficient and comprehensive subdomain enumeration, it aids in identifying potential security threats. Subdominator leverages over 35 free passive resources, offering configurable API keys setup and an integrated notification system for streamlined operations. Perfect for those seeking to enhance their reconnaissance efforts in cybersecurity.",Learn how Subdominator can transform your bug hunting and cybersecurity reconnaissance by efficiently enumerating subdomains using 35+ passive resources. A must-have tool for security professionals.,Cybersecurity Tool,"Python





        216





        32


        Built by

          





        22 stars today",,,216,2023-07-24T15:23:44Z
2024-05-08,https://github.com/huggingface/lerobot,https://raw.githubusercontent.com/huggingface/lerobot/main/README.md,"LeRobot, developed by Hugging Face, focuses on lowering the entry barrier to the field of robotics. It is a comprehensive library designed for real-world robotics applications, leveraging PyTorch for machine learning models. The library includes pretrained models, human-collected dataset demonstrations, and simulation environments, eliminating the need for users to assemble physical robots. It incorporates state-of-the-art techniques in imitation and reinforcement learning, promising easy transferability to real-world scenarios. With an aim to expand its support for affordable and capable robots, LeRobot encourages sharing and collaboration within the community. The repository offers thorough guidance for installation, utilization of pretrained models, visualization of datasets, and evaluation of policies. It also provides a framework for training new policies with detailed instructions and supports experiment tracking with Weights and Biases. Acknowledgments extend to contributors of various policies, environments, and datasets adapted within LeRobot, highlighting a collaborative effort to advance robotics research and accessibility. The library not only facilitates robotics research but also invites contributions to expand its capabilities, including adding new datasets or pretrained policies to the Hugging Face hub.",Introducing LeRobot: Revolutionizing Robotics with Machine Learning,"LeRobot, powered by Hugging Face, is set to transform the robotics landscape by offering state-of-the-art machine learning models, datasets, and tools designed for real-world applications. Aimed at lowering the barrier to entry, it encourages contributions and sharing within the community. With a focus on imitation and reinforcement learning, LeRobot provides everything needed to get started, including pretrained models and simulation environments. It's not just about making advanced robotics more accessible; it's about building a community around shared innovation and progress.","Discover how LeRobot by Hugging Face is making advanced robotics accessible to all through community-driven sharing of models, datasets, and tools, focusing on real-world applications.",Collaborative AI Framework,"Python





        1,618





        85


        Built by

          









        321 stars today",https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png; http://remicadene.com/assets/gif/aloha_act.gif; http://remicadene.com/assets/gif/simxarm_tdmpc.gif; http://remicadene.com/assets/gif/pusht_diffusion.gif; https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png,,1619,2024-01-26T15:50:41Z
2024-05-08,https://github.com/hbb1/2d-gaussian-splatting,https://raw.githubusercontent.com/hbb1/2d-gaussian-splatting/main/README.md,"The official implementation of ""2D Gaussian Splatting for Geometrically Accurate Radiance Fields"" is presented, focusing on representing scenes with 2D oriented disks for accurate surfel rasterization and enhanced reconstruction quality through developed regularizations and meshing approaches. Key updates include support for unbounded mesh extraction using adaptive TSDF truncation. The provided installation and training guides facilitate the use of COLMAP or NeRF Synthetic datasets, with specific parameter adjustments recommended for different scene scales. Testing instructions cover both bounded and unbounded mesh extraction. The project also offers evaluation scripts for novel view synthesis and geometric reconstruction accuracy, built upon notable repositories like 3DGS and Open3D. Acknowledgments highlight contributions from related works, encouraging citations for academic use.",Exploring 2D Gaussian Splatting for Realistic Radiance Fields in Computer Graphics,Discover the groundbreaking technique of 2D Gaussian Splatting for enhancing geometrically accurate radiance fields in computer graphics. This advanced method uses 2D oriented disks to accurately represent scenes and incorporates perspective-correct differentiable rasterization for improved scene reconstruction. Learn how recent updates like unbounded mesh extraction and adaptive TSDF truncation are setting new standards in the field. Perfect for developers and researchers in 3D computer graphics seeking cutting-edge rendering solutions.,"Unveil the potential of 2D Gaussian Splatting for creating geometrically accurate radiance fields, featuring updates like unbounded mesh extraction and adaptive TSDF truncation for superior 3D scene reconstruction.",3D Image Rendering,"Python





        752





        20


        Built by

          






        136 stars today",https://raw.githubusercontent.com/hbb1/2d-gaussian-splatting/main/assets/teaser.jpg; https://raw.githubusercontent.com/hbb1/2d-gaussian-splatting/main/assets/unbounded.gif,https://www.youtube.com/watch?v=oaHCtB6yiKU,753,2024-04-27T05:28:40Z
2024-05-08,https://github.com/OpenDevin/OpenDevin,https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/README.md,"OpenDevin is an AI and LLM-powered platform designed to assist software engineers in coding, debugging, and feature development. It fosters collaboration between agents and human developers, aiming to enhance the efficiency of software engineering tasks. Users can quickly start with OpenDevin using Docker and its documentation offers comprehensive guides on utilizing different LLM providers, troubleshooting, and advanced configurations. The project encourages community contributions in various forms, including code development, research, evaluation, and feedback. OpenDevin has a thriving community on Slack and Discord, inviting discussions on research, architecture, and development. It is a community-driven project open to contributions and aims at improving software engineering through AI. The platform is licensed under the MIT License, ensuring wide accessibility and collaborative improvement.",Maximizing Productivity with OpenDevin: AI-Powered Software Development,"Discover OpenDevin, the cutting-edge platform transforming software development through AI and Machine Learning Language Models (LLMs). OpenDevin's unique system enables autonomous collaboration between humans and AI agents, streamlining code writing, debugging, and feature deployment. With Docker support and an inclusive community backing, OpenDevin is revolutionizing how developers approach software projects. Join us to explore how you can enhance your coding efficiency and contribute to the forefront of AI in software engineering.","Explore OpenDevin, the AI and LLM-powered platform that's changing software development. Learn how it fosters collaboration between AI agents and developers to code faster and more efficiently.",AI Development Platform,"Python





        24,390





        2,694


        Built by

          









        224 stars today",https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/./docs/static/img/logo.png; https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/./docs/static/img/screenshot.png; https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/docs/static/img/results.png,,24390,2024-03-13T03:33:31Z
2024-05-08,https://github.com/Kroery/DiffMOT,https://raw.githubusercontent.com/Kroery/DiffMOT/main/README.md,"DiffMOT, presented at CVPR 2024, introduces a real-time, diffusion-based system for tracking multiple objects with non-linear prediction capabilities, significantly improving tracking performance across various datasets. It incorporates trained motion models and is compatible with several detectors, showcasing flexibility within different application scenarios. The framework's efficacy is demonstrated through benchmarks on DanceTrack, SportsMOT, MOT17, and MOT20 datasets, with notable improvements in tracking accuracy and speed. The tracker achieves optimal results when paired with different YOLOX detectors, balancing accuracy and frame rate to suit user needs. Installation and usage instructions are detailed for users to set up the system, which involves preparing data, downloading pre-trained models, and executing tracking. The project acknowledges contributions from related works and provides thorough documentation to assist users. Contact information is provided for further inquiries, emphasizing collaboration and support within the research community.",Revolutionizing Object Tracking: Explore DiffMOT's Real-time Capabilities,"DiffMOT, introduced at CVPR 2024, sets a new standard for real-time, diffusion-based multiple object tracking with its innovative non-linear prediction model. The breakthrough framework boasts impressive tracking performance across diverse benchmarks like DanceTrack and SportsMOT. It integrates seamlessly with different detectors, achieving up to 30.3 FPS on an RTX 3090 GPU, ensuring flexibility for various application scenarios. The research team has also made their trained motion model publicly available, promising significant advancements in the field of computer vision. This milestone highlights their commitment to pushing the boundaries of object tracking technology.","Discover DiffMOT, the cutting-edge real-time, diffusion-based multiple object tracker from CVPR 2024. Learn how it achieves top performance with non-linear prediction on benchmarks like DanceTrack and SportsMOT.",Computer Vision Platform,"Python





        213





        26


        Built by

          






        50 stars today",https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/teaser_git.png; https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/diffmot_git.png; https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/ddmp_git.png; https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/DiffMOT_DanceTrack.gif; https://raw.githubusercontent.com/Kroery/DiffMOT/main/assets/DiffMOT_SportsMOT.gif,,213,2023-11-23T11:07:14Z
2024-05-08,https://github.com/deepseek-ai/DeepSeek-Coder,https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/README.md,"DeepSeek Coder is a comprehensive suite of state-of-the-art code language models, trainable in various sizes ranging from 1B to 33B. It uses a vast dataset of 2T tokens, primarily code (87%) with English and Chinese natural language (13%), optimizing for project-level code completion and infilling via a 16K window size and a unique fill-in-the-blank task. The models excel in performance across multiple programming languages and standards, outperforming other open-source alternatives in benchmarks like HumanEval, MultiPL-E, MBPP, DS-1000, and APPS. Additionally, it supports a wide range of programming languages, from common ones like Python and Java to less mainstream ones. The creation process involved collecting and filtering GitHub code, parsing dependencies, and concatenating files for quality training data. DeepSeek Coder's training capitalized on an initial and extended pre-training phase, followed by instruction fine-tuning. Users can interact with DeepSeek Coder through code completion, insertion, chat model inference, and repository level code completion, with mentioned examples illustrating its advanced capabilities. For further customization, DeepSeek offers a fine-tuning script supporting training on user-specific tasks with deep learning acceleration technologies. The detailed evaluation results provided showcase its versatility and superior performance across a spectrum of coding tasks and benchmarks, asserting its position as a leading solution in the realm of code intelligence and model training.",Unleashing the Power of DeepSeek Coder: AI-Driven Code Solutions,"Discover the revolutionary DeepSeek Coder, your AI-powered coding assistant designed with a massive 2T token dataset for unparalleled coding assistance in multiple languages. Boasting superior model sizes up to 33B, DeepSeek Coder excels at project-level code completion and infilling, making it a leader among open-source code models. Its remarkable training on a blend of code and natural language ensures it meets diverse coding requirements with ease, from basic code completions to advanced project-level challenges. Enhance your coding projects with state-of-the-art performance across various benchmarks.","Explore DeepSeek Coder, an advanced AI coding assistant trained on 2T tokens for superior code completion and infilling in English and Chinese. Achieve state-of-the-art coding performance with models up to 33B.",Language Models Tool,"Python





        5,444





        367


        Built by

          









        42 stars today",https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/logo.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/home.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/result.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/table.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/data_clean.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/model_pretraining.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/completion_demo.gif; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/HumanEval.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/MBPP.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/DS-1000.png; https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Coder/main/pictures/Math.png,,5444,2023-10-20T06:38:01Z
2024-05-08,https://github.com/instructlab/instructlab,https://raw.githubusercontent.com/instructlab/instructlab/main/README.md,"InstructLab CLI is a tool developed for enhancing Large Language Models (LLMs) with synthetic data-based alignment tuning, focusing on chatbots. The CLI facilitates downloading a pre-trained LLM, interacting with it, and adding new knowledge or skills through a companion taxonomy repository. Following this, users can generate synthetic training data, re-train the LLM, and chat with the updated model to see improvements. The CLI is designed to be accessible on Linux and macOS systems, requiring Python 3.9+, a C++ compiler, and about 60GB of disk space. The documentation guides users through installing `ilab`, initializing it, downloading the model, serving it, and iterating on the model by adding new knowledge, generating synthetic datasets, and re-training. InstructLab emphasizes community contributions, allowing users to contribute new knowledge or skills back to the project. The process, though optimized for modest hardware, offers options for higher-fidelity results with more advanced setups.",Unlocking the Power of Synthetic Data with InstructLab CLI: A Comprehensive Guide,"Discover the cutting-edge InstructLab CLI, a revolutionary tool designed for enhancing Large Language Models (LLMs) with synthetic data-based alignment tuning. Learn how to install, initialize, and utilize `ilab` for downloading pre-trained models, generating new synthetic training data, and retraining LLMs to integrate new knowledge and skills. With InstructLab, users can leverage modest hardware to achieve remarkable fidelity in model training, paving the way for more sophisticated language applications.","Explore how InstructLab CLI leverages synthetic data to fine-tune Large Language Models (LLMs) on any hardware. This guide covers installation, initialization, and how to use `ilab` for model enhancement.",AI Development Platform,"Python





        275





        94


        Built by

          









        107 stars today",,,275,2024-02-21T22:59:01Z
2024-05-08,https://github.com/NVIDIA/Megatron-LM,https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/README.md,"The Megatron repository consists of two core components, Megatron-LM and Megatron-Core, designed for optimizing transformer model training at scale on GPUs. Megatron-LM is research-oriented, providing a foundation for large language model (LLM) training leveraging the GPU-optimized techniques of Megatron-Core. Megatron-Core is a library that introduces advanced system-level optimizations and modular APIs for efficient, customizable transformer model training. It supports all NVIDIA Tensor Core GPUs, including the latest with FP8 acceleration. This toolset allows for training of models ranging from billions to a trillion parameters, demonstrating near-linear scaling across thousands of GPUs. It integrates with NVIDIA's NeMo Framework for end-to-end, cloud-native solutions and supports advanced parallelization, activation checkpointing, and distributed optimization for peak performance and efficiency. Pretrained models are available for immediate use or fine-tuning, and the documentation provides guidance on training, data preprocessing, and evaluation for various models like BERT, GPT, and T5 within a distributed architecture.",NVIDIA's Breakthrough in AI: Megatron-Core for Optimized Transformer Models,"NVIDIA has revolutionized the AI landscape with the release of Megatron-Core, enhancing the capabilities of Megatron-LM through GPU-optimized techniques for training transformer models at an unprecedented scale. This advancement offers developers and researchers modular and composable APIs, scaling up system-level optimizations for efficient, large language model training. Compatible with NVIDIA Tensor Core GPUs, Megatron-Core supports the most advanced model and data parallelism techniques. Dive into the future of AI with NVIDIA's latest innovation, enabling the construction of sophisticated models for a wide range of applications.","Unlock the full potential of transformer model training with NVIDIAâ€™s Megatron-Core. Explore cutting-edge system-level optimizations and GPU-optimized techniques for scalable, efficient AI development.",Large-scale Language Models,"Python





        8,710





        1,948


        Built by

          









        15 stars today",https://raw.githubusercontent.com/NVIDIA/Megatron-LM/main/images/Achieved_petaFLOPs.png,,8710,2019-03-21T16:15:52Z
2024-05-09,https://github.com/AdityaNG/kan-gpt,https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/README.md,"KAN-GPT is a PyTorch implementation that utilizes Generative Pre-trained Transformers (GPTs) with Kolmogorov-Arnold Networks (KANs) for efficient language modeling. Available for installation via PyPI, it offers an easy setup for both usage and development, including detailed steps for setting up the environment, installing dependencies, and preparing data. It provides a user-friendly interface for training and deploying the model with support for different configurations and datasets like WebText and Tiny Shakespeare. Early results show that KAN-GPT slightly outperforms its MLP-GPT counterpart on selected metrics. The project integrates tools and scripts for dataset downloading, training, prompting the model, and results visualization, alongside planned improvements and extensive documentation for contributors.",Exploring KAN-GPT: A Novel Approach in Language Modeling with PyTorch,"Discover the cutting-edge KAN-GPT, a PyTorch-based generative pre-trained transformer model leveraging Kolmogorov-Arnold Networks (KANs) for unparalleled language modeling. This blog sheds light on how to install, use, and develop the KAN-GPT model, alongside insights into its superior performance over traditional MLP models on datasets like Tiny Shakespeare. Dive into the technical architecture, setup instructions, and use cases that set KAN-GPT apart as a significant advancement in natural language processing.","Learn about KAN-GPT, the breakthrough in language modeling using PyTorch and Kolmogorov-Arnold Networks. Find out how it excels over traditional models with a guide on installation, usage, and development.",Deep Learning,"Python





        394





        25


        Built by

          







        86 stars today",https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_loss.png; https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_cross_entropy.png; https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_perplexity.png,,394,2024-05-02T15:41:42Z
2024-05-09,https://github.com/wilsonfreitas/awesome-quant,https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/README.md,"This text provides a comprehensive list of libraries, packages, and resources for quantitative finance, available across various programming languages like Python, R, Matlab, Julia, Java, JavaScript, Haskell, Scala, Ruby, Golang, CPP (C++), CSharp, and Rust. For Python, it highlights numerical libraries and data structures, financial instruments and pricing, indicators, trading, backtesting, and risk analysis tools. Similar tools and libraries are listed for other languages, each suited for different aspects of quantitative finance such as data analysis, trading algorithms, market data retrieval, financial modeling, and strategy backtesting. The list also includes frameworks for several languages, enhancing quantitative analysis and trading strategies development. Additionally, it mentions resources for Excel integration and data visualization, ensuring a wide array of tools for quants to conduct financial analysis, develop trading systems, and manage portfolios efficiently.","Ultimate Guide to Quantitative Finance Tools: Libraries, Packages, and Resources","Discover the ultimate curated list of insanely awesome libraries, packages, and resources for Quants in this comprehensive guide. Covering a wide range of programming languages like Python, R, Matlab, and more, this article is your gateway to numerical libraries, financial instruments pricing, trading strategies, and risk analysis tools. Dive into the world of Quantitative Finance with essential tools like Python's NumPy for scientific computing, pandas for data analysis, QuantLib for financial instruments and pricing, and much more. Whether you're a seasoned finance professional or new to quantitative analysis, this guide offers valuable insights and tools to enhance your financial modeling and analytical skills.","Explore the best libraries, packages, and resources for quantitative finance across Python, R, Matlab, and more. From scientific computing, data analysis to financial modeling, get started with essential tools for your quantitative finance journey.",Finance Programming Resources,"Python





        16,144





        2,428


        Built by

          









        35 stars today",,,16144,2015-09-30T09:53:49Z
2024-05-09,https://github.com/mouredev/hello-git,https://raw.githubusercontent.com/mouredev/hello-git/main/README.md,"CÃ©sar Tolentino from Peru shares his excitement about a comprehensive Git & GitHub course offered by MoureDev, praising its coverage from basics to advanced concepts, including Laravel or mobile development tips with Java or Kotlin, and architecture best practices. The course spans 5 hours and 45 lessons, teaching Git essentials, over 25 commands, GitHub from scratch, integration of Git with GitHub, collaborative workflows, and includes practical examples. Additionally, a new 300-page guidebook to complement the video course is highlighted. The course is designed for beginners and aims to provide a thorough understanding of Git & GitHub, offering a blend of theory and practice to equip learners with the necessary skills for version control and collaboration.",Master Git & GitHub: The Ultimate Guide for Beginners,"Discover the comprehensive 5-hour course for mastering Git and GitHub from scratch designed for beginners. Learn everything from Git basics and essential commands to GitHub integration and collaborative workflows, all through practical examples and a community-supported learning environment. This course also includes access to a detailed theoretical-practical guide to deepen your understanding. Join the MoureDev community on Twitch for live sessions and become proficient in Git and GitHub to boost your development projects.","Kickstart your Git & GitHub journey with our beginner-friendly course. Learn the fundamentals, commands, and GitHub collaboration in just 5 hours with practical examples. Join our community for ongoing support.",System Design Education,"Python





        7,178





        2,192


        Built by

          









        24 stars today",https://raw.githubusercontent.com/mouredev/hello-git/main/./Media/header.jpg; https://raw.githubusercontent.com/mouredev/hello-git/main/./Media/book.jpg; http://i3.ytimg.com/vi/3GymExBkKjE/maxresdefault.jpg; http://i3.ytimg.com/vi/pNtcTmCiXzw/maxresdefault.jpg; https://raw.githubusercontent.com/mouredev/hello-git/main/./Media/terminal.gif; https://raw.githubusercontent.com/mouredev/mouredev/master/mouredev_emote.png,https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=3GymExBkKjE; https://www.youtube.com/watch?v=pNtcTmCiXzw; https://www.youtube.com/watch?v=pNtcTmCiXzw,7178,2023-01-04T09:01:03Z
2024-05-10,https://github.com/lllyasviel/IC-Light,https://raw.githubusercontent.com/lllyasviel/IC-Light/main/README.md,"IC-Light is a project focusing on changing the lighting in images, known as ""Imposing Consistent Light"". It offers two models: one conditioned on text and the other on background, both targeting the enhancement of foreground images. Users can start with provided scripts for either the text-conditioned or background-conditioned relighting model, with automated model downloads. The text-conditioned model outputs vary based on lighting preferences and prompts, showcasing a wide range of lighting effects from neon to natural sunlight. The project demonstrates the consistent relighting capability of the models, even allowing the combination of relights into normal maps without direct training on them. This is shown through experiments that highlight the models' ability to produce consistent and high-quality lighting adjustments. IC-Light is open for use and further exploration, with its codebase and models available on GitHub, complemented by examples and detailed instructions for replication. The project contributes to the broader field of image manipulation, with references to related works on portrait relighting and background replacement.",Unveiling IC-Light: Revolutionize Image Illumination with Advanced Relighting Models,"IC-Light stands at the forefront of image manipulation, specializing in advanced relighting models that adapt illumination with unmatched consistency. The project introduces two groundbreaking models: text-conditioned and background-conditioned relighting, catering to diverse lighting preferences using foreground images. Its seamless integration through a simple script setup promises to elevate visuals in a hyper-realistic manner. IC-Light not only enhances images but invites users to experiment with its capabilities, ensuring each output maintains the highest fidelity to natural light conditions. Embark on a journey of visual transformation with IC-Light, where every image is a canvas for illumination perfection.","Discover IC-Light, the cutting-edge project transforming image illumination through text-conditioned and background-conditioned relighting models. Elevate your visuals with consistent, hyper-realistic lighting conditions.",Image Generation Platform,"Python





        1,781





        71


        Built by

          





        351 stars today",,,1781,2024-05-07T13:26:05Z
2024-05-10,https://github.com/google-research/timesfm,https://raw.githubusercontent.com/google-research/timesfm/master/README.md,"TimesFM, developed by Google Research, is a pretrained model for time-series forecasting, set to be discussed at ICML 2024. Its code and checkpoints are available on Hugging Face. The model, not officially backed by Google, specializes in univariate forecasting up to 512 timepoints for any horizon length, requiring contiguous context of the same frequency. It mainly generates point forecasts, with experimental quantile forecasts uncalibrated post-pretraining. Installation involves creating a conda environment for either GPU or CPU setups, followed by package installation. Usage includes model initialization with specific parameters and performing inference on array inputs or pandas dataframes, accommodating various time series frequencies.",Unlocking the Future of Time-Series Forecasting with Google's TimesFM,"Discover TimesFM, the latest innovation in time-series forecasting by Google Research, designed to revolutionize how we predict future trends. This model highlights the seamless integration of cutting-edge technology for univariate time-series forecasting, supporting context lengths up to 512 timepoints without the need for probabilistic forecasts. Dive deep into its capabilities, including uncalibrated quantile heads and the requirement for contiguous context, by exploring the comprehensive benchmarks and practical applications. Learn how to seamlessly install and utilize TimesFM for both GPU and CPU setups, ensuring you can leverage this powerful tool regardless of your hardware preferences. Understand the impact of TimesFM on the future of forecasting by visiting the official Google Research blog and accessing the model through the Hugging Face checkpoint repository.","Explore TimesFM, Google's innovative foundation model for accurate time series forecasting, including setup instructions, benchmarks, and practical applications for advanced forecasting challenges.",Time Series Forecasting,"Python





        855





        33


        Built by

          







        178 stars today",,,856,2024-04-29T21:26:26Z
2024-05-10,https://github.com/QwenLM/Qwen,https://raw.githubusercontent.com/QwenLM/Qwen/main/README.md,"The text details the release and features of the Qwen series, including base language models and chat models in various sizes (1.8B, 7B, 14B, 72B) with a focus on multilingual data, emphasizing Chinese and English. The models have been pretrained on up to 3 trillion tokens, achieving competitive performance on benchmark datasets. Introduced are models' quantization strategies for efficient memory usage and speed, including Int4 and Int8 versions. It highlights the utility of models in tasks like chatting, content creation, summarization, translation, coding, and solving math problems. The text mentions the support for system prompt enhancement and tool use in models. It provides technical details for finetuning, including LoRA and Q-LoRA techniques, and deployment on CPUs and multi-GPU setups. The availability of a technical report with detailed model performance and the hosting of models on platforms like ModelScope and Hugging Face for easy access and inference are also mentioned. Finally, it offers guidance on getting started, including environmental setup, quickstart examples, and troubleshooting assistance through FAQs.",Exploring Qwen Series: A Deep Dive into Multilingual Language Models,"Discover the power of Qwen series language models, including Qwen-1.8B, Qwen-7B, Qwen-14B, and Qwen-72B, which have shown exceptional performance across a range of benchmark datasets. These models have been pretrained on up to 3 trillion tokens covering multiple languages and domains, demonstrating their versatility in tasks like chatting, content creation, summarization, translation, and more. With detailed quantization and fine-tuning instructions, the Qwen series stands out for its robust training, inference efficiency, and multilingual capabilities, marking a significant advancement in AI language processing.","Learn about the Qwen series AI multilingual language models, including their capabilities, performances on benchmarks, and advanced features like quantization and fine-tuning for efficient AI language processing across various tasks.",Large-scale Language Models,"Python





        11,285





        907


        Built by

          









        56 stars today",https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg; https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/radar_72b.jpg; https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/web_demo.gif; https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/cli_demo.gif; https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/openai_api.gif; https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/system_prompt_language_style.png; https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/system_prompt_role_play_en.png; https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/code_interpreter_showcase_001.jpg; https://raw.githubusercontent.com/QwenLM/Qwen/main/assets/qwen_72b_needle_in_a_haystack.png,,11285,2023-08-03T04:56:38Z
2024-05-10,https://github.com/JaveleyQAQ/WeChatOpenDevTools-Python,https://raw.githubusercontent.com/JaveleyQAQ/WeChatOpenDevTools-Python/main/README.md,"This document details a library intended strictly for learning purposes, disclaiming any responsibility by the developers for issues that may arise from its use. If the library infringes on any rights, contact for removal is advised. It outlines the compatibility of various WeChat versions with this tool across Windows and Mac platforms, offering detailed guides on how to check the current WeChat version running on a system, and procedures for utilizing the features like enabling F12 for mini-programs and the WeChat built-in browser. Additionally, it addresses common issues such as incompatibility messages, inability to modify the Chinese language, and version rollback queries, providing solutions and suggesting precautions. For Mac users, it notes specific limitations and steps to avoid crashes when debugging mini-programs, also highlighting where to find assistance for process access issues.",Maximizing Development with WeChatOpenDevTools-Python: A Guide,"Dive into the depths of WeChatOpenDevTools-Python, a pivotal tool for developers seeking to enhance their WeChat applications. This tool, initially birthed from the need to automate mundane tasks and now fully rewritten in Python3, promises a streamlined workflow. Discover how to leverage this tool for opening developer tools in WeChat, thereby significantly improving development efficiency. Whether you're troubleshooting or pushing the envelope on innovation, this guide illuminates the path to mastering WeChat's developmental capabilities. Note: This tool is for educational purposes only, and the developer disclaims responsibility for any misuse.",Explore WeChatOpenDevTools-Python for advanced WeChat app development. Learn how to access F12 developer tools in WeChat and enhance your development process. Use responsibly.,Open Source Tool,"Python





        946





        310


        Built by

          






        35 stars today",https://raw.githubusercontent.com/JaveleyQAQ/WeChatOpenDevTools-Python/main/./docs/images/version0.jpg; https://raw.githubusercontent.com/JaveleyQAQ/WeChatOpenDevTools-Python/main/./docs/images/version1.jpg; https://raw.githubusercontent.com/JaveleyQAQ/WeChatOpenDevTools-Python/main/./docs/images/version2.jpg; https://raw.githubusercontent.com/JaveleyQAQ/WeChatOpenDevTools-Python/main/./docs/images/run.jpg; https://raw.githubusercontent.com/JaveleyQAQ/WeChatOpenDevTools-Python/main/./docs/images/MG38.jpg; https://raw.githubusercontent.com/JaveleyQAQ/WeChatOpenDevTools-Python/main/./docs/images/demo1.png; https://raw.githubusercontent.com/JaveleyQAQ/WeChatOpenDevTools-Python/main/./docs/images/demo2.png,,946,2024-01-17T02:17:07Z
2024-05-10,https://github.com/vocodedev/vocode-python,https://raw.githubusercontent.com/vocodedev/vocode-python/main/README.md,"Vocode is an open-source library designed to simplify the creation of voice-based LLM applications. It empowers developers to easily generate real-time streaming conversations, deployable over phone calls, Zoom meetings, and more, enabling the development of personal assistants or unique applications like voice-controlled chess. The library offers straightforward abstractions and integrations, consolidating essential tools in one package. Vocode supports a range of out-of-the-box integrations with transcription and synthesis services, including popular ones like AssemblyAI, Deepgram, Google Cloud, Microsoft Azure, and Whisper among transcribers, and Azure, Google Cloud, and AWS Polly among synthesizers. It is compatible with various LLMs like ChatGPT and GPT-4. The project encourages community contributions and is actively seeking maintainers. Installation is streamlined through pip, and the platform provides comprehensive documentation to facilitate development.",Unlock the Power of Voice: Building LLM Apps with Vocode,"Discover how Vocode, the open-source library, simplifies creating voice-based LLM apps for various platforms, including phone calls and Zoom meetings. Perfect for developers aiming to integrate real-time streaming conversations with large language models (LLMs), Vocode offers a comprehensive suite of tools and integrations. Whether you're building personal assistants or innovative apps like voice-based chess, Vocode provides the resources you need in one library. Join our community of maintainers to contribute to the future of voice-based applications.","Explore Vocode to build voice-based LLM apps quickly and efficiently. From personal assistants to voice-enabled games, find out how this open-source library can transform your projects with easy integrations and powerful features.",Voice Conversion Tool,"Python





        2,338





        374


        Built by

          









        2 stars today",https://user-images.githubusercontent.com/6234599/228337850-e32bb01d-3701-47ef-a433-3221c9e0e56e.png,,2338,2023-02-24T18:44:46Z
2024-05-11,https://github.com/kyegomez/AlphaFold3,https://raw.githubusercontent.com/kyegomez/AlphaFold3/main/README.md,"The text introduces AlphaFold3, an implementation of the algorithm from the paper ""Accurate structure prediction of biomolecular interactions with AlphaFold3,"" available in PyTorch. Users can install the software using pip. It outlines how to generate input tensor sizes for both pair and single representations using PyTorch, details the Genetic Diffusion process which operates on atomic coordinates, and provides a full model example showcasing a forward pass through the AlphaFold3 model. The model aims to predict biomolecular interactions' structure accurately by working with explicit atomic positions, simplifying MSA processing, and utilizing a diffusion module to produce a distribution of potential structures. This approach helps counteract the model's tendency to ""hallucinate"" plausible but incorrect structures by incorporating cross-distillation with training data enriched by AlphaFold multimer predictions and incorporating various techniques for predicting structural confidence levels. The provided citation details the contributors and the journal it's published in. Lastly, it discusses the model's ability to predict protein nuclear structures and the plan for future improvements, including generating a large number of predictions to rank them and using top confidence samples for better inference accuracy.",Exploring AlphaFold3: A New Horizon in Biomolecular Structure Prediction,"AlphaFold3 introduces groundbreaking advancements in biomolecular structure prediction through its implementation in PyTorch. With features like genetic diffusion and enhanced sequence processing, AlphaFold3 leverages deep learning to predict atomic coordinates with unmatched accuracy. Its innovative approach combines pair and single representations, diffusing through steps to refine predictions and counteract hallucinations in structure generation. AlphaFold3 not only predicts structures but also includes confidence measures, opening new avenues in scientific research and drug discovery.","Dive into the capabilities of AlphaFold3, the latest breakthrough in predicting biomolecular interactions with unparalleled accuracy. Learn how its PyTorch implementation and unique features set a new standard in structural biology.",Deep Learning Tool,"Python





        363





        36


        Built by

          





        46 stars today",https://raw.githubusercontent.com/kyegomez/AlphaFold3/main/agorabanner.png,,363,2024-05-08T16:54:21Z
2024-05-11,https://github.com/stooged/PI-Pwn,https://raw.githubusercontent.com/stooged/PI-Pwn/main/README.md,"PI Pwn is a script designed for setting up PPPwn on a Raspberry Pi to facilitate running GoldHen on a PS4 with firmware version 11.0. Installation is demonstrated through videos, and it's compatible with Raspberry Pi models 3B+, 4, 5, and 400, with slower performance on Raspberry Pi Zero 2 W and Pi Zero W using USB to Ethernet adapters. Users need to install Raspberry Pi OS Lite on an SD card, adjust settings if using a USB to Ethernet adapter, and follow specific commands to install and run PPPwn. For the PS4 setup, adjustments in the Network settings are required, including PPPoE selection and DNS configurations. GoldHen necessitates placing a file on a USB drive. The Raspberry Pi attempts to exploit the PS4 automatically, requiring patience as the process may fail and retry multiple times. The system is designed for ease, aiming to exploit the console without user intervention after initial setup. Editing of exploit scripts is possible by accessing the SD card directly.",Ultimate Guide to Installing PPPwn on Raspberry Pi for PS4 Exploits,"Discover how to set up PPPwn on your Raspberry Pi and run GoldHen on PS4 firmware 11.0, adding new levels of functionality to your gaming experience. This post guides you through installing Raspberry Pi OS Lite, configuring your system for success, and linking your PS4 for an automated exploit process. Whether you're using a Raspberry Pi 3B+, 4 Model B, or even the newer Raspberry Pi 5, we've got you covered. Plus, find out how to tackle the setup using a USB to Ethernet adapter for models like the Raspberry Pi Zero 2 W. Dive into the endless possibilities of gaming enhancements with this simple, step-by-step tutorial.","Learn how to effortlessly install PPPwn on Raspberry Pi and execute GoldHen on PS4 fw 11.0. Follow our easy guide for a seamless setup process, compatible with multiple Raspberry Pi models.",Game Development Tool,"Python





        113





        15


        Built by

          





        28 stars today",,,113,2024-05-09T07:28:30Z
2024-05-11,https://github.com/stanford-oval/storm,https://raw.githubusercontent.com/stanford-oval/storm/main/README.md,"STORM (Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking) is an advanced system designed to write Wikipedia-like articles by conducting internet-based research. Launched in April 2024, the system has since introduced a refactored codebase, enabling customization across different language models and integration with various search engines. STORM operates in two main stages: firstly, the pre-writing stage where it gathers references and generates an outline through internet research, and secondly, the writing stage where it compiles a full-length article using the outlined structure and citations. The unique approach of STORM lies in its method of generating in-depth and broad questions, utilizing strategies like Perspective-Guided Question Asking and Simulated Conversation. These innovations allow the system to cover various angles of a topic by simulating a dialogue between a Wikipedia writer and an expert, enriching the content creation process. While initially not generating publication-ready articles, STORM serves as a valuable tool for experienced Wikipedia editors at the pre-writing phase. The system's modularity and flexibility in customization invite contributions from users to further enhance its capabilities, from refining the knowledge curation engine to integrating diverse language models and search engines.",Unlocking the Power of Automated Research and Writing with STORM,"STORM propels the synthesis of Wikipedia-like articles by automating the research and writing stages, forming a bridge between raw information and structured knowledge. By breaking down the article generation process into collecting references and creating detailed outlines before the final write-up, it serves as a versatile tool for Wikipedia editors. Its innovative approach includes perspective-guided question asking and simulated conversations to enrich content depth. STORM is not just software but a highly customizable platform, encouraging contributions to refine and enhance its capabilities. Dive into STORM's world to revolutionize how knowledge is curated and presented.","Explore how STORM revolutionizes writing Wikipedia-like articles through automated research and multi-perspective question asking, offering a customizable platform for knowledge curation.",AI Development Platform,"Python





        4,224





        408


        Built by

          









        72 stars today",https://raw.githubusercontent.com/stanford-oval/storm/main/assets/overview.png; https://raw.githubusercontent.com/stanford-oval/storm/main/assets/two_stages.jpg,,4224,2024-03-24T16:23:39Z
2024-05-11,https://github.com/jgravelle/AutoGroq,https://raw.githubusercontent.com/jgravelle/AutoGroq/main/README.md,"The PDF file provides the project's code, enabling modifications via Claude.AI or ChatGPT, created using a specific GitHub tool. AutoGroq is highlighted as an AI conversational assistant improving user interaction with AI tools. It's characterized by dynamic generation of expert agents and workflows, facilitating customized and efficient support for various tasks. AutoGroq's features include natural conversation flow, code snippet extraction, and flexible agent management. To use AutoGroq, one can access an online demo or install it locally following a detailed setup guide, involving prerequisites, virtual environment setup, dependency installation, and application launch procedures. The process involves inputting requests to generate agents and workflows. AutoGroq's architecture comprises a Streamlit application and utility modules for API interactions and agent management. Contributions to AutoGroq are welcome, and it's open-source under the MIT License.",Revolutionize AI Interactions with AutoGroq: Your Ultimate Conversational Assistant,"Discover how AutoGroq is transforming AI interactions with its powerful, AI-driven conversational assistant. Offering dynamic expert agent generation, customizable workflows, and a user-friendly interface, AutoGroq addresses the limitations of existing solutions. It's designed for intuitive, natural conversations and effortless code snippet extraction. Experience the future of AI with AutoGroq's live demo or explore its impactful features by installing it today.","Explore AutoGroq, an AI-powered conversational assistant revolutionizing user interactions with AI. Featuring dynamic agent generation, intuitive conversations, and seamless code extraction, AutoGroq promises a superior, user-friendly experience.",AI Development Platform,"Python





        238





        148


        Built by

          





        34 stars today",,https://www.youtube.com/watch?v=JkYzuL8V_4g,238,2024-03-29T14:32:46Z
2024-05-11,https://github.com/IAHispano/Applio,https://raw.githubusercontent.com/IAHispano/Applio/main/README.md,"Applio is a VITS-based Voice Conversion application highlighted for its simplicity, quality, and performance. It is accessible via its [official website](https://applio.org) and provides various resources such as documentation, plugins, compiled versions, and a playground for users to experiment with. The tool is cross-platform, supporting installations on Windows, Linux, and through Makefile, catering to different platform users. Users are guided on how to run Applio on Windows and Linux platforms, ensuring a seamless setup experience. Significant enhancements have been made to the repository, including a modular codebase, hop length implementation for increased efficiency, translations in over 30 languages, cross-platform compatibility, optimized requirements, and a more streamlined installation process. Furthermore, Applio features include a hybrid F0 estimation method, a user-friendly UI, an efficient plugin system, an overtraining detector, model search, and support for various output formats. It also employs a unique ID hashing system for models to prevent duplication, a model download system from multiple sources, and advanced Text-to-Speech functionalities. Enhancements aim to make Applio more robust, scalable, and user-friendly, inviting contributions from the community to further enrich its capabilities.",Maximize Your Audio Projects with Applio Voice Conversion: A Comprehensive Guide,"Discover the simplicity, quality, and performance of Applio, a VITS-based voice conversion tool designed for accessibility and enhanced performance. From easy installation on various platforms to robust usage guidelines, Applio streamlines voice conversion processes. The recent enhancements, including modular codebase, optimized requirements, and a user-friendly interface, underscore its commitment to innovation. With features like a plugin system, overtraining detector, and enhanced TTS functionality, Applio stands out as a versatile tool for diverse audio projects. Join the Applio community today to elevate your audio projects!","Explore the advanced features of Applio, the VITS-based voice conversion platform that emphasizes ease of use, quality, and performance. Learn how its recent updates, including modular codebase and optimized UI, make it a top choice for audio professionals.",Voice Conversion Tool,"Python





        1,049





        176


        Built by

          









        4 stars today",,,1049,2023-08-07T22:42:16Z
2024-05-11,https://github.com/mustafaaljadery/gemma-2B-10M,https://raw.githubusercontent.com/mustafaaljadery/gemma-2B-10M/main/README.md,"The Gemma 2B model offers a significant advancement in language model development by supporting a 10-million token context length while operating under a memory constraint of less than 32GB. This efficiency is achieved through the innovative use of recurrent local attention mechanisms, optimized for CUDA, allowing for linear memory usage (O(N)) contrary to the quadratic memory requirement found in standard multi-head attention designs. The initial release is an early checkpoint at 200 steps, with plans for extensive further training. To use the model, it can be installed via pip and the Huggingface platform. The implementation highlights not only technical prowess but also a collaborative effort among developers Mustafa Aljadery, Siddharth Sharma, and Aksh Garg. This approach draws inspiration from the Transformer-XL paper and employs strategies from the InfiniAttention concept, addressing the main memory bottleneck in large language models by segmenting the attention into manageable local blocks. For those interested, a deeper dive into the model's motivations, design, and theoretical underpinnings is available on Medium.",Introducing Gemma 2B: Cutting-Edge AI with 10M Sequence Length and Efficient Memory Use,"Explore the revolutionary Gemma 2B model, boasting a 10M sequence length while operating under 32GB of memory. This groundbreaking technology employs recurrent local attention and cuda-optimized native inference, achieving O(N) memory consumption. An early checkpoint is now accessible, with more extensive training planned. Quick start guides and technical insights are readily available for enthusiasts eager to delve into the future of AI model efficiency.","Discover Gemma 2B, the AI model reshaping large sequence processing with a 10M context and <32GB memory requirement. Dive into the features, quick start guide, and the innovative technology behind it.",Language Models,"Python





        348





        20


        Built by

          







        175 stars today",https://raw.githubusercontent.com/mustafaaljadery/gemma-2B-10M/main/./images/graphic.png,,348,2024-05-07T00:52:21Z
2024-05-12,https://github.com/sol3dev/SOL-shitcoinbot,https://raw.githubusercontent.com/sol3dev/SOL-shitcoinbot/main/README.md,"The Solana-shitcoinbot is a trading and sniping bot designed for automated trading of Shitcoins on the Solana blockchain, emphasizing speed, security, and flexibility. It uses premium APIs like Birdeye for rapid data analysis and supports major exchanges, including Raydium, Jupiter, and Orca. Key features include in-depth market analysis, data visualization, advanced notification options, and customizable trading strategies. Users maintain complete control over their data, with everything processed locally for enhanced security. Installation involves downloading the package, running the auto-installer, and setting up trading parameters. It offers tools for detailed operation analysis, account management, and supports multiple trading profiles. Originally a personal project by Sol3Dev, the bot has gained traction for its precision and effectiveness. Users can support its development through donations to a provided Solana wallet address.",Maximize Your Trading Efficiency with Solana-Shitcoinbot: The Ultimate Sniping Tool,"Discover the power of Solana-Shitcoinbot, your go-to solution for automated trading and sniping of Shitcoins on the Solana blockchain. With features like high-speed data analysis, comprehensive market analysis, and support for major exchanges, this bot offers unparalleled trading efficiency and security. Customize your trading strategy with advanced notification options and data visualization tools, all while maintaining complete control over your trading operations. Perfect for both novice and experienced traders, Solana-Shitcoinbot is the key to unlocking the full potential of your trading endeavors.","Enhance your trading on Solana with Solana-Shitcoinbot: a highly efficient and secure bot for sniping and trading Shitcoins. Key features include real-time data analysis, market monitoring, and compatibility with major exchanges. Learn how to maximize your trading potential today.",Crypto Trading Bot,"Python





        230





        24


        Built by

          





        115 stars today",https://raw.githubusercontent.com/sol3dev/SOL-shitcoinbot/main/readme/Solanabot.png; https://raw.githubusercontent.com/sol3dev/SOL-shitcoinbot/main/readme/dwsmall.png; https://raw.githubusercontent.com/sol3dev/SOL-shitcoinbot/main/readme/interface.png; https://raw.githubusercontent.com/sol3dev/SOL-shitcoinbot/main/readme/res1.png; https://raw.githubusercontent.com/sol3dev/SOL-shitcoinbot/main/readme/res2.png,,230,2024-05-04T12:18:45Z
2024-05-12,https://github.com/modelscope/agentscope,https://raw.githubusercontent.com/modelscope/agentscope/main/README.md,"AgentScope is an innovative platform intended for developers to create multi-agent applications with large-scale models easily. It is released under Apache License 2.0, and contributions are welcome. This project supports a wide range of model libraries through `ModelWrapper` for both local model services and third-party APIs. Key features include easy usage, high robustness, and actor-based distribution for simplified development of distributed applications. Various services such as web search, data query, and text processing are supported, alongside a plethora of example applications. AgentScope can be installed from the source or via pip, requiring Python 3.9 or higher. It allows for decoupled model deployment and invocation, facilitated by model configuration files. For interaction, agents communicate through messages containing necessary fields like name and content. Detailed tutorials are available for getting started, along with advanced topics like custom agent creation, pipeline management, and distribution strategies. Additionally, AgentScope provides a runtime user interface for a better development experience.",Exploring AgentScope: Revolutionizing Multi-Agent Applications Development,"Discover AgentScope, the cutting-edge platform for developing LLM-empowered multi-agent applications with ease. Offering robust features like customizable fault-tolerance, streamlined distributed application development, and extensive model library support, AgentScope stands out as the go-to solution for developers. Embrace the future of application development by leveraging AgentScope's comprehensive documentation, examples, and community support. Dive into the world of AgentScope and elevate your developer journey.","Unlock the potential of building multi-agent applications with AgentScope. Explore its easy-to-use interface, high robustness, and actor-based distribution system for an enhanced development experience. Join the AgentScope community today.",AI Development Platform,"Python





        1,729





        106


        Built by

          









        173 stars today",https://img.alicdn.com/imgextra/i2/O1CN01cdjhVE1wwt5Auv7bY_!!6000000006373-0-tps-1792-1024.jpg; https://gw.alicdn.com/imgextra/i1/O1CN01hhD1mu1Dd3BWVUvxN_!!6000000000238-2-tps-400-400.png; https://img.alicdn.com/imgextra/i2/O1CN01tuJ5971OmAqNg9cOw_!!6000000001747-0-tps-444-460.jpg; https://img.alicdn.com/imgextra/i3/O1CN01SFL0Gu26nrQBFKXFR_!!6000000007707-2-tps-500-500.png; https://img.alicdn.com/imgextra/i3/O1CN01SFL0Gu26nrQBFKXFR_!!6000000007707-2-tps-500-500.png; https://img.alicdn.com/imgextra/i3/O1CN01SFL0Gu26nrQBFKXFR_!!6000000007707-2-tps-500-500.png; https://gw.alicdn.com/imgextra/i3/O1CN01X673v81WaHV1oCxEN_!!6000000002804-0-tps-2992-1498.jpg,,1729,2024-01-12T03:41:59Z
2024-05-12,https://github.com/Alpha-VLLM/Lumina-T2X,https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/README.md,"Lumina-T2X, with its Flow-based Large Diffusion Transformers (Flag-DiT), turns textual descriptions into images, videos, multi-view 3D images, and synthesized speech. The Flag-DiT, supporting up to 7 billion parameters for sequences up to 128,000 tokens long, enables high-resolution, aspect ratio, and duration flexible outputs. Employing techniques like RoPE, RMSNorm, and KQ-norm, it accelerates training and stabilizes dynamics. Lumina-T2X models, particularly Lumina-T2I and Lumina-Next-T2I, exhibit notable achievements in generating high-quality visual and audio content from text. The project, showcasing advanced computational efficiency, also released substantial checkpoints and interactive demos, contributing significantly to textual content transformation research.",Unveiling Lumina-T2X: A New Frontier in Text-to-Modality Transformations,"Discover the cutting-edge Lumina-T2X, a groundbreaking series of Diffusion Transformers aimed at converting text into vivid images, dynamic videos, 3D models, and synthesized speech. Boasting a robust Flow-based Large Diffusion Transformer core, Lumina-T2X can generate content of any resolution, aspect ratio, and duration. Its innovative design ensures fast training, stable dynamics, and supports an array of modalities within a single framework. Transform text into stunning visuals or audio with minimal computational resources, thanks to Lumina-T2X's efficient learning model.","Explore Lumina-T2X, the revolutionary text-to-any modality transformer capable of generating images, videos, 3D models, and speech from textual descriptions. Experience unparalleled resolution, aspect ratio, and duration flexibility with cutting-edge technology.",Multimodal AI Model,"Python





        451





        19


        Built by

          









        74 stars today",https://raw.githubusercontent.com/Alpha-VLLM/Lumina-T2X/main/./assets/lumina-logo.png,https://www.youtube.com/watch?v=KFtHmS5eUCM; https://www.youtube.com/watch?v=KFtHmS5eUCM,451,2024-03-28T13:23:28Z
2024-05-12,https://github.com/xinntao/Real-ESRGAN,https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/README.md,"Real-ESRGAN is a practical application for general image/video restoration, extending from the powerful ESRGAN model and trained with purely synthetic data. It introduces new models optimized for anime images and videos, featuring the **AnimeVideo-v3** and **RealESRGAN_x4plus_anime_6B** for enhanced anime visual quality. The project offers various ways to use the models, including online demos, portable executable files for Windows/Linux/MacOS without the need for a CUDA or PyTorch environment, and a Python script for more flexibility. Real-ESRGAN also integrates with GFPGAN for face enhancement and supports arbitrary output sizes, tiling, images with alpha channels, gray images, and 16-bit images in its inference process. The project has seen multiple updates, adding new models, implementation options, and detailed guides for training and fine-tuning on custom datasets. Real-ESRGAN is part of a broader ecosystem of projects aimed at image and video restoration and enhancement, and it encourages community contributions and feedback.",Exploring Real-ESRGAN: Enhance Your Images with AI,"Discover the power of Real-ESRGAN, an advanced AI tool designed for image and video restoration, enhancing visual content with incredible detail. From boosting the quality of anime videos to improving general scene images, Real-ESRGAN offers versatile solutions for both personal and professional use. Explore its easy-to-use demos, portable executable files, and Python scripts for quick inference. Whether you're enhancing faces with GFPGAN or enlarging images without losing quality, Real-ESRGAN is reshaping the future of digital imagery.",Unleash the potential of Real-ESRGAN for superior image and video enhancement. Learn how this AI-powered tool can transform your visual content with ease. Explore now!,Image Generation Platform,"Python





        26,293





        3,301


        Built by

          









        14 stars today",https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/assets/realesrgan_logo.png; https://raw.githubusercontent.com/xinntao/Real-ESRGAN/master/assets/teaser.jpg; https://raw.githubusercontent.com/xinntao/public-figures/master/Real-ESRGAN/cmp_realesrgan_anime_1.png,https://www.youtube.com/watch?v=fxHWoDSSvSc,26293,2021-07-19T03:23:30Z
2024-05-13,https://github.com/linyiLYi/bilibot,https://raw.githubusercontent.com/linyiLYi/bilibot/main/README.md,"The Bilibili chatbot project is developed from user comments on Bilibili, supporting text and voice dialogues generated from a `questions.txt` file. It utilizes the [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat) model with fine-tuning from Apple's [mlx-lm LORA](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md) project. Voice generation is based on the [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) open-source project, with voice models trained by Bilibili user [ç™½èœå·¥åŽ‚1145å·å‘˜å·¥](https://space.bilibili.com/518098961). The project structure includes scripts, models, templates, and a compression tool for model optimization. Installation requires Python 3.10, with setup recommended via Anaconda, and has been tested on macOS. Instructions cover environment setup, model fine-tuning, inference testing, and voice generation setup using [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS), followed by running an API for voice service and a QA dialogue program.",Unlocking the Potential of Bilibili Chatbots with Advanced AI Training,"Explore the creation of a sophisticated chatbot trained on Bilibili user comments, leveraging the latest in AI from Qwen1.5-32B-Chat and Apple's mlx-lm LORA project for text generation, and the GPT-SoVITS open-source project for voice synthesis. This project stands out by not only supporting text chats but generating voice dialogues for specific questions, making it a comprehensive guide for Python enthusiasts and developers interested in AI chatbot development and voice synthesis.","Discover how to develop your own advanced AI chatbot with voice synthesis capabilities using Bilibili comments and the latest AI models, including a detailed guide on setting up the environment, training, and generating voice responses.",AI Development Platform,"Python





        844





        111


        Built by

          





        317 stars today",,,844,2024-05-06T09:02:52Z
2024-05-13,https://github.com/alibaba-damo-academy/FunClip,https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/README.md,"FunClip is a fully open-source, locally deployed, automated video clipping tool utilizing Alibaba's Damo Academy's open-source FunASR Paraformer series models for speech recognition in videos. Users can select textual segments or speakers identified from the speech recognition results to easily clip corresponding video segments. It features integration with Alibaba's Paraformer-Large, a leading Chinese ASR model known for its accuracy and predictive timestamp abilities, and SeACo-Paraformer for hot-word customization to enhance recognition. Additionally, it incorporates CAM++ for speaker ID recognition, allowing users to clip specific speakers' segments. Utilizing Gradio for interaction, FunClip offers simplicity in installation and use, supports multi-segment clipping, and automatically generates SRT subtitles for entire videos and selected clips. Recent updates include configuration options for output directories, UI improvements, bug fixes from FunASR API updates, and time offset settings for each clip. The tool plans to integrate Whisper for English video editing and large language models for advanced editing capabilities. Installation involves cloning the repository, installing Python dependencies, and optionally, imagemagick for auto subtitle generation. Users can access FunClip via Gradio on localhost, command line, or through the Modelscope space for a hands-on experience.",Explore FunClip: The Ultimate Open-Source Video Clipping Tool,"Discover FunClip, an open-source, precise, and user-friendly video clipping tool that leverages Alibaba DAMO Academy's FunASR for speech recognition. With features like integration of industrial-grade Paraformer-Large model for Chinese ASR, custom hotword functionality via SeACo-Paraformer, and speaker identification with CAM++, FunClip offers unparalleled accuracy in video editing. It supports multi-segment clipping, providing both full video and targeted segment SRT subtitles for a convenient editing experience. Join the community to explore more and contribute to subtitle generation or speech recognition advancements.","Learn about FunClip, an advanced open-source video clipping tool integrating Alibaba's FunASR for efficient video editing. Features include high-accuracy speech recognition, hotword customization, speaker ID cutting, and multi-segment clipping with subtitle support.",Video Translation Tool,"Python





        1,202





        137


        Built by

          









        134 stars today",https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/interface.jpg; https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/guide.jpg; https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/dingding.png; https://raw.githubusercontent.com/alibaba-damo-academy/FunClip/main/docs/images/wechat.png,,1202,2023-05-17T07:23:44Z
2024-05-13,https://github.com/multimodal-art-projection/MAP-NEO,https://raw.githubusercontent.com/multimodal-art-projection/MAP-NEO/main/README.md,"MAP-NEO is an open-source Large Language Model (LLM) trained on 4.5T English and Chinese tokens, showcasing performance on par with LLaMA2 7B. It excels in complex tasks such as reasoning, mathematics, and coding, aiming for transparency in LLM training by releasing all components of its development, including data, processing pipelines, pretraining scripts, and alignment code. The release includes MAP-NEO 7B models (base and chat versions), intermediate checkpoints, a custom tokenizer, the training corpus, and optimized pre-training code. These resources are available for both academic and commercial use under the terms of the MIT License, with the hope of fostering a wide range of research. Models can be accessed through Hugging Face.",Introducing MAP-NEO: The Groundbreaking Open-Sourced Large Language Model,"Discover MAP-NEO, a fully open-sourced Large Language Model that rivals proprietary models in reasoning, mathematics, and coding. Trained on over 4.5T tokens in English and Chinese, MAP-NEO matches the performance of top-tier models like LLaMA2 7B. With complete transparency, it includes pretraining data, a novel data processing pipeline, and efficient pretraining scripts. Publicly accessible, including various model sizes and the full data matrix, MAP-NEO aims to empower both academic and commercial research. Dive into this revolutionary project for an unrestricted exploration of Large Language Models.","Explore MAP-NEO: A fully open-sourced Large Language Model offering transparent, high-performance tools for research in reasoning, mathematics, coding, and more. Access the full suite of resources including data, models, and code.",Large-scale Language Models,"Python





        296





        31


        Built by

          









        65 stars today",,,296,2024-05-08T11:58:35Z
2024-05-13,https://github.com/LLaVA-VL/LLaVA-NeXT,https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/master/README.md,"LLaVA-NeXT introduces stronger multimodal models, enhancing capabilities for processing images, video, and more complex tasks. Released models, such as LLaVA-NeXT-34B, showcase advancements over previous versions like LLaVA-1.5, demonstrating improved performance on various benchmarks. Notably, the LLaVA-NeXT for Video model excels in video understanding through zero-shot modality transfer and benefits significantly from DPO training and AI feedback. The project aims at facilitating the development of new Large Multimodal Models (LMMs) by providing accessible training and evaluation resources, including a highly efficient evaluation pipeline (LMMs-Eval) and upgraded models with enhanced reasoning, OCR, and world knowledge. The models are built upon Vicuna and offer high scalability and efficiency. The project acknowledges contributions from numerous individuals and relies on frameworks such as lmms-eval for evaluation support. It adheres to the original licensing agreements of datasets and checkpoints used, highlighting the project's commitment to compliance and ethical usage.",Exploring LLaVA-NeXT: Enhancements in Large Multimodal Models,"Discover the latest advancements in multimodal AI with LLaVA-NeXT's release, showcasing stronger, image, and video models with cutting-edge capabilities. The newly released LLaVA-NeXT models, including stronger LMMs and video understanding models, offer improved performance on various benchmarks. Special features include zero-shot modality transfer and DPO training, indicating significant advancements in multimodal AI. Learn how these developments pave the way for future applications across diverse fields. Explore the demos, blogs, and code repositories to witness these innovations in action.","Explore the groundbreaking LLaVA-NeXT release, featuring enhanced large multimodal models with superior image and video understanding capabilities. Dive into the future of AI with our latest developments.",Multimodal AI Model,"Python





        333





        14


        Built by

          








        63 stars today",https://i.postimg.cc/pL17YtG4/WX20240508-220230-2x.png; https://raw.githubusercontent.com/LLaVA-VL/LLaVA-NeXT/master/assets/demo.gif,https://www.youtube.com/watch?v=mkI7EPD1vp8,333,2024-03-08T12:54:09Z
2024-05-13,https://github.com/pentestfunctions/BlueDucky,https://raw.githubusercontent.com/pentestfunctions/BlueDucky/main/README.md,"BlueDucky is an exploitation tool for Bluetooth devices, leveraging a vulnerability (CVE-2023-45866) for code execution via DuckyScript. It allows users to load, automatically save scanned Bluetooth devices, and send messages. Developed for Raspberry Pi 4, it operates on various phones except for some exceptions like Vodafone in New Zealand. Installation involves updating system packages, installing dependencies, and setting up BlueDucky and additional tools like bdaddr. Running the tool involves cloning the BlueDucky repository, initializing Bluetooth configuration, and executing the script with options for automatic scanning and using a known_devices.txt file for targeted attacks. Users can create custom payloads in DuckyScript format for different operations, such as opening websites in private browsing mode.",Exploit Bluetooth Vulnerabilities with BlueDucky: A Complete Guide,"Discover the power of BlueDucky, an innovative tool designed to exploit a major vulnerability in Bluetooth devices, allowing for unauthenticated peering and code execution. Learn how to set up and run BlueDucky on a Raspberry Pi 4, target invisible devices with saved Bluetooth profiles, and send commands using DuckyScript. Explore the steps for successful execution and automatic connection to various devices, including a caveat for Vodafone phones in New Zealand. Join the community on HackNexus to discuss more and share ideas for Duckyscript payloads. Dive into the world of Bluetooth security with BlueDucky today.","Unlock the potential of BlueDucky for exploiting Bluetooth vulnerabilities. This guide covers setup, usage, and operational steps for successful device targeting and command execution. Join our community on HackNexus.",Cybersecurity Tool,"Python





        571





        94


        Built by

          








        76 stars today",https://raw.githubusercontent.com/pentestfunctions/BlueDucky/main/./images/duckmenu.png; https://raw.githubusercontent.com/pentestfunctions/BlueDucky/main/./images/BlueDucky.gif,,571,2024-01-16T06:52:02Z
2024-05-13,https://github.com/OS-Copilot/OS-Copilot,https://raw.githubusercontent.com/OS-Copilot/OS-Copilot/main/README.md,"OS-Copilot is an open-source platform designed to create generalist agents capable of interacting with various elements of an operating system, including web, code terminals, files, multimedia, and third-party applications. It was accepted at the LLM Agents Workshop@ICLR 2024. The platform offers a guide to quickly start with cloning its repository, setting up Python environment, installing dependencies, configuring OpenAI API key, and running a demo script. The tutorials provide instructions for beginners to advanced users, covering installation, getting started, adding tools, deploying API services, automating tasks with Excel, and enhancing functionalities with self-learning and custom API tools. The community around OS-Copilot is accessible via Discord and Twitter, encouraging contributions and sharing developments. The project includes a disclaimer regarding its ""as is"" use and potential risks. The creators invite contact for inquiries or suggestions via email and provide a citation for academic use.",OS-Copilot: Revolutionizing Computer Task Automation with AI,"Discover OS-Copilot, the cutting-edge open-source library designed for creating generalist computer agents that can seamlessly interface with various OS components, including web, code terminals, files, multimedia, and third-party applications. This revolutionary tool marks a significant stride towards automating general computer tasks through intuitive agent interactions, empowering users to boost their productivity and simplify complex workflows. Join the OS-Copilot community now and contribute to the future of automated computer operations.","Explore OS-Copilot, an innovative AI-driven platform for building generalist agents that automate interactions across OS elements. Enhance productivity with smart automation. Join our community today!",AI Development Platform,"Python





        1,244





        128


        Built by

          









        34 stars today",,,1244,2024-02-02T01:55:49Z
2024-05-13,https://github.com/h2oai/h2ogpt,https://raw.githubusercontent.com/h2oai/h2ogpt/main/README.md,"h2oGPT is an open-source Apache V2 project enabling users to interact with or summarize documents privately using GPT LLMs. It supports a wide range of document types including PDFs, Excel, Word, images, videos, audio, code, and more, using a private offline database that supports various storage backends and accurate embeddings. The project boasts efficient, parallel processing capabilities and supports a variety of models with GPU and CPU options. It features a user-friendly UI and CLI, supporting document upload and view, image and voice processing, and an AI assistant voice control mode. Additionally, h2oGPT offers easy installation for Linux, macOS, and Windows, supports inference servers, and complies with OpenAI's server proxy API. Users can also integrate web-search, evaluate model performance, and explore a wide range of experimental features. The project emphasizes quality with extensive testing and aims to enhance large language models' code completion, reasoning, and factual accuracy while minimizing unwanted outputs.",Transforming Digital Document Management with h2oGPT: The Ultimate Open-Source Solution,"Discover h2oGPT, an Apache V2 open-source project revolutionizing document management and GPT LLM interactions in a private, efficient, and versatile environment. With support for a wide array of document types and a robust database, h2oGPT offers parallel summarization, cutting-edge model support, and intuitive UI/CLI access. Experience unparalleled performance and privacy in managing, summarizing, and querying documents or engaging in conversational AI, all supported across major operating systems. Join the h2oGPT community and elevate your GPT LLM interactions today.","Explore h2oGPT, the leading open-source project for private and efficient management of documents and GPT LLM conversations. Offers versatile model support, parallel processing, and a user-friendly interface across all major OS.",AI Development Platform,"Python





        10,643





        1,176


        Built by

          









        107 stars today",https://raw.githubusercontent.com/h2oai/h2ogpt/main/docs/img-small.png,https://www.youtube.com/watch?v=Coj72EzmX20; https://www.youtube.com/watch?v=FTm5C_vV_EY; https://www.youtube.com/watch?v=H8Dx-iUY49s; https://www.youtube.com/watch?v=_iktbj4obAI,10643,2023-03-24T21:31:25Z
2024-05-14,https://github.com/entropy-research/Devon,https://raw.githubusercontent.com/entropy-research/Devon/main/README.md,"""Devon"" is an open-source project designed to function as a pair programmer, helping developers with multi-file editing, codebase exploration, config and test writing, bug fixing, and architecture exploration. It's particularly geared towards Python, with limited functionality for other languages. To install, prerequisites include node.js, npm, pipx, and an Anthropic API key. Installation can be done via a bash script or using pipx and npm for the Python backend and CLI tool. The project is in its early stages, seeking contributions to enhance features like multi-model support, a plugin system, a self-hostable Electron app, and setting new benchmarks. Contributions are welcomed in core functionality, research, feedback, and testing. The project also emphasizes the value of community feedback and contributions, inviting members to join their Discord server or file issues and suggestions on GitHub. Devon operates under the Apache 2.0 License.",Devon: Revolutionizing Pair Programming with Open Source Collaboration,"Discover Devon, the open-source pair programmer that's changing the game for developers everywhere. With features like multi-file editing, bug fixing, and architecture exploration, it's designed to enhance your coding efficiency. Installation is straightforward, requiring node.js, npm, and an Anthropic API key. Despite being in its early stages, Devon promises to significantly impact how developers write code, thanks to its community-driven approach and future plans for multi-model support and a self-hostable Electron app. Join the journey and contribute to making Devon an indispensable tool for developers.","Explore Devon, an open-source pair programmer enhancing coding efficiency with features like multi-file editing and bug fixing. Easy to install and community-driven, it's set to revolutionize software development.",AI Coding Assistant,"Python





        495





        36


        Built by

          









        74 stars today",,,495,2024-03-15T04:52:31Z
2024-05-14,https://github.com/CTFd/CTFd,https://raw.githubusercontent.com/CTFd/CTFd/master/README.md,"CTFd is a customizable Capture The Flag (CTF) framework designed for ease of use, facilitating individuals or teams to run CTF competitions. It boasts a wide array of features including creating challenges, dynamic scoring, a plugin architecture for custom challenges, automatic bruteforce protection, and support for both individual and team competitions. The platform offers a comprehensive admin interface for managing challenges, categories, hints, flags, and more, along with a scoreboard, scoregraphs, and a Markdown content management system. It supports SMTP and Mailgun for email functionalities and includes options for automatic competition management. CTFd can be easily installed via Docker or by running a Python script, with extensive documentation available for deployment and customization. The platform also integrates with MajorLeagueCyber for event tracking and registration. For those seeking a managed hosting solution, CTFd offers services for hassle-free deployment.",Maximizing CTF Performance: An Expert Guide to CTFd Features and Setup,"Discover how CTFd offers a comprehensive framework designed for ease of use and adaptability in Capture The Flag competitions. With dynamic scoring, team competitions, and extensive customization through plugins and themes, CTFd is the ultimate platform for organizing and running CTF challenges. Uncover the steps to install CTFd, explore its vast range of features, including automatic bruteforce protection and team management, and learn how it integrates seamlessly with MajorLeagueCyber for enhanced competition management.","Unlock the full potential of your Capture The Flag competition with CTFd. Learn about its user-friendly design, plethora of features for robust competition management, and easy steps for setup and integration with MajorLeagueCyber.",Cybersecurity Tool,No specific star count found,,,5344,2015-01-01T05:36:55Z
2024-05-14,https://github.com/Chaphlagical/Deblur-GS,https://raw.githubusercontent.com/Chaphlagical/Deblur-GS/main/README.md,"The paper ""Deblur-GS: 3D Gaussian Splatting from Camera Motion Blurred Images,"" to be presented at I3D 2024, introduces Deblur-GS, a novel method for reconstructing sharp radiance fields from blurred images using 3D Gaussian Splatting (3DGS). This approach addresses the limitations of previous methods that struggle with the quality of input images and initial camera pose initialization. By treating motion blur as a joint optimization challenge involving camera trajectory estimation and time sampling, Deblur-GS cohesively optimizes the parameters of Gaussian points and the camera trajectory during the shutter time. This method significantly enhances performance and rendering quality over existing approaches, as validated through evaluations on both synthetic and real datasets. The official implementation, accompanied by installation and running instructions, supports various command-line arguments for customization, including training, evaluation, rendering, and video generation.",Revolutionizing 3D Visuals with Deblur-GS: A Groundbreaking Method,"Deblur-GS introduces a novel approach to enhance novel view synthesis by reconstructing sharp radiance fields from camera motion blurred image sets. This innovative method considerably reduces training times and accelerates rendering speeds without compromising on quality. It tackles the challenge of motion blur through a joint optimization of camera trajectory estimation and time sampling, setting a new standard in 3D Gaussian splatting technology. Deblur-GS's superior performance has been validated on both synthetic and real-world datasets, marking a significant advancement in the field of 3D imaging and rendering.","Explore how Deblur-GS transforms 3D imaging by accurately reconstructing quality visuals from motion-blurred images, significantly improving rendering speeds and quality. A deep dive into the future of 3D visualization.",Image Generation Platform,"Python





        193





        17


        Built by

          





        64 stars today",https://raw.githubusercontent.com/Chaphlagical/Deblur-GS/main/asset/teaser.png,,193,2024-03-06T03:54:32Z
2024-05-15,https://github.com/pipecat-ai/pipecat,https://raw.githubusercontent.com/pipecat-ai/pipecat/main/README.md,"Pipecat is a versatile framework designed for creating voice and multimodal conversational agents, including personal assistants, storytelling toys for children, customer support bots, and various other interactive applications. It allows for easy development and deployment, supporting both local setup and cloud-based solutions, with capabilities to integrate functionalities like telephone numbers, image and video processing, alongside different Large Language Models (LLMs). The framework provides a straightforward installation process, and while it comes with basic functionality, it allows for the addition of optional dependencies to extend its capabilities, covering a wide range of AI services and transport methods. Pipecat includes a variety of example applications and foundational code snippets to help users start developing their own conversational agents. It demonstrates a simple voice agent setup using WebRTC for real-time media transport and text-to-speech services, with detailed guidance on setting up, running tests, and customizing the development environment for optimal use. Pipecat also emphasizes the importance of Voice Activity Detection (VAD) for interactive applications and offers community support through Discord and X for further assistance and collaboration.",Unleashing Potential with Pipecat: The Next Generation Conversational Agent Framework,"Pipecat is revolutionizing the way we interact with technology through its advanced framework for building voice and multimodal conversational agents. From personal coaches to customer support bots, Pipecat offers a range of applications. The platform supports various AI services and transports, providing developers with the tools to create innovative applications. Whether running locally or in the cloud, Pipecat simplifies the development of voice agents with its flexible framework. Dive into the future of conversational agents by exploring Pipecat's capabilities and example applications.","Discover Pipecat, the advanced framework for building voice and multimodal conversational agents. Explore innovative applications and start developing with Pipecat today.",AI Development Platform,"Python





        887





        25


        Built by

          









        265 stars today",https://raw.githubusercontent.com/pipecat-ai/pipecat/main/pipecat.png; https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/simple-chatbot/image.png; https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/storytelling-chatbot/image.png; https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/translation-chatbot/image.png; https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/moondream-chatbot/image.png,https://www.youtube.com/watch?v=lDevgsp9vn0,887,2023-12-27T12:59:00Z
2024-05-15,https://github.com/openai/tiktoken,https://raw.githubusercontent.com/openai/tiktoken/main/README.md,"TikToken is a rapid Byte Pair Encoding (BPE) tokenizer designed for compatibility with OpenAI's models, offering a 3-6x speed improvement over similar open-source tokenizers. It effectively converts text into a sequence of tokensâ€”allowing for reversible, lossless text transformations that support both known and unseen text while compressing the original text and identifying common subwords to aid in model generalization. Users can easily install it via PyPI and find its documentation and usage examples online, including in the OpenAI Cookbook. TikToken can be extended either by directly creating custom `Encoding` objects or by using its plugin mechanism to register new encodings. Support and detailed information on BPE and TikToken's functionality, including educational resources for better understanding, are readily accessible.",Introducing tiktoken: The High-Speed BPE Tokenizer for OpenAI Models,"Discover tiktoken, the latest Byte Pair Encoding (BPE) tokenizer optimized for OpenAI's models, offering 3-6x faster performance compared to existing solutions. This open-source tool is easy to install from PyPI and integrates seamlessly with the OpenAI API, providing a robust and efficient method for text tokenization. Detailed documentation and example code available in the OpenAI Cookbook facilitate its application in diverse AI tasks. Whether you're extending its capabilities or integrating with custom encodings, tiktoken proves to be a valuable asset for developers working with language models.","Explore tiktoken, a fast and efficient BPE tokenizer for OpenAI models, featuring easy installation, extensive documentation, and superior performance. Ideal for developers enhancing AI applications.",AI Coding Assistant,"Python





        10,258





        697


        Built by

          









        90 stars today",,,10258,2022-12-01T23:22:11Z
2024-05-15,https://github.com/openai/openai-python,https://raw.githubusercontent.com/openai/openai-python/main/README.md,"The OpenAI Python library provides easy access to the OpenAI REST API for Python 3.7 and later versions, supporting both synchronous and asynchronous clients. It is automatically generated from the OpenAPI specification and offers type definitions for request parameters and response fields to facilitate reliable development practices. The library's detailed documentation is available online, and installation is straightforward using Pip. The SDK underwent a significant rewrite in version 1, released on November 6th, 2023, complete with a migration guide for existing users.

The library facilitates various operations, including chat completions, polling for asynchronous actions, bulk uploads, and streaming for handling ongoing interactions. It emphasizes security by recommending environment variables for API keys to avoid hardcoding sensitive information in source code. Error handling is robust, distinguishing between connection issues and API status errors, providing detailed feedback for debugging. It supports extensive customization, such as setting request timeouts and retries, and offers detailed logging capabilities. Advanced features include the handling of nested request parameters, file uploads, and pagination for list methods. For Microsoft Azure OpenAI users, a specific class is available that caters to Azure's API requirements, including support for Azure Active Directory tokens and custom endpoints.
",Mastering the OpenAI Python Library: A Complete Guide for Developers,"Explore the powerful features of the OpenAI Python library, your gateway to integrating sophisticated AI into your Python applications. Harness the power of GPT models with ease thanks to detailed type definitions, asynchronous support, and comprehensive API documentation. Discover how to install the library, navigate its usage, and leverage advanced features like streaming responses, file uploads, and error handling. This guide covers everything from installation to advanced usage, making it a must-read for developers looking to integrate OpenAI's AI capabilities into their projects.","A developer's guide to using the OpenAI Python library. Learn to easily integrate OpenAI's GPT models into your applications with detailed instructions on installation, usage, and leveraging advanced features.",AI Python Client,"Python





        20,187





        2,737


        Built by

          









        41 stars today",,,20187,2020-10-25T23:23:54Z
2024-05-15,https://github.com/aqlaboratory/openfold,https://raw.githubusercontent.com/aqlaboratory/openfold/main/README.md,"The text introduces OpenFold, a trainable PyTorch adaptation of DeepMind's AlphaFold 2, designed for protein structure prediction. OpenFold's documentation, installation, and training instructions are available on its ReadTheDocs page. The project's copyright section highlights that while OpenFold and AlphaFold's source code is under the Apache License 2.0, DeepMind's pretrained parameters are CC BY 4.0 licensed, a change from the previous CC BY-NC 4.0 license as of January 2022. The community is encouraged to contribute through issues and pull requests. The text also includes a request to cite their paper if OpenFold's work is used, along with a separate citation for using the OpenProteinSet. It emphasizes citing both AlphaFold and AlphaFold-Multimer works if relevant to the user's research.",Exploring OpenFold: A Trainable PyTorch Reproduction of AlphaFold 2,"OpenFold represents a remarkable effort to reproduce AlphaFold 2 using PyTorch, making it both faithful to the original and adaptable for further training. It's licensed under Apache License 2.0, making its advancements accessible while respecting DeepMind's original CC BY 4.0 licensed parameters. The project's documentation and resources are readily available online, encouraging contributions from the global community. Researchers and developers interested in structural biology and machine learning can benefit from OpenFold's insights into protein folding mechanisms and generalization capabilities. OpenFold not only advances the field of protein structure prediction but also fosters an open science culture.","Discover OpenFold, the PyTorch-based counterpart to DeepMind's AlphaFold 2, designed for enhanced trainability and open-source advancements in protein structure prediction. Learn how it contributes to the scientific community's understanding of protein folding mechanisms.",Deep Learning Tool,"Python





        2,458





        445


        Built by

          









        22 stars today",https://raw.githubusercontent.com/aqlaboratory/openfold/main/imgs/of_banner.png,,2458,2021-09-14T23:59:02Z
2024-05-15,https://github.com/nkasmanoff/pi-card,https://raw.githubusercontent.com/nkasmanoff/pi-card/main/README.md,"Pi-C.A.R.D is an AI-powered voice assistant specifically designed for the Raspberry Pi, utilizing its capabilities for conversational AI tasks similar to models like ChatGPT. If equipped with a camera, Pi-C.A.R.D can also capture images, describe them, and interact based on visual data. It operates by listening for a wake word to initiate conversation, eliminating the need for repeating the wake word for further commands. The device aims to offer a fun and possibly helpful assistant experience while prioritizing user privacy by operating entirely offline. Setup involves using cpp implementations for audio transcription and vision language models. The hardware requires a Raspberry Pi 5 Model B, a USB microphone, speaker, and a camera. Future updates aim to improve response times, add external services, and enhance interactive capabilities. The project name stands for Rasberry Pi - Camera Audio Recognition Device, highlighting its core functionalities.",Explore the Power of Pi-C.A.R.D: Your AI Voice Assistant on Raspberry Pi,"Discover the innovative Pi-C.A.R.D, an AI voice assistant designed to run on Raspberry Pi, offering conversational interactions and visual recognition capabilities. This device, short for Camera Audio Recognition Device, transforms your Raspberry Pi into a privacy-focused, standalone assistant. It operates locally, ensuring user data privacy and providing functionalities like taking photos and interpreting them without relying on the internet. Perfect for hobbyists and privacy advocates, Pi-C.A.R.D promises a blend of fun and functionality while keeping future expansions in sight.","Learn about Pi-C.A.R.D, the AI voice assistant for Raspberry Pi that supports voice commands and visual recognition, all while operating offline to protect your privacy. Ideal for DIY enthusiasts.",AI Development Platform,"Python





        482





        13


        Built by

          






        158 stars today",https://raw.githubusercontent.com/nkasmanoff/pi-card/main/assets/assistant.png; https://raw.githubusercontent.com/nkasmanoff/pi-card/main/assets/picard-facepalm.jpg,https://www.youtube.com/watch?v=OryGVbh5JZE,482,2024-05-04T18:23:28Z
2024-05-15,https://github.com/Chainlit/cookbook,https://raw.githubusercontent.com/Chainlit/cookbook/main/README.md,"The Chainlit Demos repository is a compilation of example projects showcasing the use of Chainlit for creating chatbot UIs. To run a demo, you first clone the repository, navigate to a demo folder, install dependencies, set up a `.env` file with necessary configurations, and then run the Chainlit app. The steps are designed to be straightforward, making it easy for users to get a demo running in their browser. Additionally, the document encourages community contributions, inviting users to share their own demos or suggest new ideas by opening issues or creating pull requests, highlighting the project's collaborative approach.",Ultimate Guide to Building Chatbot UIs with Chainlit: Step-by-Step Demos,"Discover the potential of Chainlit with our comprehensive cookbook-style repository, designed to guide you through creating cutting-edge chatbot UIs effortlessly. From cloning the repository to launching your own demo chatbot UI, we cover all the essential steps. Dive into various demo projects, each showcasing unique capabilities and integrations, making it simple to kickstart your Chainlit project. Get hands-on experience by following our easy-to-navigate steps and witness your chatbot take shape. Contribute to our ever-growing community with your own demos or by enhancing existing ones.","Explore our Chainlit Cookbook for a step-by-step guide on creating innovative chatbot UIs with various demos. Learn to clone, setup, and run your Chainlit chatbot effortlessly. Join our community today.",Collaborative AI Framework,"Python





        578





        212


        Built by

          









        3 stars today",,,578,2023-05-10T08:52:57Z
